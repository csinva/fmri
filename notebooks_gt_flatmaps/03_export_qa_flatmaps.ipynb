{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from ridge_utils.DataSequence import DataSequence\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cortex\n",
    "import seaborn as sns\n",
    "from os.path import join\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import sasc.viz\n",
    "import joblib\n",
    "import imodelsx.process_results\n",
    "import dvu\n",
    "import sys\n",
    "sys.path.append('../notebooks')\n",
    "from tqdm import tqdm\n",
    "from sasc.config import FMRI_DIR, STORIES_DIR, RESULTS_DIR\n",
    "from neuro.config import repo_dir, PROCESSED_DIR\n",
    "from neuro import analyze_helper, viz\n",
    "# from neuro.features.qa_questions import get_questions, get_merged_questions_v3_boostexamples\n",
    "flatmaps_per_question = __import__('06_flatmaps_per_question')\n",
    "# from neurosynth import term_dict, term_dict_rev\n",
    "# import viz\n",
    "from load_coef_flatmaps import _load_coefs_individual, _load_coefs_full, \\\n",
    "_load_coefs_individual_wordrate, _load_coefs_wordrate, _load_coefs_shapley, _load_coefs_individual_gpt4\n",
    "# imodelsx.process_results.delete_runs_in_dataframe(\n",
    "    # rr[rr.use_added_wordrate_feature == 1], actually_delete=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1944/1944 [04:46<00:00,  6.79it/s]\n",
      "/home/chansingh/imodelsx/imodelsx/process_results.py:92: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[k] = df[k].fillna(np.nan)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment varied these params: ['subject', 'pc_components', 'feature_selection_alpha', 'use_added_wordrate_feature', 'qa_questions_version', 'use_random_subset_features', 'single_question_idx', 'ndelays', 'seed']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 0/0 directories.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# results_dir = '/home/chansingh/mntv1/deep-fMRI/encoding/aug14_neurosynth_gemv'\n",
    "results_dir = '/home/chansingh/mntv1/deep-fMRI/encoding/oct17_neurosynth_gemv'\n",
    "rr, cols_varied, mets = analyze_helper.load_clean_results(results_dir)\n",
    "imodelsx.process_results.delete_runs_in_dataframe(\n",
    "    rr[rr.use_test_setup == 1], actually_delete=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.to_pickle('oct17_tmp.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this has the most recent result\n",
    "# results_dir = '/home/chansingh/mntv1/deep-fMRI/encoding/aug16_gpt4'\n",
    "# rr_gpt4, cols_varied, mets = analyze_helper.load_clean_results(results_dir)\n",
    "# rr_gpt4 = rr_gpt4[rr_gpt4.use_test_setup == 0]\n",
    "# imodelsx.process_results.delete_runs_in_dataframe(\n",
    "#     rr_gpt4[rr_gpt4.use_test_setup == 1], actually_delete=True)\n",
    "rr_gpt4.use_added_wordrate_feature.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_all = pd.read_pickle('oct17_tmp.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ndelays\n",
       "4    1676\n",
       "8     268\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr_all.ndelays.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes (268, 70) (1676, 70)\n",
      "\tskipping S01 v3_boostexamples_merged 0\n",
      "\tskipping S01 v3_boostexamples_merged 1\n",
      "\tskipping S01 v1neurosynth 0\n",
      "\tskipping S01 v1neurosynth 1\n",
      "weight_enet_mask 35 (606,)\n",
      "questions 35\n",
      "35 questions 35 weights\n",
      "saved S01 full_35_ndel=8_pc.pkl\n",
      "\tskipping S01 v3_boostexamples_merged 1\n",
      "39 questions 39 weights\n",
      "saved S01 full_neurosynth_ndel=8_pc.pkl\n",
      "\tskipping S01 v1neurosynth 1\n",
      "weight_enet_mask 35 (606,)\n",
      "questions 35\n",
      "35 questions 35 weights\n",
      "saved S01 full_35.pkl\n",
      "weight_enet_mask 36 (607,)\n",
      "questions 36\n",
      "36 questions 36 weights\n",
      "saved S01 full_35_wordrate.pkl\n",
      "39 questions 39 weights\n",
      "saved S01 full_neurosynth.pkl\n",
      "40 questions 40 weights\n",
      "saved S01 full_neurosynth_wordrate.pkl\n",
      "weight_enet_mask 35 (606,)\n",
      "questions 35\n",
      "35 questions 35 weights\n",
      "saved S01 full_35_pc.pkl\n",
      "weight_enet_mask 36 (607,)\n",
      "questions 36\n",
      "36 questions 36 weights\n",
      "saved S01 full_35_wordrate_pc.pkl\n",
      "39 questions 39 weights\n",
      "saved S01 full_neurosynth_pc.pkl\n",
      "40 questions 40 weights\n",
      "saved S01 full_neurosynth_wordrate_pc.pkl\n",
      "shapes (268, 70) (1676, 70)\n",
      "weight_enet_mask 35 (606,)\n",
      "questions 35\n",
      "35 questions 35 weights\n",
      "saved S02 full_35_ndel=8.pkl\n",
      "weight_enet_mask 36 (607,)\n",
      "questions 36\n",
      "36 questions 36 weights\n",
      "saved S02 full_35_wordrate_ndel=8.pkl\n",
      "39 questions 39 weights\n",
      "saved S02 full_neurosynth_ndel=8.pkl\n",
      "\tskipping S02 v1neurosynth 1\n",
      "weight_enet_mask 35 (606,)\n",
      "questions 35\n",
      "35 questions 35 weights\n",
      "saved S02 full_35_ndel=8_pc.pkl\n",
      "weight_enet_mask 36 (607,)\n",
      "questions 36\n",
      "36 questions 36 weights\n",
      "saved S02 full_35_wordrate_ndel=8_pc.pkl\n",
      "39 questions 39 weights\n",
      "saved S02 full_neurosynth_ndel=8_pc.pkl\n",
      "40 questions 40 weights\n",
      "saved S02 full_neurosynth_wordrate_ndel=8_pc.pkl\n",
      "weight_enet_mask 35 (606,)\n",
      "questions 35\n",
      "35 questions 35 weights\n",
      "saved S02 full_35.pkl\n",
      "weight_enet_mask 36 (607,)\n",
      "questions 36\n",
      "36 questions 36 weights\n",
      "saved S02 full_35_wordrate.pkl\n",
      "39 questions 39 weights\n",
      "saved S02 full_neurosynth.pkl\n",
      "40 questions 40 weights\n",
      "saved S02 full_neurosynth_wordrate.pkl\n",
      "weight_enet_mask 35 (606,)\n",
      "questions 35\n",
      "35 questions 35 weights\n",
      "saved S02 full_35_pc.pkl\n",
      "weight_enet_mask 36 (607,)\n",
      "questions 36\n",
      "36 questions 36 weights\n",
      "saved S02 full_35_wordrate_pc.pkl\n",
      "39 questions 39 weights\n",
      "saved S02 full_neurosynth_pc.pkl\n",
      "40 questions 40 weights\n",
      "saved S02 full_neurosynth_wordrate_pc.pkl\n",
      "shapes (268, 70) (1676, 70)\n",
      "\tskipping S03 v3_boostexamples_merged 0\n",
      "\tskipping S03 v3_boostexamples_merged 1\n",
      "\tskipping S03 v1neurosynth 0\n",
      "\tskipping S03 v1neurosynth 1\n",
      "weight_enet_mask 35 (606,)\n",
      "questions 35\n",
      "35 questions 35 weights\n",
      "saved S03 full_35_ndel=8_pc.pkl\n",
      "\tskipping S03 v3_boostexamples_merged 1\n",
      "39 questions 39 weights\n",
      "saved S03 full_neurosynth_ndel=8_pc.pkl\n",
      "\tskipping S03 v1neurosynth 1\n",
      "weight_enet_mask 35 (606,)\n",
      "questions 35\n",
      "35 questions 35 weights\n",
      "saved S03 full_35.pkl\n",
      "weight_enet_mask 36 (607,)\n",
      "questions 36\n",
      "36 questions 36 weights\n",
      "saved S03 full_35_wordrate.pkl\n",
      "39 questions 39 weights\n",
      "saved S03 full_neurosynth.pkl\n",
      "40 questions 40 weights\n",
      "saved S03 full_neurosynth_wordrate.pkl\n",
      "weight_enet_mask 35 (606,)\n",
      "questions 35\n",
      "35 questions 35 weights\n",
      "saved S03 full_35_pc.pkl\n",
      "weight_enet_mask 36 (607,)\n",
      "questions 36\n",
      "36 questions 36 weights\n",
      "saved S03 full_35_wordrate_pc.pkl\n",
      "39 questions 39 weights\n",
      "saved S03 full_neurosynth_pc.pkl\n",
      "40 questions 40 weights\n",
      "saved S03 full_neurosynth_wordrate_pc.pkl\n",
      "shapes (268, 70) (1676, 70)\n",
      "\tskipping S04 v3_boostexamples_merged 0\n",
      "\tskipping S04 v3_boostexamples_merged 1\n",
      "\tskipping S04 v1neurosynth 0\n",
      "\tskipping S04 v1neurosynth 1\n",
      "\tskipping S04 v3_boostexamples_merged 0\n",
      "\tskipping S04 v3_boostexamples_merged 1\n",
      "\tskipping S04 v1neurosynth 0\n",
      "\tskipping S04 v1neurosynth 1\n",
      "weight_enet_mask 35 (606,)\n",
      "questions 35\n",
      "35 questions 35 weights\n",
      "saved S04 full_35.pkl\n",
      "weight_enet_mask 36 (607,)\n",
      "questions 36\n",
      "36 questions 36 weights\n",
      "saved S04 full_35_wordrate.pkl\n",
      "39 questions 39 weights\n",
      "saved S04 full_neurosynth.pkl\n",
      "40 questions 40 weights\n",
      "saved S04 full_neurosynth_wordrate.pkl\n",
      "weight_enet_mask 35 (606,)\n",
      "questions 35\n",
      "35 questions 35 weights\n",
      "saved S04 full_35_pc.pkl\n",
      "weight_enet_mask 36 (607,)\n",
      "questions 36\n",
      "36 questions 36 weights\n",
      "saved S04 full_35_wordrate_pc.pkl\n",
      "39 questions 39 weights\n",
      "saved S04 full_neurosynth_pc.pkl\n",
      "40 questions 40 weights\n",
      "saved S04 full_neurosynth_wordrate_pc.pkl\n",
      "shapes (268, 70) (1676, 70)\n",
      "\tskipping S05 v3_boostexamples_merged 0\n",
      "\tskipping S05 v3_boostexamples_merged 1\n",
      "\tskipping S05 v1neurosynth 0\n",
      "\tskipping S05 v1neurosynth 1\n",
      "\tskipping S05 v3_boostexamples_merged 0\n",
      "\tskipping S05 v3_boostexamples_merged 1\n",
      "\tskipping S05 v1neurosynth 0\n",
      "\tskipping S05 v1neurosynth 1\n",
      "weight_enet_mask 35 (606,)\n",
      "questions 35\n",
      "35 questions 35 weights\n",
      "saved S05 full_35.pkl\n",
      "weight_enet_mask 36 (607,)\n",
      "questions 36\n",
      "36 questions 36 weights\n",
      "saved S05 full_35_wordrate.pkl\n",
      "39 questions 39 weights\n",
      "saved S05 full_neurosynth.pkl\n",
      "40 questions 40 weights\n",
      "saved S05 full_neurosynth_wordrate.pkl\n",
      "weight_enet_mask 35 (606,)\n",
      "questions 35\n",
      "35 questions 35 weights\n",
      "saved S05 full_35_pc.pkl\n",
      "weight_enet_mask 36 (607,)\n",
      "questions 36\n",
      "36 questions 36 weights\n",
      "saved S05 full_35_wordrate_pc.pkl\n",
      "39 questions 39 weights\n",
      "saved S05 full_neurosynth_pc.pkl\n",
      "40 questions 40 weights\n",
      "saved S05 full_neurosynth_wordrate_pc.pkl\n",
      "shapes (268, 70) (1676, 70)\n",
      "\tskipping S06 v3_boostexamples_merged 0\n",
      "\tskipping S06 v3_boostexamples_merged 1\n",
      "\tskipping S06 v1neurosynth 0\n",
      "\tskipping S06 v1neurosynth 1\n",
      "\tskipping S06 v3_boostexamples_merged 0\n",
      "\tskipping S06 v3_boostexamples_merged 1\n",
      "\tskipping S06 v1neurosynth 0\n",
      "\tskipping S06 v1neurosynth 1\n",
      "weight_enet_mask 35 (606,)\n",
      "questions 35\n",
      "35 questions 35 weights\n",
      "saved S06 full_35.pkl\n",
      "weight_enet_mask 36 (607,)\n",
      "questions 36\n",
      "36 questions 36 weights\n",
      "saved S06 full_35_wordrate.pkl\n",
      "39 questions 39 weights\n",
      "saved S06 full_neurosynth.pkl\n",
      "40 questions 40 weights\n",
      "saved S06 full_neurosynth_wordrate.pkl\n",
      "weight_enet_mask 35 (606,)\n",
      "questions 35\n",
      "35 questions 35 weights\n",
      "saved S06 full_35_pc.pkl\n",
      "weight_enet_mask 36 (607,)\n",
      "questions 36\n",
      "36 questions 36 weights\n",
      "saved S06 full_35_wordrate_pc.pkl\n",
      "39 questions 39 weights\n",
      "saved S06 full_neurosynth_pc.pkl\n",
      "40 questions 40 weights\n",
      "saved S06 full_neurosynth_wordrate_pc.pkl\n",
      "shapes (268, 70) (1676, 70)\n",
      "\tskipping S07 v3_boostexamples_merged 0\n",
      "\tskipping S07 v3_boostexamples_merged 1\n",
      "\tskipping S07 v1neurosynth 0\n",
      "\tskipping S07 v1neurosynth 1\n",
      "\tskipping S07 v3_boostexamples_merged 0\n",
      "\tskipping S07 v3_boostexamples_merged 1\n",
      "\tskipping S07 v1neurosynth 0\n",
      "\tskipping S07 v1neurosynth 1\n",
      "weight_enet_mask 35 (606,)\n",
      "questions 35\n",
      "35 questions 35 weights\n",
      "saved S07 full_35.pkl\n",
      "weight_enet_mask 36 (607,)\n",
      "questions 36\n",
      "36 questions 36 weights\n",
      "saved S07 full_35_wordrate.pkl\n",
      "39 questions 39 weights\n",
      "saved S07 full_neurosynth.pkl\n",
      "40 questions 40 weights\n",
      "saved S07 full_neurosynth_wordrate.pkl\n",
      "weight_enet_mask 35 (606,)\n",
      "questions 35\n",
      "35 questions 35 weights\n",
      "saved S07 full_35_pc.pkl\n",
      "weight_enet_mask 36 (607,)\n",
      "questions 36\n",
      "36 questions 36 weights\n",
      "saved S07 full_35_wordrate_pc.pkl\n",
      "39 questions 39 weights\n",
      "saved S07 full_neurosynth_pc.pkl\n",
      "40 questions 40 weights\n",
      "saved S07 full_neurosynth_wordrate_pc.pkl\n",
      "shapes (268, 70) (1676, 70)\n",
      "\tskipping S08 v3_boostexamples_merged 0\n",
      "\tskipping S08 v3_boostexamples_merged 1\n",
      "\tskipping S08 v1neurosynth 0\n",
      "\tskipping S08 v1neurosynth 1\n",
      "\tskipping S08 v3_boostexamples_merged 0\n",
      "\tskipping S08 v3_boostexamples_merged 1\n",
      "\tskipping S08 v1neurosynth 0\n",
      "\tskipping S08 v1neurosynth 1\n",
      "weight_enet_mask 35 (606,)\n",
      "questions 35\n",
      "35 questions 35 weights\n",
      "saved S08 full_35.pkl\n",
      "weight_enet_mask 36 (607,)\n",
      "questions 36\n",
      "36 questions 36 weights\n",
      "saved S08 full_35_wordrate.pkl\n",
      "39 questions 39 weights\n",
      "saved S08 full_neurosynth.pkl\n",
      "40 questions 40 weights\n",
      "saved S08 full_neurosynth_wordrate.pkl\n",
      "weight_enet_mask 35 (606,)\n",
      "questions 35\n",
      "35 questions 35 weights\n",
      "saved S08 full_35_pc.pkl\n",
      "weight_enet_mask 36 (607,)\n",
      "questions 36\n",
      "36 questions 36 weights\n",
      "saved S08 full_35_wordrate_pc.pkl\n",
      "39 questions 39 weights\n",
      "saved S08 full_neurosynth_pc.pkl\n",
      "40 questions 40 weights\n",
      "saved S08 full_neurosynth_wordrate_pc.pkl\n"
     ]
    }
   ],
   "source": [
    "def save_df(df, subject, filename):\n",
    "    if df:\n",
    "        joblib.dump(df, join(PROCESSED_DIR, subject, filename))\n",
    "        print(f'saved {subject} {filename}')\n",
    "\n",
    "\n",
    "# subject = 'S02'\n",
    "# for subject in ['S02', 'S01', 'S03']:\n",
    "for subject in [f'S0{i}' for i in range(1, 9)]:\n",
    "    os.makedirs(join(PROCESSED_DIR, subject), exist_ok=True)\n",
    "\n",
    "    rr_8 = rr_all[rr_all.ndelays == 8]\n",
    "    rr_4 = rr_all[rr_all.ndelays == 4]\n",
    "    print('shapes', rr_8.shape, rr_4.shape)\n",
    "    for rr, suffix1 in zip([rr_8, rr_4], ['_ndel=8', '']):\n",
    "\n",
    "        # df = _load_coefs_individual_gpt4(rr_gpt4, subject=subject)\n",
    "        # joblib.dump(df, join(PROCESSED_DIR, subject, 'individual_gpt4.pkl'))\n",
    "\n",
    "        # df = _load_coefs_individual_gpt4(\n",
    "        #     rr_gpt4, subject='S02', use_added_wordrate_feature=1)\n",
    "        # joblib.dump(df, join(PROCESSED_DIR, subject,\n",
    "        #             'individual_gpt4_wordrate.pkl'))\n",
    "\n",
    "        # df = _load_coefs_individual(\n",
    "        # rr, subject=subject, qa_questions_version='v1neurosynth')\n",
    "        # joblib.dump(df, join(PROCESSED_DIR, subject, 'individual_neurosynth.pkl'))\n",
    "\n",
    "        # df = _load_coefs_individual(\n",
    "        #     rr, subject=subject, qa_questions_version='v3_boostexamples_merged')\n",
    "        # joblib.dump(df, join(PROCESSED_DIR, subject, 'individual_35.pkl'))\n",
    "\n",
    "        rr_nonpc = rr[rr.pc_components == -1]\n",
    "        rr_pc = rr[rr.pc_components != -1]\n",
    "\n",
    "        for r, suffix in zip([rr_nonpc, rr_pc], ['', '_pc']):\n",
    "\n",
    "            df = _load_coefs_full(\n",
    "                r, subject=subject, qa_questions_version='v3_boostexamples_merged')\n",
    "            save_df(df, subject, f'full_35{suffix1}{suffix}.pkl')\n",
    "\n",
    "            df = _load_coefs_full(\n",
    "                r, subject=subject, qa_questions_version='v3_boostexamples_merged', use_added_wordrate_feature=1)\n",
    "            save_df(df, subject, f'full_35_wordrate{suffix1}{suffix}.pkl')\n",
    "\n",
    "            df = _load_coefs_full(\n",
    "                r, subject=subject, qa_questions_version='v1neurosynth')\n",
    "            save_df(df, subject, f'full_neurosynth{suffix1}{suffix}.pkl')\n",
    "\n",
    "            df = _load_coefs_full(\n",
    "                r, subject=subject, qa_questions_version='v1neurosynth', use_added_wordrate_feature=1)\n",
    "            save_df(df, subject,\n",
    "                    f'full_neurosynth_wordrate{suffix1}{suffix}.pkl')\n",
    "\n",
    "        # df = _load_coefs_shapley(\n",
    "        #     rr, subject, qa_questions_version='v3_boostexamples_merged')\n",
    "        # joblib.dump(df, join(\n",
    "        #     PROCESSED_DIR, subject, 'shapley_35.pkl'))\n",
    "\n",
    "        # df = _load_coefs_shapley(\n",
    "        #     rr, subject, qa_questions_version='v1neurosynth')\n",
    "        # joblib.dump(df, join(PROCESSED_DIR,\n",
    "        #             subject, 'shapley_neurosynth.pkl'))\n",
    "\n",
    "        ########### use old models ###################\n",
    "        # jointly fitted 35-question model\n",
    "        # df_w_selected35 = _load_coefs_35questions(subject=subject)\n",
    "\n",
    "        # individually fitted question models\n",
    "        # df_w_individual = _load_coefs_individual(rr_shapley, subject=subject)\n",
    "        # joblib.dump(df_w_individual, join(PROCESSED_DIR,\n",
    "        # subject, 'individual.pkl'))\n",
    "\n",
    "        # individually fitted question models *with wordrate\n",
    "        # df_w_individual_wordrate = _load_coefs_individual_wordrate(\n",
    "        # subject=subject)\n",
    "\n",
    "        # wordrate\n",
    "        # df_w_wordrate_alone = _load_coefs_wordrate(subject=subject)\n",
    "\n",
    "        # # collate individual dfs #########################\n",
    "        # # average weights for df_w_selected35 and df_w_individual\n",
    "        # if subject == 'S02':\n",
    "        #     df_avg = df_w_selected35.merge(df_w_individual, on='question')\n",
    "        #     df_avg['weights'] = df_avg.apply(\n",
    "        #         lambda x: np.mean([x['weights_x'], x['weights_y']], axis=0), axis=1)\n",
    "\n",
    "        # df_avg_individual = df_w_individual.merge(\n",
    "        #     df_w_individual_wordrate, on='question')\n",
    "        # df_avg_individual['weights'] = df_avg_individual.apply(\n",
    "        #     lambda x: np.mean([x['weights_x'], x['weights_y']], axis=0), axis=1)\n",
    "\n",
    "        # df_qa_dict = {\n",
    "        #     'selected35': df_w_selected35,\n",
    "        #     'individual': df_w_individual,\n",
    "        #     'individual_wordrate': df_w_individual_wordrate,\n",
    "        #     'wordrate_alone': df_w_wordrate_alone,\n",
    "        #     # 'avg': df_avg,\n",
    "        #     'shapley_neurosynth': df_w_shapley_neurosynth,\n",
    "        #     'shapley35': df_w_shapley35,\n",
    "        #     'avg_individual': df_avg_individual\n",
    "        # }\n",
    "        # joblib.dump(df_qa_dict, f'df_qa_dict_{subject}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export eng1000 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = '/home/chansingh/mntv1/deep-fMRI/encoding/aug16_eng1000'\n",
    "rr, cols_varied, mets = analyze_helper.load_clean_results(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = rr[rr.feature_space == 'eng1000']\n",
    "r = r[r.num_stories == -1]\n",
    "r = r[r.ndelays == 8]\n",
    "r = r[r.feature_selection_alpha == -1.000000]\n",
    "for subject in ['S02', 'S01', 'S03']:\n",
    "    row = r[r.subject == subject]\n",
    "    assert len(row) == 1\n",
    "    row = row.iloc[0]\n",
    "    weights, weights_pc = flatmaps_per_question.get_weights_top(row)\n",
    "    print(weights.shape)\n",
    "    joblib.dump(weights, join(PROCESSED_DIR, subject, 'eng1000_weights.pkl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
