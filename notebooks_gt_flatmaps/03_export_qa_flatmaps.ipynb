{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from ridge_utils.DataSequence import DataSequence\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cortex\n",
    "import seaborn as sns\n",
    "from os.path import join\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import sasc.viz\n",
    "import joblib\n",
    "import imodelsx.process_results\n",
    "import dvu\n",
    "import neurosynth\n",
    "import sys\n",
    "from neuro.data.response_utils import load_response_huge\n",
    "from neuro.data.story_names import get_story_names\n",
    "import numpy as np\n",
    "import joblib\n",
    "from os.path import join\n",
    "sys.path.append('../notebooks')\n",
    "from tqdm import tqdm\n",
    "from sasc.config import FMRI_DIR, STORIES_DIR, RESULTS_DIR\n",
    "from neuro.config import repo_dir, PROCESSED_DIR\n",
    "from neuro import analyze_helper, viz\n",
    "from copy import deepcopy\n",
    "# from neuro.features.qa_questions import get_questions, get_merged_questions_v3_boostexamples\n",
    "flatmaps_per_question = __import__('06_flatmaps_per_question')\n",
    "# from neurosynth import term_dict, term_dict_rev\n",
    "# import viz\n",
    "from load_coef_flatmaps import _load_coefs_individual, _load_coefs_full, \\\n",
    "_load_coefs_individual_wordrate, _load_coefs_wordrate, _load_coefs_shapley, _load_coefs_individual_gpt4\n",
    "# imodelsx.process_results.delete_runs_in_dataframe(\n",
    "    # rr[rr.use_added_wordrate_feature == 1], actually_delete=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_dir = '/home/chansingh/mntv1/deep-fMRI/encoding/aug14_neurosynth_gemv'\n",
    "results_dir = '/home/chansingh/mntv1/deep-fMRI/encoding/oct17_neurosynth_gemv'\n",
    "rr, cols_varied, mets = analyze_helper.load_clean_results(results_dir)\n",
    "imodelsx.process_results.delete_runs_in_dataframe(\n",
    "    rr[rr.use_test_setup == 1], actually_delete=True)\n",
    "rr.to_pickle('oct17_tmp.pkl')\n",
    "\n",
    "# # # this has the most recent result\n",
    "# results_dir = '/home/chansingh/mntv1/deep-fMRI/encoding/aug16_gpt4'\n",
    "# rr_gpt4, cols_varied, mets = analyze_helper.load_clean_results(results_dir)\n",
    "# rr_gpt4 = rr_gpt4[rr_gpt4.use_test_setup == 0]\n",
    "# imodelsx.process_results.delete_runs_in_dataframe(\n",
    "#     rr_gpt4[rr_gpt4.use_test_setup == 1], actually_delete=True)\n",
    "# # rr_gpt4.use_added_wordrate_feature.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_all = pd.read_pickle('oct17_tmp.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df(df, subject, filename):\n",
    "    if df:\n",
    "        joblib.dump(df, join(PROCESSED_DIR, subject, filename))\n",
    "        print(f'saved {subject} {filename}')\n",
    "\n",
    "\n",
    "# subject = 'S02'\n",
    "# for subject in ['S01', 'S02', 'S03']:\n",
    "# for subject in ['S02', 'S01', 'S03']:\n",
    "for subject in [f'S0{i}' for i in range(1, 9)]:\n",
    "    os.makedirs(join(PROCESSED_DIR, subject), exist_ok=True)\n",
    "\n",
    "    # print('shapes', rr_8.shape, rr_4.shape)\n",
    "    for ndelays, suffix1 in zip([16, 8, 4, 1], ['_ndel=16', '_ndel=8', '', '_ndel=1']):\n",
    "        # for ndelays, suffix1 in zip([8, 4, 1], ['_ndel=8', '', '_ndel=1']):\n",
    "        rr = rr_all[rr_all.ndelays == ndelays]\n",
    "        if len(rr) == 0:\n",
    "            continue\n",
    "\n",
    "        for pc_components, suffix in zip([-1, 100], ['', '_pc']):\n",
    "            # for pc_components, suffix in zip([100], ['_pc']):\n",
    "            r = rr[rr.pc_components == pc_components]\n",
    "            if len(r) == 0:\n",
    "                continue\n",
    "            # print('shape', r.shape)\n",
    "\n",
    "            r_gpt = r[r.qa_embedding_model == 'gpt4']\n",
    "            suff = f'{suffix1}{suffix}_new'\n",
    "            df, corrs_test_dict = _load_coefs_individual_gpt4(\n",
    "                r_gpt, subject=subject, avg_over_delays=True)\n",
    "            save_df(df, subject, f'individual_gpt4{suff}.pkl')\n",
    "            joblib.dump(corrs_test_dict, join(PROCESSED_DIR,\n",
    "                                              subject, f'corrs_test_individual_gpt4_qs_35{suff}.pkl'))\n",
    "            df, corrs_test_dict = _load_coefs_individual_gpt4(\n",
    "                r_gpt, subject=subject, avg_over_delays=False)\n",
    "            save_df(df, subject, f'individual_gpt4{suff}_all_delays.pkl')\n",
    "            # r_gpt = r[r.qa_embedding_model == 'gpt4']\n",
    "            # df = _load_coefs_individual_gpt4(r_gpt, subject=subject)\n",
    "            # save_df(df, subject, f'individual_gpt4{suffix1}{suffix}.pkl')\n",
    "\n",
    "            # df = _load_coefs_full(r_gpt, subject='S02', qa_questions_version='qs_35',\n",
    "            #                       use_added_wordrate_feature=0)\n",
    "            # save_df(df, subject, f'full_35_gpt4{suffix1}{suffix}.pkl')\n",
    "            # df = _load_coefs_shapley(\n",
    "            #     r_gpt, subject, qa_questions_version='qs_35')\n",
    "            # save_df(df, subject, f'shapley_35_gpt4{suffix1}{suffix}.pkl')\n",
    "\n",
    "            # r = r[r.qa_embedding_model == 'ensemble2']\n",
    "            # df = _load_coefs_full(\n",
    "            #     r, subject=subject, qa_questions_version='v3_boostexamples_merged')\n",
    "            # save_df(df, subject, f'full_35{suffix1}{suffix}.pkl')\n",
    "            # df = _load_coefs_full(\n",
    "            #     r, subject=subject, qa_questions_version='v3_boostexamples_merged', use_added_wordrate_feature=1)\n",
    "            # save_df(df, subject, f'full_35_wordrate{suffix1}{suffix}.pkl')\n",
    "            # df = _load_coefs_full(\n",
    "            #     r, subject=subject, qa_questions_version='v1neurosynth')\n",
    "            # save_df(df, subject, f'full_neurosynth{suffix1}{suffix}.pkl')\n",
    "            # df = _load_coefs_full(\n",
    "            #     r, subject=subject, qa_questions_version='v1neurosynth', use_added_wordrate_feature=1)\n",
    "            # save_df(df, subject,\n",
    "            #         f'full_neurosynth_wordrate{suffix1}{suffix}.pkl')\n",
    "\n",
    "        # OLD STUFF\n",
    "        # df = _load_coefs_individual_gpt4(rr_gpt4, subject=subject)\n",
    "        # joblib.dump(df, join(PROCESSED_DIR, subject, 'individual_gpt4.pkl'))\n",
    "\n",
    "        # df = _load_coefs_individual_gpt4(\n",
    "        #     rr_gpt4, subject='S02', use_added_wordrate_feature=1)\n",
    "        # joblib.dump(df, join(PROCESSED_DIR, subject,\n",
    "        #             'individual_gpt4_wordrate.pkl'))\n",
    "\n",
    "        # df = _load_coefs_individual(\n",
    "        # rr, subject=subject, qa_questions_version='v1neurosynth')\n",
    "        # joblib.dump(df, join(PROCESSED_DIR, subject, 'individual_neurosynth.pkl'))\n",
    "\n",
    "        # df = _load_coefs_individual(\n",
    "        #     rr, subject=subject, qa_questions_version='v3_boostexamples_merged')\n",
    "        # joblib.dump(df, join(PROCESSED_DIR, subject, 'individual_35.pkl'))\n",
    "\n",
    "        # df = _load_coefs_shapley(\n",
    "        #     rr, subject, qa_questions_version='v3_boostexamples_merged')\n",
    "        # joblib.dump(df, join(\n",
    "        #     PROCESSED_DIR, subject, 'shapley_35.pkl'))\n",
    "\n",
    "        # df = _load_coefs_shapley(\n",
    "        #     rr, subject, qa_questions_version='v1neurosynth')\n",
    "        # joblib.dump(df, join(PROCESSED_DIR,\n",
    "        #             subject, 'shapley_neurosynth.pkl'))\n",
    "\n",
    "        ########### use old models ###################\n",
    "        # jointly fitted 35-question model\n",
    "        # df_w_selected35 = _load_coefs_35questions(subject=subject)\n",
    "\n",
    "        # individually fitted question models\n",
    "        # df_w_individual = _load_coefs_individual(rr_shapley, subject=subject)\n",
    "        # joblib.dump(df_w_individual, join(PROCESSED_DIR,\n",
    "        # subject, 'individual.pkl'))\n",
    "\n",
    "        # individually fitted question models *with wordrate\n",
    "        # df_w_individual_wordrate = _load_coefs_individual_wordrate(\n",
    "        # subject=subject)\n",
    "\n",
    "        # wordrate\n",
    "        # df_w_wordrate_alone = _load_coefs_wordrate(subject=subject)\n",
    "\n",
    "        # # collate individual dfs #########################\n",
    "        # # average weights for df_w_selected35 and df_w_individual\n",
    "        # if subject == 'S02':\n",
    "        #     df_avg = df_w_selected35.merge(df_w_individual, on='question')\n",
    "        #     df_avg['weights'] = df_avg.apply(\n",
    "        #         lambda x: np.mean([x['weights_x'], x['weights_y']], axis=0), axis=1)\n",
    "\n",
    "        # df_avg_individual = df_w_individual.merge(\n",
    "        #     df_w_individual_wordrate, on='question')\n",
    "        # df_avg_individual['weights'] = df_avg_individual.apply(\n",
    "        #     lambda x: np.mean([x['weights_x'], x['weights_y']], axis=0), axis=1)\n",
    "\n",
    "        # df_qa_dict = {\n",
    "        #     'selected35': df_w_selected35,\n",
    "        #     'individual': df_w_individual,\n",
    "        #     'individual_wordrate': df_w_individual_wordrate,\n",
    "        #     'wordrate_alone': df_w_wordrate_alone,\n",
    "        #     # 'avg': df_avg,\n",
    "        #     'shapley_neurosynth': df_w_shapley_neurosynth,\n",
    "        #     'shapley35': df_w_shapley35,\n",
    "        #     'avg_individual': df_avg_individual\n",
    "        # }\n",
    "        # joblib.dump(df_qa_dict, f'df_qa_dict_{subject}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export eng1000 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = '/home/chansingh/mntv1/deep-fMRI/encoding/aug16_eng1000'\n",
    "rr, cols_varied, mets = analyze_helper.load_clean_results(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = rr[rr.feature_space == 'eng1000']\n",
    "r = r[r.num_stories == -1]\n",
    "r = r[r.ndelays == 8]\n",
    "r = r[r.feature_selection_alpha == -1.000000]\n",
    "for subject in ['S02', 'S01', 'S03']:\n",
    "    row = r[r.subject == subject]\n",
    "    assert len(row) == 1\n",
    "    row = row.iloc[0]\n",
    "    weights, weights_pc = flatmaps_per_question.get_weights_top(row)\n",
    "    print(weights.shape)\n",
    "    joblib.dump(weights, join(PROCESSED_DIR, subject, 'eng1000_weights.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export resp chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in ['UTS01', 'UTS02', 'UTS03']:\n",
    "    story_names = get_story_names(\n",
    "        subject, train_or_test=\"train\", use_huge=True)\n",
    "    resps = load_response_huge(story_names, subject=subject)\n",
    "    # resps = joblib.load(join(config.root_dir, 'data',\n",
    "    #  'huge_data', f'{subject}_responses  .jbl'))\n",
    "    # return np.vstack([resps[story] for story in stories])\n",
    "    chunks = []\n",
    "    n = 1\n",
    "    for i in np.arange(0, resps.shape[0], n):\n",
    "        chunk = resps[i:i + n].mean(axis=0)\n",
    "        chunks.append(chunk)\n",
    "        if len(chunks) >= 2000:\n",
    "            break\n",
    "    joblib.dump(chunks, join(PROCESSED_DIR, subject.replace(\n",
    "        'UT', ''), f'resp_chunks_{n}trs.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in ['UTS01', 'UTS02', 'UTS03']:\n",
    "    flatmaps_null = np.array(joblib.load(\n",
    "        join(PROCESSED_DIR, subject.replace('UT', ''), 'resp_chunks_1trs.pkl')))\n",
    "    subj_mapper, fs_mapper, (ltrans, rtrans) = neurosynth.subj_vol_to_mni_surf_setup(\n",
    "        subject)\n",
    "    resp_chunks_1trs_MNI = [\n",
    "        neurosynth.subj_vol_to_mni_surf(\n",
    "            subj_vol=cortex.Volume(\n",
    "                a, 'UT' + subject, xfmname=f\"UT{subject}_auto\"),\n",
    "            subject=subject,\n",
    "            cached_tuple=(subj_mapper, fs_mapper, (ltrans, rtrans)),\n",
    "        ).data\n",
    "        for a in tqdm(flatmaps_null)\n",
    "    ]\n",
    "    joblib.dump(resp_chunks_1trs_MNI, join(PROCESSED_DIR,\n",
    "                subject.replace('UT', ''), 'resp_chunks_1trs_MNI.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrs = []\n",
    "for subject in ['UTS01', 'UTS02', 'UTS03']:\n",
    "    arrs.append(np.array(joblib.load(\n",
    "        join(PROCESSED_DIR, subject.replace('UT', ''), 'resp_chunks_1trs_MNI.pkl'))))\n",
    "arr_mean = np.mean(np.array(arrs), axis=0).squeeze()\n",
    "np.savez_compressed(\n",
    "    join(PROCESSED_DIR, 'mean_resp_chunks_1trs_MNI.npz'), arr_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load with np.load('mean_resp_chunks_1trs_MNI.npz')['arr_0'] and will get arr with shape=(2000, 91, 109, 91)\n",
    "!rclone copy /home/chansingh/fmri/qa_results/processed/mean_resp_chunks_1trs_MNI.npz box:DeepTune/QA/flatmaps_mni --progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export corrs MNI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in ['S01', 'S02', 'S03']:\n",
    "    corrs_test = joblib.load(join(PROCESSED_DIR, subject.replace(\n",
    "        'UT', ''), 'corrs_test_35.pkl')).values[0]\n",
    "\n",
    "    subj_mapper, fs_mapper, (ltrans, rtrans) = neurosynth.subj_vol_to_mni_surf_setup(\n",
    "        subject.replace('UT', ''))\n",
    "    corrs_test_MNI = neurosynth.subj_vol_to_mni_surf(\n",
    "        subj_vol=cortex.Volume(\n",
    "            corrs_test, 'UT' + subject, xfmname=f\"UT{subject}_auto\"),\n",
    "        subject=subject,\n",
    "        cached_tuple=(subj_mapper, fs_mapper, (ltrans, rtrans)),\n",
    "    ).data\n",
    "    joblib.dump(corrs_test_MNI, join(PROCESSED_DIR,\n",
    "                subject.replace('UT', ''), 'corrs_test_35_MNI.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrs = []\n",
    "for subject in ['UTS01', 'UTS02', 'UTS03']:\n",
    "    arrs.append(np.array(joblib.load(\n",
    "        join(PROCESSED_DIR, subject.replace('UT', ''), 'corrs_test_35_MNI.pkl'))))\n",
    "arr_mean = np.mean(np.array(arrs), axis=0).squeeze()\n",
    "np.savez_compressed(\n",
    "    join(PROCESSED_DIR, 'mean_corrs_test_35_MNI.npz'), arr_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load with np.load('mean_resp_chunks_1trs_MNI.npz')['arr_0'] and will get arr with shape=(2000, 91, 109, 91)\n",
    "!rclone copy /home/chansingh/fmri/qa_results/processed/mean_corrs_test_35_MNI.npz box:DeepTune/QA/flatmaps_mni --progress"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
