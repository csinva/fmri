{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from ridge_utils.DataSequence import DataSequence\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cortex\n",
    "import seaborn as sns\n",
    "from os.path import join\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import sasc.viz\n",
    "import joblib\n",
    "import imodelsx.process_results\n",
    "import dvu\n",
    "import sys\n",
    "sys.path.append('../notebooks')\n",
    "from tqdm import tqdm\n",
    "from sasc.config import FMRI_DIR, STORIES_DIR, RESULTS_DIR\n",
    "from neuro.config import repo_dir, PROCESSED_DIR\n",
    "from neuro import analyze_helper, viz\n",
    "# from neuro.features.qa_questions import get_questions, get_merged_questions_v3_boostexamples\n",
    "flatmaps_per_question = __import__('06_flatmaps_per_question')\n",
    "# from neurosynth import term_dict, term_dict_rev\n",
    "# import viz\n",
    "from load_coef_flatmaps import _load_coefs_individual, _load_coefs_full, \\\n",
    "_load_coefs_individual_wordrate, _load_coefs_wordrate, _load_coefs_shapley, _load_coefs_individual_gpt4\n",
    "# imodelsx.process_results.delete_runs_in_dataframe(\n",
    "    # rr[rr.use_added_wordrate_feature == 1], actually_delete=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_dir = '/home/chansingh/mntv1/deep-fMRI/encoding/aug14_neurosynth_gemv'\n",
    "results_dir = '/home/chansingh/mntv1/deep-fMRI/encoding/oct17_neurosynth_gemv'\n",
    "rr, cols_varied, mets = analyze_helper.load_clean_results(results_dir)\n",
    "imodelsx.process_results.delete_runs_in_dataframe(\n",
    "    rr[rr.use_test_setup == 1], actually_delete=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this has the most recent result\n",
    "# results_dir = '/home/chansingh/mntv1/deep-fMRI/encoding/aug16_gpt4'\n",
    "# rr_gpt4, cols_varied, mets = analyze_helper.load_clean_results(results_dir)\n",
    "# rr_gpt4 = rr_gpt4[rr_gpt4.use_test_setup == 0]\n",
    "# imodelsx.process_results.delete_runs_in_dataframe(\n",
    "#     rr_gpt4[rr_gpt4.use_test_setup == 1], actually_delete=True)\n",
    "rr_gpt4.use_added_wordrate_feature.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = pd.read_pickle('oct17_tmp.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_nonpc = rr[rr.pc_components == -1]\n",
    "rr_pc = rr[rr.pc_components != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject = 'S02'\n",
    "# for subject in ['S02', 'S01', 'S03']:\n",
    "for subject in [f'S0{i}' for i in range(1, 9)]:\n",
    "    os.makedirs(join(PROCESSED_DIR, subject), exist_ok=True)\n",
    "\n",
    "    # df = _load_coefs_individual_gpt4(rr_gpt4, subject=subject)\n",
    "    # joblib.dump(df, join(PROCESSED_DIR, subject, 'individual_gpt4.pkl'))\n",
    "\n",
    "    # df = _load_coefs_individual_gpt4(\n",
    "    #     rr_gpt4, subject='S02', use_added_wordrate_feature=1)\n",
    "    # joblib.dump(df, join(PROCESSED_DIR, subject,\n",
    "    #             'individual_gpt4_wordrate.pkl'))\n",
    "\n",
    "    # df = _load_coefs_individual(\n",
    "    # rr, subject=subject, qa_questions_version='v1neurosynth')\n",
    "    # joblib.dump(df, join(PROCESSED_DIR, subject, 'individual_neurosynth.pkl'))\n",
    "\n",
    "    # df = _load_coefs_individual(\n",
    "    #     rr, subject=subject, qa_questions_version='v3_boostexamples_merged')\n",
    "    # joblib.dump(df, join(PROCESSED_DIR, subject, 'individual_35.pkl'))\n",
    "\n",
    "    for r, suffix in zip([rr_nonpc, rr_pc], ['', '_pc']):\n",
    "        # for r, suffix in zip([rr], ['']):\n",
    "\n",
    "        df = _load_coefs_full(\n",
    "            r, subject=subject, qa_questions_version='v3_boostexamples_merged')\n",
    "        joblib.dump(df, join(PROCESSED_DIR, subject, f'full_35{suffix}.pkl'))\n",
    "\n",
    "        df = _load_coefs_full(\n",
    "            r, subject=subject, qa_questions_version='v3_boostexamples_merged', use_added_wordrate_feature=1)\n",
    "        joblib.dump(df, join(PROCESSED_DIR, subject,\n",
    "                    f'full_35_wordrate{suffix}.pkl'))\n",
    "\n",
    "        df = _load_coefs_full(\n",
    "            r, subject=subject, qa_questions_version='v1neurosynth')\n",
    "        joblib.dump(df, join(PROCESSED_DIR, subject,\n",
    "                    f'full_neurosynth{suffix}.pkl'))\n",
    "\n",
    "        df = _load_coefs_full(\n",
    "            r, subject=subject, qa_questions_version='v1neurosynth', use_added_wordrate_feature=1)\n",
    "        joblib.dump(df, join(PROCESSED_DIR, subject,\n",
    "                             f'full_neurosynth_wordrate{suffix}.pkl'))\n",
    "\n",
    "    # df = _load_coefs_shapley(\n",
    "    #     rr, subject, qa_questions_version='v3_boostexamples_merged')\n",
    "    # joblib.dump(df, join(\n",
    "    #     PROCESSED_DIR, subject, 'shapley_35.pkl'))\n",
    "\n",
    "    # df = _load_coefs_shapley(\n",
    "    #     rr, subject, qa_questions_version='v1neurosynth')\n",
    "    # joblib.dump(df, join(PROCESSED_DIR,\n",
    "    #             subject, 'shapley_neurosynth.pkl'))\n",
    "\n",
    "    ########### use old models ###################\n",
    "    # jointly fitted 35-question model\n",
    "    # df_w_selected35 = _load_coefs_35questions(subject=subject)\n",
    "\n",
    "    # individually fitted question models\n",
    "    # df_w_individual = _load_coefs_individual(rr_shapley, subject=subject)\n",
    "    # joblib.dump(df_w_individual, join(PROCESSED_DIR,\n",
    "    # subject, 'individual.pkl'))\n",
    "\n",
    "    # individually fitted question models *with wordrate\n",
    "    # df_w_individual_wordrate = _load_coefs_individual_wordrate(\n",
    "    # subject=subject)\n",
    "\n",
    "    # wordrate\n",
    "    # df_w_wordrate_alone = _load_coefs_wordrate(subject=subject)\n",
    "\n",
    "    # # collate individual dfs #########################\n",
    "    # # average weights for df_w_selected35 and df_w_individual\n",
    "    # if subject == 'S02':\n",
    "    #     df_avg = df_w_selected35.merge(df_w_individual, on='question')\n",
    "    #     df_avg['weights'] = df_avg.apply(\n",
    "    #         lambda x: np.mean([x['weights_x'], x['weights_y']], axis=0), axis=1)\n",
    "\n",
    "    # df_avg_individual = df_w_individual.merge(\n",
    "    #     df_w_individual_wordrate, on='question')\n",
    "    # df_avg_individual['weights'] = df_avg_individual.apply(\n",
    "    #     lambda x: np.mean([x['weights_x'], x['weights_y']], axis=0), axis=1)\n",
    "\n",
    "    # df_qa_dict = {\n",
    "    #     'selected35': df_w_selected35,\n",
    "    #     'individual': df_w_individual,\n",
    "    #     'individual_wordrate': df_w_individual_wordrate,\n",
    "    #     'wordrate_alone': df_w_wordrate_alone,\n",
    "    #     # 'avg': df_avg,\n",
    "    #     'shapley_neurosynth': df_w_shapley_neurosynth,\n",
    "    #     'shapley35': df_w_shapley35,\n",
    "    #     'avg_individual': df_avg_individual\n",
    "    # }\n",
    "    # joblib.dump(df_qa_dict, f'df_qa_dict_{subject}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export eng1000 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = '/home/chansingh/mntv1/deep-fMRI/encoding/aug16_eng1000'\n",
    "rr, cols_varied, mets = analyze_helper.load_clean_results(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = rr[rr.feature_space == 'eng1000']\n",
    "r = r[r.num_stories == -1]\n",
    "r = r[r.ndelays == 8]\n",
    "r = r[r.feature_selection_alpha == -1.000000]\n",
    "for subject in ['S02', 'S01', 'S03']:\n",
    "    row = r[r.subject == subject]\n",
    "    assert len(row) == 1\n",
    "    row = row.iloc[0]\n",
    "    weights, weights_pc = flatmaps_per_question.get_weights_top(row)\n",
    "    print(weights.shape)\n",
    "    joblib.dump(weights, join(PROCESSED_DIR, subject, 'eng1000_weights.pkl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
