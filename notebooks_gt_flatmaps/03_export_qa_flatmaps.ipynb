{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from ridge_utils.DataSequence import DataSequence\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cortex\n",
    "import seaborn as sns\n",
    "from os.path import join\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import sasc.viz\n",
    "import joblib\n",
    "import imodelsx.process_results\n",
    "import dvu\n",
    "import sys\n",
    "sys.path.append('../notebooks')\n",
    "from tqdm import tqdm\n",
    "from sasc.config import FMRI_DIR, STORIES_DIR, RESULTS_DIR\n",
    "from neuro.config import repo_dir, PROCESSED_DIR\n",
    "from neuro import analyze_helper, viz\n",
    "# from neuro.features.qa_questions import get_questions, get_merged_questions_v3_boostexamples\n",
    "flatmaps_per_question = __import__('06_flatmaps_per_question')\n",
    "# from neurosynth import term_dict, term_dict_rev\n",
    "# import viz\n",
    "from load_coef_flatmaps import _load_coefs_individual, _load_coefs_full, \\\n",
    "_load_coefs_individual_wordrate, _load_coefs_wordrate, _load_coefs_shapley, _load_coefs_individual_gpt4\n",
    "# imodelsx.process_results.delete_runs_in_dataframe(\n",
    "    # rr[rr.use_added_wordrate_feature == 1], actually_delete=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2157/2157 [30:31<00:00,  1.18it/s]\n",
      "/home/chansingh/imodelsx/imodelsx/process_results.py:92: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[k] = df[k].fillna(np.nan)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment varied these params: ['subject', 'pc_components', 'feature_selection_alpha', 'feature_selection_stability_seeds', 'use_added_wordrate_feature', 'qa_embedding_model', 'qa_questions_version', 'use_random_subset_features', 'single_question_idx', 'ndelays', 'seed']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 0/0 directories.\n"
     ]
    }
   ],
   "source": [
    "# results_dir = '/home/chansingh/mntv1/deep-fMRI/encoding/aug14_neurosynth_gemv'\n",
    "results_dir = '/home/chansingh/mntv1/deep-fMRI/encoding/oct17_neurosynth_gemv'\n",
    "rr, cols_varied, mets = analyze_helper.load_clean_results(results_dir)\n",
    "imodelsx.process_results.delete_runs_in_dataframe(\n",
    "    rr[rr.use_test_setup == 1], actually_delete=True)\n",
    "rr.to_pickle('oct17_tmp.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_all = pd.read_pickle('oct17_tmp.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 172/172 [00:18<00:00,  9.15it/s]\n",
      "/home/chansingh/imodelsx/imodelsx/process_results.py:92: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[k] = df[k].fillna(np.nan)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment varied these params: ['use_added_wordrate_feature', 'qa_questions_version']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 0/0 directories.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # this has the most recent result\n",
    "results_dir = '/home/chansingh/mntv1/deep-fMRI/encoding/aug16_gpt4'\n",
    "rr_gpt4, cols_varied, mets = analyze_helper.load_clean_results(results_dir)\n",
    "rr_gpt4 = rr_gpt4[rr_gpt4.use_test_setup == 0]\n",
    "imodelsx.process_results.delete_runs_in_dataframe(\n",
    "    rr_gpt4[rr_gpt4.use_test_setup == 1], actually_delete=True)\n",
    "# rr_gpt4.use_added_wordrate_feature.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 35 questions 35 weights\n",
      "corr 0.09298482340272012\n",
      "saved S01 full_35_gpt4_pc.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 35/35 [01:43<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved S02 individual_gpt4_pc.pkl\n",
      "\t 35 questions 35 weights\n",
      "corr 0.09298482340272012\n",
      "saved S02 full_35_gpt4_pc.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:53<00:00,  3.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved S02 shapley_35_gpt4_pc.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 35 questions 35 weights\n",
      "corr 0.09298482340272012\n",
      "saved S03 full_35_gpt4_pc.pkl\n"
     ]
    }
   ],
   "source": [
    "def save_df(df, subject, filename):\n",
    "    if df:\n",
    "        joblib.dump(df, join(PROCESSED_DIR, subject, filename))\n",
    "        print(f'saved {subject} {filename}')\n",
    "\n",
    "\n",
    "# subject = 'S02'\n",
    "for subject in ['S01', 'S02', 'S03']:\n",
    "    # for subject in ['S02', 'S01', 'S03']:\n",
    "    # for subject in [f'S0{i}' for i in range(1, 9)]:\n",
    "    os.makedirs(join(PROCESSED_DIR, subject), exist_ok=True)\n",
    "\n",
    "    # print('shapes', rr_8.shape, rr_4.shape)\n",
    "    for ndelays, suffix1 in zip([4], ['']):\n",
    "        # for ndelays, suffix1 in zip([8, 4, 1], ['_ndel=8', '', '_ndel=1']):\n",
    "        rr = rr_all[rr_all.ndelays == ndelays]\n",
    "        if len(rr) == 0:\n",
    "            continue\n",
    "\n",
    "        for pc_components, suffix in zip([-1, 100], ['', '_pc']):\n",
    "            r = rr[rr.pc_components == pc_components]\n",
    "            if len(r) == 0:\n",
    "                continue\n",
    "\n",
    "            r_gpt = r[r.qa_embedding_model == 'gpt4']\n",
    "            df = _load_coefs_individual_gpt4(r_gpt, subject=subject)\n",
    "            save_df(df, subject, f'individual_gpt4{suffix1}{suffix}.pkl')\n",
    "            df = _load_coefs_full(r_gpt, subject='S02', qa_questions_version='qs_35',\n",
    "                                  use_added_wordrate_feature=0)\n",
    "            save_df(df, subject, f'full_35_gpt4{suffix1}{suffix}.pkl')\n",
    "            df = _load_coefs_shapley(\n",
    "                r_gpt, subject, qa_questions_version='qs_35')\n",
    "            save_df(df, subject, f'shapley_35_gpt4{suffix1}{suffix}.pkl')\n",
    "\n",
    "            # r = r[r.qa_embedding_model == 'ensemble2']\n",
    "            # df = _load_coefs_full(\n",
    "            #     r, subject=subject, qa_questions_version='v3_boostexamples_merged')\n",
    "            # save_df(df, subject, f'full_35{suffix1}{suffix}.pkl')\n",
    "            # df = _load_coefs_full(\n",
    "            #     r, subject=subject, qa_questions_version='v3_boostexamples_merged', use_added_wordrate_feature=1)\n",
    "            # save_df(df, subject, f'full_35_wordrate{suffix1}{suffix}.pkl')\n",
    "            # df = _load_coefs_full(\n",
    "            #     r, subject=subject, qa_questions_version='v1neurosynth')\n",
    "            # save_df(df, subject, f'full_neurosynth{suffix1}{suffix}.pkl')\n",
    "            # df = _load_coefs_full(\n",
    "            #     r, subject=subject, qa_questions_version='v1neurosynth', use_added_wordrate_feature=1)\n",
    "            # save_df(df, subject,\n",
    "            #         f'full_neurosynth_wordrate{suffix1}{suffix}.pkl')\n",
    "\n",
    "        # OLD STUFF\n",
    "        # df = _load_coefs_individual_gpt4(rr_gpt4, subject=subject)\n",
    "        # joblib.dump(df, join(PROCESSED_DIR, subject, 'individual_gpt4.pkl'))\n",
    "\n",
    "        # df = _load_coefs_individual_gpt4(\n",
    "        #     rr_gpt4, subject='S02', use_added_wordrate_feature=1)\n",
    "        # joblib.dump(df, join(PROCESSED_DIR, subject,\n",
    "        #             'individual_gpt4_wordrate.pkl'))\n",
    "\n",
    "        # df = _load_coefs_individual(\n",
    "        # rr, subject=subject, qa_questions_version='v1neurosynth')\n",
    "        # joblib.dump(df, join(PROCESSED_DIR, subject, 'individual_neurosynth.pkl'))\n",
    "\n",
    "        # df = _load_coefs_individual(\n",
    "        #     rr, subject=subject, qa_questions_version='v3_boostexamples_merged')\n",
    "        # joblib.dump(df, join(PROCESSED_DIR, subject, 'individual_35.pkl'))\n",
    "\n",
    "        # df = _load_coefs_shapley(\n",
    "        #     rr, subject, qa_questions_version='v3_boostexamples_merged')\n",
    "        # joblib.dump(df, join(\n",
    "        #     PROCESSED_DIR, subject, 'shapley_35.pkl'))\n",
    "\n",
    "        # df = _load_coefs_shapley(\n",
    "        #     rr, subject, qa_questions_version='v1neurosynth')\n",
    "        # joblib.dump(df, join(PROCESSED_DIR,\n",
    "        #             subject, 'shapley_neurosynth.pkl'))\n",
    "\n",
    "        ########### use old models ###################\n",
    "        # jointly fitted 35-question model\n",
    "        # df_w_selected35 = _load_coefs_35questions(subject=subject)\n",
    "\n",
    "        # individually fitted question models\n",
    "        # df_w_individual = _load_coefs_individual(rr_shapley, subject=subject)\n",
    "        # joblib.dump(df_w_individual, join(PROCESSED_DIR,\n",
    "        # subject, 'individual.pkl'))\n",
    "\n",
    "        # individually fitted question models *with wordrate\n",
    "        # df_w_individual_wordrate = _load_coefs_individual_wordrate(\n",
    "        # subject=subject)\n",
    "\n",
    "        # wordrate\n",
    "        # df_w_wordrate_alone = _load_coefs_wordrate(subject=subject)\n",
    "\n",
    "        # # collate individual dfs #########################\n",
    "        # # average weights for df_w_selected35 and df_w_individual\n",
    "        # if subject == 'S02':\n",
    "        #     df_avg = df_w_selected35.merge(df_w_individual, on='question')\n",
    "        #     df_avg['weights'] = df_avg.apply(\n",
    "        #         lambda x: np.mean([x['weights_x'], x['weights_y']], axis=0), axis=1)\n",
    "\n",
    "        # df_avg_individual = df_w_individual.merge(\n",
    "        #     df_w_individual_wordrate, on='question')\n",
    "        # df_avg_individual['weights'] = df_avg_individual.apply(\n",
    "        #     lambda x: np.mean([x['weights_x'], x['weights_y']], axis=0), axis=1)\n",
    "\n",
    "        # df_qa_dict = {\n",
    "        #     'selected35': df_w_selected35,\n",
    "        #     'individual': df_w_individual,\n",
    "        #     'individual_wordrate': df_w_individual_wordrate,\n",
    "        #     'wordrate_alone': df_w_wordrate_alone,\n",
    "        #     # 'avg': df_avg,\n",
    "        #     'shapley_neurosynth': df_w_shapley_neurosynth,\n",
    "        #     'shapley35': df_w_shapley35,\n",
    "        #     'avg_individual': df_avg_individual\n",
    "        # }\n",
    "        # joblib.dump(df_qa_dict, f'df_qa_dict_{subject}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export eng1000 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = '/home/chansingh/mntv1/deep-fMRI/encoding/aug16_eng1000'\n",
    "rr, cols_varied, mets = analyze_helper.load_clean_results(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = rr[rr.feature_space == 'eng1000']\n",
    "r = r[r.num_stories == -1]\n",
    "r = r[r.ndelays == 8]\n",
    "r = r[r.feature_selection_alpha == -1.000000]\n",
    "for subject in ['S02', 'S01', 'S03']:\n",
    "    row = r[r.subject == subject]\n",
    "    assert len(row) == 1\n",
    "    row = row.iloc[0]\n",
    "    weights, weights_pc = flatmaps_per_question.get_weights_top(row)\n",
    "    print(weights.shape)\n",
    "    joblib.dump(weights, join(PROCESSED_DIR, subject, 'eng1000_weights.pkl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
