{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cortex\n",
    "from os.path import join\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "sys.path.append('../notebooks')\n",
    "from neuro.config import repo_dir, PROCESSED_DIR\n",
    "from neuro import viz\n",
    "from neurosynth import term_dict, term_dict_rev, get_neurosynth_flatmaps\n",
    "neurosynth_compare = __import__('04_neurosynth_compare')\n",
    "import viz\n",
    "import neurosynth\n",
    "from cortex import mni\n",
    "import os\n",
    "os.environ[\"FSLDIR\"] = \"/home/chansingh/fsl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, this notebook requires first running `03_export_qa_flatmaps.ipynb` into `df_qa_dict.pkl` files for each subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute correlations with qa flatmaps and plot avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "settings ['individual_gpt4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 38.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num common qs 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:16<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Does the sentence describe a physical sensation?'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 41\u001b[0m\n\u001b[1;32m     37\u001b[0m qs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(flatmaps_qa_dicts_by_subject[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUTS02\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m&\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m     38\u001b[0m     flatmaps_gt_dict_mni\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum common qs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(qs))\n\u001b[0;32m---> 41\u001b[0m corrs_df \u001b[38;5;241m=\u001b[39m \u001b[43mneurosynth_compare\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_corrs_df\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrac_voxels_to_keep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubjects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflatmaps_qa_dicts_by_subject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m neurosynth_compare\u001b[38;5;241m.\u001b[39mplot_corrs_df(corrs_df, frac_voxels_to_keep, out_dir)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# compute pvals\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# pvals_subject = compute_pvals_for_subject(\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# corrs_df, 'UTS01', frac_voxels_to_keep_list)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#                                        corrs_df_mni.abs().max().max(), vmax=corrs_df_mni.abs().max().max()).format(precision=3).to_html(\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#     join(out_dir, 'corrs_df_mni.html'))\u001b[39;00m\n",
      "File \u001b[0;32m~/fmri/notebooks_gt_flatmaps/04_neurosynth_compare.py:51\u001b[0m, in \u001b[0;36mcompute_corrs_df\u001b[0;34m(qs, frac_voxels_to_keep, subjects, flatmaps_qa_dicts_by_subject, apply_mask)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m qs:\n\u001b[1;32m     50\u001b[0m     d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestions\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(k)\n\u001b[0;32m---> 51\u001b[0m     d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorr\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mcorrcoef(\u001b[43mflatmaps_qa_dict_masked\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     52\u001b[0m                                  flatmaps_gt_masked[k])[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     53\u001b[0m     d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflatmap_qa\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(flatmaps_qa_dict_masked[k])\n\u001b[1;32m     54\u001b[0m     d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflatmap_neurosynth\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(flatmaps_gt_masked[k])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Does the sentence describe a physical sensation?'"
     ]
    }
   ],
   "source": [
    "# setting = 'shapley_neurosynth'\n",
    "# setting = 'full_neurosynth'\n",
    "# setting = 'individual_gpt4''\n",
    "for settings in [\n",
    "    ['full_neurosynth_pc'],\n",
    "    # ['full_neurosynth_wordrate_pc'],\n",
    "    # ['full_35_pc'],\n",
    "    # ['full_35_wordrate_pc'],\n",
    "    # ['full_neurosynth'],\n",
    "    # ['full_neurosynth_wordrate'],\n",
    "    # ['full_35'],\n",
    "    # ['full_35_wordrate'],\n",
    "    # ['individual_gpt4'],\n",
    "]:\n",
    "    print('settings', settings)\n",
    "    # settings = ['']  # shapley_neurosynth, individual_gpt4\n",
    "    subjects = ['UTS01', 'UTS02', 'UTS03']\n",
    "    # subjects = [f'UTS0{i}' for i in range(1, 9)]\n",
    "\n",
    "    # comparison hyperparams\n",
    "    apply_mask = True\n",
    "    frac_voxels_to_keep = 0.1  # 0.10\n",
    "    frac_voxels_to_keep_list = [frac_voxels_to_keep]\n",
    "    # hyperparams\n",
    "    out_dir = join(repo_dir, 'qa_results',\n",
    "                   'neurosynth_compare', '___'.join(settings))\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # load flatmaps\n",
    "    flatmaps_qa_dicts_by_subject = neurosynth.load_flatmaps_qa_dicts_by_subject(\n",
    "        subjects, settings)\n",
    "\n",
    "    # flatmaps_gt_dict_list_subject_mni = {subject: [convert_to_mni_space(flatmaps_gt_dict_list_subject[subject][qs[i]], subject=subject)\n",
    "    #                                                for i in tqdm(range(len(qs)))]\n",
    "    #                                      for subject in ['UTS01', 'UTS02', 'UTS03']}\n",
    "    flatmaps_gt_dict_mni = neurosynth.get_neurosynth_flatmaps(mni=True)\n",
    "    qs = list(set(flatmaps_qa_dicts_by_subject['UTS02'].keys()) & set(\n",
    "        flatmaps_gt_dict_mni.keys()))\n",
    "    print('num common qs', len(qs))\n",
    "\n",
    "    corrs_df = neurosynth_compare.compute_corrs_df(\n",
    "        qs, frac_voxels_to_keep, subjects, flatmaps_qa_dicts_by_subject, apply_mask)\n",
    "\n",
    "    neurosynth_compare.plot_corrs_df(corrs_df, frac_voxels_to_keep, out_dir)\n",
    "\n",
    "    # compute pvals\n",
    "    # pvals_subject = compute_pvals_for_subject(\n",
    "    # corrs_df, 'UTS01', frac_voxels_to_keep_list)\n",
    "    # pvals_subject.style.background_gradient().format(precision=3)\n",
    "\n",
    "    # compute mni space correlations\n",
    "    # corrs_df_mni = neurosynth_compare.compute_mni_corr_df(\n",
    "    #     flatmaps_qa_dicts_by_subject, flatmaps_gt_dict_mni, qs)\n",
    "    # print('avg', corrs_df_mni.loc['avg'])\n",
    "    # corrs_df_mni.to_pickle(join(out_dir, 'corrs_df_mni.pkl'))\n",
    "    # corrs_df_mni.style.background_gradient(axis=None, cmap=\"coolwarm_r\", vmin=-\n",
    "    #                                        corrs_df_mni.abs().max().max(), vmax=corrs_df_mni.abs().max().max()).format(precision=3).to_html(\n",
    "    #     join(out_dir, 'corrs_df_mni.html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Does the input mention anything related to an action?', 'Does the input mention anything related to arithmetic?', 'Does the input contain a sense of ambiguity?', 'Does the input mention anything related to anger?', 'Does the input mention anything related to calculation?', 'Does the input mention anything related to color?', 'Does the input mention anything related to conflict?', 'Does the input mention anything related to disgust?', 'Does the input mention anything related to empathy?', 'Does the input mention anything related to age?', 'Does the input mention anything related to children?', 'Does the input mention anything related to diseases?', 'Does the input mention anything related to eyes?', 'Does the input mention anything related to knowledge?', 'Does the input mention anything related to gender?', 'Does the input mention anything related to navigation?', 'Does the input mention anything related to motor movements?', 'Does the input mention or describe highly positive emotional valence?', 'Does the input mention or describe highly negative emotional valence?', 'Does the sentence describe a physical sensation?', 'Does the input involve planning or organizing?', 'Does the input mention anything related to food?', 'Does the input mention or describe a smell?', 'Does the input mention or describe a sound?', 'Does the input mention or describe high emotional intensity?', 'Does the sentence contain a negation?', 'Does the sentence describe a personal reflection or thought?', 'Does the sentence describe a sensory experience?', 'Does the sentence mention a specific location?', 'Does the text describe a mode of communication?', 'Is the sentence abstract rather than concrete?'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatmaps_gt_dict_mni.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-do analysis in MNI space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_df_mni = neurosynth_compare.compute_mni_corr_df(\n",
    "    flatmaps_qa_dicts_by_subject, flatmaps_gt_dict_mni, qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_df_mni.round(3).style.background_gradient(axis=None, cmap=\"coolwarm_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surf example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortex.db.get_mri_surf2surf_matrix(\"fsaverage\", \"pial\", target_subj='UTS01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNI example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'Does the input mention anything related to navigation?'\n",
    "flatmaps_q = [flatmaps_qa_dicts_by_subject[s][q] for s in subjects]\n",
    "flatmap_q_s01 = flatmaps_q[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatmap_gt = get_neurosynth_flatmaps('UTS01', mni=False)[q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(flatmap_q_s01.ravel(), flatmap_gt.ravel())[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatmaps_gt_dict_mni[q].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatmaps_q_mni = [\n",
    "    neurosynth_compare.convert_to_mni_space(flatmaps_q[i], subject=subjects[i])\n",
    "    for i in range(len(subjects))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_flatmap_q_mni = np.mean(flatmaps_q_mni, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatmaps_gt_dict_mni[q].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(flatmaps_q_mni[0].ravel(), flatmaps_gt_dict_mni[q].ravel())[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(avg_flatmap_q_mni.ravel(), flatmaps_gt_dict_mni[q].ravel())[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View flatmaps in 1 plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_df = pd.read_pickle(join(repo_dir, 'qa_results',\n",
    "                               'neurosynth', setting + '_corrs_df.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = 'shapley_neurosynth'\n",
    "for subject in ['UTS01', 'UTS02', 'UTS03']:\n",
    "    img_dir1 = join(repo_dir, 'qa_results', 'neurosynth',\n",
    "                    subject, 'neurosynth')\n",
    "    img_dir2 = join(repo_dir, 'qa_results', 'neurosynth',\n",
    "                    subject, setting)\n",
    "\n",
    "    # read images and combine them with their filenames on a single plot\n",
    "    # fnames = os.listdir(img_dir1)\n",
    "    # fnames = [f for f in fnames if f.endswith('.png')]\n",
    "    # only keep the ones that are in both directories\n",
    "    # fnames = [f for f in fnames if f in os.listdir(img_dir2)]\n",
    "\n",
    "    corrs = corrs_df[corrs_df['subject'] == subject]\n",
    "    # corrs = corrs.sort_values(f'corrs_{frac_voxels_to_keep}', ascending=False)\n",
    "    fnames = [v + '.png' for v in corrs['questions'].values]\n",
    "\n",
    "    n = len(fnames)\n",
    "    C = 4\n",
    "    R = int(np.ceil(n / C))\n",
    "\n",
    "    fig, axs = plt.subplots(R, C, figsize=(C * 3.2, R * 1))\n",
    "    axs = axs.flatten()\n",
    "    for i in range(len(axs)):\n",
    "        axs[i].axis('off')\n",
    "    for i, fname in enumerate(fnames):\n",
    "        img1 = plt.imread(join(img_dir1, fname))\n",
    "        img2 = plt.imread(join(img_dir2, fname))\n",
    "        axs[i].imshow(np.concatenate([img1, img2], axis=1))\n",
    "        axs[i].set_title(\n",
    "            f'{term_dict_rev[fname[:-4]]} ({corrs[\"corrs\"].values[i]:0.3f})', fontsize=8)\n",
    "\n",
    "    # add text in bottom right of figure\n",
    "    fig.text(0.99, 0.01, f'{subject}\\nNeurosynth on left, QA on right',\n",
    "             ha='right', va='bottom', fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(join(repo_dir, 'qa_results', 'neurosynth',\n",
    "                subject, f'flatmaps_{setting}_{subject}.png'), dpi=300)\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
