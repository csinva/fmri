{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache, StaticCache\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    'Is the input related to food preparation?',\n",
    "    'Does the input mention laughter?',\n",
    "    'Is there an expression of surprise?',\n",
    "    'Is there a depiction of a routine or habit?',\n",
    "    'Is there stuttering or uncertainty in the input?',\n",
    "    # 'Is there a first-person pronoun in the input?',\n",
    "]\n",
    "examples = [\n",
    "    'i sliced some cucumbers and then moved on to what was next',\n",
    "    'the kids were giggling about the silly things they did',\n",
    "    'and i was like whoa that was unexpected',\n",
    "    'walked down the path like i always did',\n",
    "    'um no um then it was all clear',\n",
    "    # 'i was walking to school and then i saw a cat',\n",
    "]\n",
    "prompt_prefix = '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a concise, helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nInput text: '\n",
    "prompt_template = '{example}\\nQuestion: {question}\\nAnswer with yes or no, then give an explanation.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'\n",
    "prompts = [\n",
    "    prompt_prefix + prompt_template.format(example=example, question=question)\n",
    "    for example in examples\n",
    "    for question in questions\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "model_id = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize cache for a batch scenario. For example, allow up to a batch size of 4.\n",
    "# Increase max_cache_len as needed for your use case.\n",
    "prompt_cache = StaticCache(config=model.config, max_batch_size=4,\n",
    "                           max_cache_len=1024, device=\"cuda\", dtype=torch.bfloat16)\n",
    "\n",
    "# INITIAL_PROMPT = \"You are a helpful assistant. \"\n",
    "INITIAL_PROMPT = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a concise, helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "\n",
    "# Suppose we have multiple prompts that share the same initial prefix.\n",
    "prompts = [\"Help me write a blog post about traveling.\",\n",
    "           \"What is the capital of France?\",\n",
    "           \"Can you summarize this article?\",\n",
    "           \"Give me a recipe for chocolate cake.\"]\n",
    "\n",
    "SUFFIX = '<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'\n",
    "\n",
    "# First, we prepare a batch of identical prefixes. Each prompt in the batch starts with the same prefix.\n",
    "batch_inputs_initial = tokenizer(\n",
    "    [INITIAL_PROMPT] * len(prompts), return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "\n",
    "# Precompute the prefix past_key_values for the entire batch.\n",
    "with torch.no_grad():\n",
    "    prompt_cache = model(**batch_inputs_initial,\n",
    "                         past_key_values=prompt_cache).past_key_values\n",
    "\n",
    "prompts_full = [INITIAL_PROMPT + p + SUFFIX\n",
    "                for p in prompts]\n",
    "\n",
    "# Now we form the full prompt by appending each user query to the INITIAL_PROMPT.\n",
    "# Since the prefix has already been cached, the model will start generating from that state.\n",
    "batch_inputs = tokenizer(prompts_full,\n",
    "                         return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "\n",
    "# Generate outputs for the entire batch at once, utilizing the cached prefix.\n",
    "outputs = model.generate(\n",
    "    **batch_inputs, past_key_values=prompt_cache, max_new_tokens=5)\n",
    "responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "# responses = []\n",
    "# for i, output_ids in enumerate(outputs):\n",
    "# prompt_len = batch_inputs[\"input_ids\"][i].shape[0]\n",
    "# responses.append(responses[i][prompt_len:])\n",
    "\n",
    "for i, response in enumerate(responses):\n",
    "    print(f\"Prompt: {repr(prompts_full[i])}\")\n",
    "    print(f\"Response: {repr(response)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeilne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "\n",
    "pipeline_ = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=checkpoint,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    # model_kwargs={'torch_dtype': torch.float16},\n",
    "    device_map=\"cuda:0\"\n",
    ")\n",
    "pipeline_.tokenizer.pad_token_id = pipeline_.tokenizer.eos_token_id\n",
    "pipeline_.tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = prompts[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_PROMPT = '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a concise, helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nInput text:'\n",
    "prompt_cache = StaticCache(config=pipeline_.model.config, max_batch_size=4,\n",
    "                           max_cache_len=1024, device=\"cuda\", dtype=torch.bfloat16)\n",
    "# First, we prepare a batch of identical prefixes. Each prompt in the batch starts with the same prefix.\n",
    "batch_inputs_initial = pipeline_.tokenizer(\n",
    "    [INITIAL_PROMPT] * len(ps), return_tensors=\"pt\", padding=True).to(\"cuda:0\")\n",
    "\n",
    "# Precompute the prefix past_key_values for the entire batch.\n",
    "with torch.no_grad():\n",
    "    prompt_cache = pipeline_.model(**batch_inputs_initial,\n",
    "                                   past_key_values=prompt_cache).past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ps\n",
    "max_new_tokens = 10\n",
    "batch_size = 4\n",
    "\n",
    "outputs = pipeline_(\n",
    "    prompt,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    batch_size=batch_size,\n",
    "    do_sample=False,\n",
    "    pad_token_id=pipeline_.tokenizer.pad_token_id,\n",
    "    past_key_values=prompt_cache,\n",
    ")\n",
    "if isinstance(prompt, str):\n",
    "    texts = outputs[0][\"generated_text\"][len(prompt):]\n",
    "else:\n",
    "    texts = [outputs[i][0]['generated_text']\n",
    "             [len(prompt[i]):] for i in range(len(outputs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = list(map(lambda x: 'yes' in x.lower(), texts))\n",
    "answers = np.array(answers).reshape(len(examples), len(questions))\n",
    "embeddings = np.array(answers, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(embeddings.astype(int), columns=[\n",
    "    q.split()[-1] for q in questions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-14 08:41:05 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=300, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "INFO 12-14 08:41:06 selector.py:135] Using Flash Attention backend.\n",
      "INFO 12-14 08:41:07 model_runner.py:1072] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "INFO 12-14 08:41:07 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c99459372784a24afb09ec1dd7db2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-14 08:41:11 model_runner.py:1077] Loading model weights took 14.9888 GB\n",
      "INFO 12-14 08:41:12 worker.py:232] Memory profiling results: total_gpu_memory=44.55GiB initial_memory_usage=33.22GiB peak_torch_memory=16.18GiB memory_usage_post_profile=33.25GiB non_torch_memory=18.25GiB kv_cache_size=5.67GiB gpu_memory_utilization=0.90\n",
      "INFO 12-14 08:41:12 gpu_executor.py:113] # GPU blocks: 2902, # CPU blocks: 2048\n",
      "INFO 12-14 08:41:12 gpu_executor.py:117] Maximum concurrency for 300 tokens per request: 154.77x\n",
      "INFO 12-14 08:41:15 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-14 08:41:15 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-14 08:41:31 model_runner.py:1518] Graph capturing finished in 16 secs, took 0.85 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.12it/s, est. speed input: 12.76 toks/s, output: 34.02 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RequestOutput(request_id=0, prompt='Hello, my name is', prompt_token_ids=[128000, 9906, 11, 856, 836, 374], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' Helen and I started this quote-of-the-day blog to share inspiring quotes that might', token_ids=(43881, 323, 358, 3940, 420, 12929, 8838, 10826, 11477, 5117, 311, 4430, 34147, 17637, 430, 2643), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1734194491.7371595, last_token_time=1734194491.7371595, first_scheduled_time=1734194491.7511826, first_token_time=1734194491.7839239, time_in_queue=0.014023065567016602, finished_time=1734194492.1947834, scheduler_time=0.001187936868518591, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# For generative models (task=generate) only\n",
    "llm = LLM(model='meta-llama/Llama-3.1-8B-Instruct', task=\"generate\",\n",
    "          max_model_len=300)  # Name or path of your model\n",
    "output = llm.generate(\"Hello, my name is\")\n",
    "sampling_params = SamplingParams(max_tokens=1)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
