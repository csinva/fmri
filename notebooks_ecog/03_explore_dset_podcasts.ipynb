{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from os.path import expanduser\n",
    "import sys\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pprint import pprint\n",
    "import imodelsx.util\n",
    "from os.path import dirname\n",
    "import pickle as pkl\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from numpy.linalg import norm\n",
    "from math import ceil\n",
    "from imodelsx.qaemb.qaemb import QAEmb, get_sample_questions_and_examples\n",
    "from neuro.ecog.config import STORIES_POPULAR, STORIES_UNPOPULAR, ECOG_DIR\n",
    "import mne\n",
    "import h5py\n",
    "import torch\n",
    "import base64\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "extract = __import__('00_extract')\n",
    "from nilearn.plotting import plot_markers\n",
    "from mne_bids import BIDSPath\n",
    "\n",
    "from himalaya.backend import set_backend, get_backend\n",
    "from himalaya.ridge import RidgeCV\n",
    "from himalaya.scoring import correlation_score\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from neuro.ecog.config import ECOG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bids_root = join(ECOG_DIR, 'podcasts_data', 'ds005574/')\n",
    "\n",
    "# Download the transcript, if required\n",
    "# transcript_path = f\"{bids_root}stimuli/gpt2-xl/transcript.tsv\"\n",
    "transcript_path = join(bids_root, \"stimuli/syntactic/transcript.tsv\")\n",
    "\n",
    "# Load transcript\n",
    "df_contextual = pd.read_csv(transcript_path, sep=\"\\t\", index_col=0)\n",
    "df_contextual.head()\n",
    "\n",
    "df_word = df_contextual.groupby(\"word_idx\").agg(\n",
    "    dict(word=\"first\", start=\"first\", end=\"last\"))\n",
    "df_word = df_word.rename(columns={'word': 'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = join(ECOG_DIR, 'data', 'transcripts')\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "df_word.to_csv(join(out_dir, '___podcasts-story___', 'features.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load wav file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "# Load the WAV file\n",
    "audio = AudioSegment.from_wav(\"podcast.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word-level\n",
    "\n",
    "# Split the audio into segments based on the word timings\n",
    "word_buffer = 0.275\n",
    "# include word_buffer seconds before and after, unless it overlaps with the previous or next word\n",
    "backup_buffer = 0.01\n",
    "# if it overlaps, use the end of the previous word or the start of the next word with a backup_buffer offset\n",
    "df_word['end_prev_word'] = df_word['end'].shift(1, fill_value=0)\n",
    "df_word['word_wav_start'] = df_word.apply(lambda row: max(\n",
    "    row['start'] - word_buffer, row['end_prev_word'] + backup_buffer), axis=1)\n",
    "df_word['start_next_word'] = df_word['start'].shift(-1, fill_value=10000000)\n",
    "df_word['word_wav_end'] = df_word.apply(lambda row: min(\n",
    "    row['end'] + word_buffer, row['start_next_word'] - backup_buffer), axis=1)\n",
    "\n",
    "df_word['6sec_wav_start'] = (df_word['word_wav_end'] - 6).clip(lower=0)\n",
    "df_word['3sec_wav_start'] = (df_word['word_wav_end'] - 3).clip(lower=0)\n",
    "df_word['1.5sec_wav_start'] = (df_word['word_wav_end'] - 1.5).clip(lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5136it [00:01, 4701.09it/s] \n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "for k in ['segments_word', 'segments_3sec', 'segments_1.5sec', 'segments_6sec']:\n",
    "    os.makedirs(k, exist_ok=True)\n",
    "\n",
    "d = defaultdict(list)\n",
    "for idx, row in list(tqdm(df_word.iterrows())):\n",
    "    segment_word = audio[1000 * row['word_wav_start']: 1000 * row['word_wav_end']]\n",
    "    segment_word.export(f\"segments_word/segment_{idx}.wav\", format=\"wav\")\n",
    "\n",
    "    segment_1_5sec = audio[1000 * row['1.5sec_wav_start']: 1000 * row['word_wav_end']]\n",
    "\n",
    "    segment_1_5sec.export(f\"segments_1.5sec/segment_{idx}.wav\", format=\"wav\")\n",
    "\n",
    "    segment_3sec = audio[1000 * row['3sec_wav_start']: 1000 * row['word_wav_end']]\n",
    "    segment_3sec.export(f\"segments_3sec/segment_{idx}.wav\", format=\"wav\")\n",
    "\n",
    "    segment_6sec = audio[1000 * row['6sec_wav_start']: 1000 * row['word_wav_end']]\n",
    "    segment_6sec.export(f\"segments_6sec/segment_{idx}.wav\", format=\"wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
