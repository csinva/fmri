{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from os.path import expanduser\n",
    "import sys\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pprint import pprint\n",
    "import imodelsx.util\n",
    "from os.path import dirname\n",
    "import pickle as pkl\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from numpy.linalg import norm\n",
    "from math import ceil\n",
    "# from imodelsx.qaemb.qaemb import QAEmb, get_sample_questions_and_examples\n",
    "from neuro.ecog.config import STORIES_POPULAR, STORIES_UNPOPULAR, ECOG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save ensemble features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suffix_qs = ''\n",
    "suffix_qs = '___qs_35_stable'\n",
    "\n",
    "# save ensemble feats\n",
    "settings = ['words', 'sec_1.5', 'sec_3', 'sec_6']\n",
    "# settings = ['sec_1.5', 'sec_6']\n",
    "# settings = ['words', 'sec_3']\n",
    "# out_checkpoint = 'ensemble1'\n",
    "ensemble1 = [\n",
    "    'mistralai/Mistral-7B-Instruct-v0.2',\n",
    "    'meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "    'google/gemma-7b-it',\n",
    "]\n",
    "\n",
    "for setting in settings:\n",
    "    print(setting)\n",
    "    output_dir_ensemble = join(\n",
    "        ECOG_DIR, f'features{suffix_qs}', out_checkpoint, setting)\n",
    "    os.makedirs(output_dir_ensemble, exist_ok=True)\n",
    "\n",
    "    # read in ensemble feats\n",
    "    ensemble_checkpoint_story_dict = {}\n",
    "    for checkpoint in tqdm(ensemble1):\n",
    "        checkpoint_clean = checkpoint.replace('/', '___')\n",
    "        output_dir_clean = join(ECOG_DIR, f'features{suffix_qs}',\n",
    "                                checkpoint_clean, setting)\n",
    "        story_fnames = os.listdir(output_dir_clean)\n",
    "        checkpoint_story_dict = {}\n",
    "        for story_fname in story_fnames:\n",
    "            if story_fname.endswith('.pkl'):\n",
    "                checkpoint_story_dict[story_fname] = joblib.load(\n",
    "                    join(output_dir_clean, story_fname))\n",
    "        ensemble_checkpoint_story_dict[checkpoint] = deepcopy(\n",
    "            checkpoint_story_dict)\n",
    "\n",
    "    # save avg feats\n",
    "    common_stories = set.intersection(\n",
    "        *[set(ensemble_checkpoint_story_dict[checkpoint].keys())\n",
    "            for checkpoint in ensemble1]\n",
    "    )\n",
    "    print('\\tsaving avg feats for', len(common_stories), 'stories')\n",
    "    for story_fname in tqdm(common_stories):\n",
    "        out_fname_pkl = join(output_dir_ensemble, story_fname)\n",
    "        if not os.path.exists(out_fname_pkl):\n",
    "            # avg over all checkpoints\n",
    "            story1_df = ensemble_checkpoint_story_dict[ensemble1[0]][story_fname]\n",
    "            story2_df = ensemble_checkpoint_story_dict[ensemble1[1]][story_fname]\n",
    "            story3_df = ensemble_checkpoint_story_dict[ensemble1[2]][story_fname]\n",
    "\n",
    "            # align the dfs to have same cols and index\n",
    "            story1_df = story1_df[story2_df.columns]\n",
    "            assert story1_df.columns.equals(story2_df.columns)\n",
    "            assert story1_df.index.equals(story2_df.index)\n",
    "\n",
    "            story2_df = story2_df[story1_df.columns]\n",
    "            assert story2_df.columns.equals(story1_df.columns)\n",
    "            assert story2_df.index.equals(story1_df.index)\n",
    "\n",
    "            # average values\n",
    "            # avg_df = (story1_df.astype(float) + story2_df.astype(float)) / 2\n",
    "            avg_df = (story1_df.astype(float) + story2_df.astype(float) +\n",
    "                      story3_df.astype(float)) / 3\n",
    "\n",
    "            # save\n",
    "            avg_df.to_csv(join(output_dir_ensemble,\n",
    "                               story_fname.replace('.pkl', '.csv')))\n",
    "            avg_df.to_pickle(out_fname_pkl)\n",
    "\n",
    "    print('\\tavg feats', output_dir_ensemble, os.listdir(output_dir_ensemble))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls /home/chansingh/mntv1/ecog/features/ensemble1/\n",
    "!ls /home/chansingh/mntv1/ecog/features___qs_35_stable/ensemble1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = '/home/chansingh/mntv1/ecog/features___qs_35_stable/ensemble1/words'\n",
    "n = 0\n",
    "for k in os.listdir(d):\n",
    "    if not k.endswith('.pkl'):\n",
    "        continue\n",
    "    df = pd.read_pickle(join(d, k))\n",
    "    n += df.shape[0]\n",
    "    # print(df.head())\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[1GTransferred:   \t         0 / 0 Bytes, -, 0 Bytes/s, ETA -\n",
      "Errors:                 0\n",
      "Checks:                 0 / 0, -\n",
      "Transferred:            0 / 0, -\n",
      "Elapsed time:          0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1GTransferred:   \t         0 / 682.259 kBytes, 0%, 0 Bytes/s, ETA -\n",
      "Errors:                 0\n",
      "Checks:                16 / 16, 100%\n",
      "Transferred:            0 / 2, 0%\n",
      "Elapsed time:       100ms\n",
      "Transferring:\n",
      " *                sec_6/___podcasts-story___.csv: transferring\n",
      " *                sec_6/___podcasts-story___.pkl:  0% /682.259k, 0/s, -\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1GTransferred:   \t  682.259k / 2.145 MBytes, 31%, 1.045 MBytes/s, ETA 1s\n",
      "Errors:                 0\n",
      "Checks:                16 / 16, 100%\n",
      "Transferred:            0 / 2, 0%\n",
      "Elapsed time:       600ms\n",
      "Transferring:\n",
      " *                sec_6/___podcasts-story___.csv:  0% /1.479M, 0/s, -\n",
      " *                sec_6/___podcasts-story___.pkl:100% /682.259k, 0/s, -\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1GTransferred:   \t    2.145M / 2.145 MBytes, 100%, 1.886 MBytes/s, ETA 0s\n",
      "Errors:                 0\n",
      "Checks:                16 / 16, 100%\n",
      "Transferred:            0 / 2, 0%\n",
      "Elapsed time:        1.1s\n",
      "Transferring:\n",
      " *                sec_6/___podcasts-story___.csv:100% /1.479M, 0/s, -\n",
      " *                sec_6/___podcasts-story___.pkl:100% /682.259k, 682.100k/s, 0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1GTransferred:   \t    2.145M / 2.145 MBytes, 100%, 1.310 MBytes/s, ETA 0s\n",
      "Errors:                 0\n",
      "Checks:                16 / 16, 100%\n",
      "Transferred:            1 / 2, 50%\n",
      "Elapsed time:        1.6s\n",
      "Transferring:\n",
      " *                sec_6/___podcasts-story___.csv:100% /1.479M, 1.479M/s, 0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1GTransferred:   \t    2.145M / 2.145 MBytes, 100%, 1.004 MBytes/s, ETA 0s\n",
      "Errors:                 0\n",
      "Checks:                16 / 16, 100%\n",
      "Transferred:            1 / 2, 50%\n",
      "Elapsed time:        2.1s\n",
      "Transferring:\n",
      " *                sec_6/___podcasts-story___.csv:100% /1.479M, 1.479M/s, 0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1GTransferred:   \t    2.145M / 2.145 MBytes, 100%, 949.396 kBytes/s, ETA 0s\n",
      "Errors:                 0\n",
      "Checks:                16 / 16, 100%\n",
      "Transferred:            2 / 2, 100%\n",
      "Elapsed time:        2.3s\n"
     ]
    }
   ],
   "source": [
    "# !rclone copy /home/chansingh/mntv1/ecog/features/ensemble1/ box:DeepTune/QA/cached_qa_tree_ensemble1 --progress\n",
    "# !rclone copy /home/chansingh/mntv1/ecog/features___qs_35_stable/ensemble1/ box:DeepTune/QA/cached_qa_tree___qs_35_stable_ensemble1 --progress\n",
    "!rclone copy /home/chansingh/mntv1/ecog/features___qs_35_stable/gpt-4o-mini/ box:DeepTune/QA/cached_qa_tree___qs_35_stable_gpt-4o-mini --progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at question answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sec_1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  7.26it/s]\n"
     ]
    }
   ],
   "source": [
    "suffix_qs = '___qs_35_stable'\n",
    "\n",
    "# save ensemble feats\n",
    "settings = ['sec_1.5']\n",
    "# settings = ['words']\n",
    "# settings = ['sec_6']\n",
    "ensemble1 = [\n",
    "    # 'mistralai/Mistral-7B-Instruct-v0.2',\n",
    "    # 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "    # 'google/gemma-7b-it',\n",
    "    'gpt-4o-mini',\n",
    "]\n",
    "# story_fnames = ['ant-man.pkl']\n",
    "# story_fnames = ['lotr-1.pkl']\n",
    "story_fnames = ['___podcasts-story___.pkl']\n",
    "\n",
    "for setting in settings:\n",
    "    print(setting)\n",
    "\n",
    "    # read in ensemble feats\n",
    "    ensemble_checkpoint_story_dict = {}\n",
    "    for checkpoint in tqdm(ensemble1):\n",
    "        checkpoint_clean = checkpoint.replace('/', '___')\n",
    "        output_dir_clean = join(ECOG_DIR, f'features{suffix_qs}',\n",
    "                                checkpoint_clean, setting)\n",
    "        checkpoint_story_dict = {}\n",
    "        for story_fname in story_fnames:\n",
    "            if story_fname.endswith('.pkl'):\n",
    "                checkpoint_story_dict[story_fname] = joblib.load(\n",
    "                    join(output_dir_clean, story_fname))\n",
    "        ensemble_checkpoint_story_dict[checkpoint] = deepcopy(\n",
    "            checkpoint_story_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini\n",
      "Is time mentioned in the input? ['Sulawesi A few years', 'A few years ago', 'A few years ago the', 'few years ago the photographer', 'years ago the photographer David', \"monkeys I'd gone a whole day\", \"monkeys I'd gone a whole day just\", \"I'd gone a whole day just following\", \"I'd gone a whole day just following them\", 'lens But every time', 'lens But every time he', 'lens But every time he got', 'But every time he got close', 'But every time he got close to', 'But every time he got close to taking', 'But every time he got close to taking the', 'time he got close to taking the shot', 'So after two', 'So after two days', 'So after two days of', 'So after two days of these', 'So after two days of these near', 'two days of these near misses', 'days of these near misses he', 'course when when it when', 'course when when it when it', 'the whole project by now In', 'wanted it to go on forever really', 'When', 'population in the last forty', 'population in the last forty years', 'the last forty years because', 'last forty years because of', 'years because of humans', 'selfies This was in two thousand', 'This was in two thousand eleven', 'eleven Within an hour', 'Within an hour it', 'Within an hour it was', 'calls Over the next', 'calls Over the next few', 'Over the next few months', 'Over the next few months the', 'the next few months the monkey', 'few months the monkey selfies', 'months the monkey selfies sold', 'months the monkey selfies sold pretty', 'And then one day', 'And then one day he goes', 'And then one day he goes online', 'And then one day he goes online to', 'one day he goes online to research', 'crested black macaque one day', 'crested black macaque one day in', 'black macaque one day in in', 'black macaque one day in in the', 'macaque one day in in the Google', 'come It was the moment', 'It was the moment when', 'Um Wait I wanna look that up', 'Um Wait I wanna look that up really', 'Um Wait I wanna look that up really fast', 'Um wanna look that up really fast here', 'UK And one day', 'this is clearly an April', \"about until you get served papers Let's\", \"about until you get served papers Let's see\", 'enough one afternoon', 'enough one afternoon I', 'one afternoon I was', 'one afternoon I was at', 'afternoon I was at home', 'afternoon I was at home And', \"afternoon I was at home And there's\", \"afternoon I was at home And there's a\", \"afternoon I was at home And there's a knock\", \"afternoon I was at home And there's a knock at\", \"afternoon I was at home And there's a knock at the\", 'And then at the end when', 'And then at the end when you get', 'And then at the end when you get to', 'And then at the end when you get to the', 'in approximately', 'approximately twenty fourteen', 'immediately that', \"now uh this is great I've got a\", 'Do you remember when', 'remember when you first heard', 'remember when you first heard about', 'about the case Was it when did', \"All these years I've been practicing\", \"All these years I've been practicing for like\", \"All these years I've been practicing for like twenty\", \"these years I've been practicing for like twenty five\", \"I've been practicing for like twenty five years\", \"I've been practicing for like twenty five years and\", \"practicing for like twenty five years and I've\", \"practicing for like twenty five years and I've never\", \"twenty five years and I've never been\", \"five years and I've never been able\", \"years and I've never been able to\", 'on a precedent set in two thousand', 'precedent set in two thousand four', 'Good afternoon This is a recording from', 'Good afternoon This is a recording from the', 'Good afternoon This is a recording from the hearing', 'afternoon is a recording from the hearing in', 'afternoon is a recording from the hearing in the', 'afternoon recording from the hearing in the federal', 'vast history of common law', 'history of common law prior', 'history of common law prior to', 'history of common law prior to the', 'prior to the Copyright Act was', 'antebellum in the antebellum days', 'in the antebellum days But', 'days But when you look at the', 'Just to pause on this', 'Just to pause on this for a', 'Just to pause on this for a moment', \"This isn't the first time\", \"This isn't the first time PETA\", \"This isn't the first time PETA has\", \"This isn't the first time PETA has tried\", \"This isn't the first time PETA has tried an\", 'first time PETA has tried an argument', 'time PETA has tried an argument like', 'courts In twenty eleven', 'courts In twenty eleven they', 'In twenty eleven they sued', 'twenty eleven they sued SeaWorld', 'prevents us many times', 'us many times from', 'many times from from', 'times from from seeing that', 'unprecedented Three years', 'Three years ago', 'Three years ago in', 'Three years ago in Argentina', 'ago in Argentina a', 'I think sometimes', 'now wait a minute No Do they have a point', 'Good morning Your Honors', 'Good morning Your Honors And', 'Good morning Your Honors And may', 'morning Your Honors And may it', \"Trump's travel ban twice this year\", 'travel ban twice this year case', 'ban twice this year case These', 'twice this year case These judges', 'Schwarz made it thirty seconds', 'Schwarz made it thirty seconds into', 'Schwarz made it thirty seconds into his', 'made it thirty seconds into his oral', 'seconds into his oral argument', 'seconds into his oral argument before', 'standard for article three And then a few minutes', 'article three And then a few minutes later', 'three then a few minutes later Judge', 'minutes later Judge Randy', 'author for twenty minutes', 'for twenty minutes before', 'twenty minutes before handing', 'minutes before handing the', 'minutes before handing the floor', 'minutes before handing the floor to', 'And after two', 'And after two years', 'And after two years of', 'after two years of research', 'after two years of research and', 'years of research and legal', 'After that', 'in court in San Francisco that day', \"day He didn't wanna spend\", 'When I watched it', 'After two', 'After two years', 'After two years the', 'After two years the humans', 'two years the humans managed', 'two years the humans managed to', 'years the humans managed to figure', 'years the humans managed to figure things', 'And in September', 'And in September David', 'And in September David and', 'in September David and PETA', 'September David and PETA announced', 'around when the selfies were being taken', 'around when the selfies were being taken that', 'the selfies were being taken that day', 'thousands of dollars, years', 'of dollars, years of', 'dollars, years of our', 'years of our lives,']\n",
      "Does the input contain a measurement? ['Sulawesi A few years', 'years ago the photographer David', 'There was one shot', 'So after two days', 'So after two days of these', 'So after two days of these near', 'two days of these near misses', 'There was one shot', \"wild They've lost eighty\", \"They've lost eighty percent\", \"They've lost eighty percent of\", \"They've lost eighty percent of their\", 'eighty percent of their population', 'eighty percent of their population in', 'eighty percent of their population in the', 'percent of their population in the last', 'the last forty years because', 'last forty years because of', 'eleven Within an hour', 'Within an hour it', 'Within an hour it was', 'a license fee Um could', 'a license fee Um could you', 'license fee Um could you show me', 'thrust a large A4', 'large A4 sized', 'large A4 sized white', 'A4 sized white envelope', 'A4 sized white envelope in', 'A4 sized white envelope in my', 'approximately twenty', 'copyright There are like twenty', 'copyright There are like twenty French', 'There are like twenty French tourists', 'There are like twenty French tourists that', 'like twenty French tourists that I', 'twenty French tourists that I need', 'twenty French tourists that I need to', \"All these years I've been practicing for like twenty\", \"these years I've been practicing for like twenty five\", \"I've been practicing for like twenty five years\", \"I've been practicing for like twenty five years and\", \"practicing for like twenty five years and I've\", \"practicing for like twenty five years and I've never\", \"twenty five years and I've never been\", \"five years and I've never been able\", 'Andrew the case was a one banana', 'Orcas are huge like twenty', 'are huge like twenty to', 'huge like twenty to thirty', 'huge like twenty to thirty feet', 'like twenty to thirty feet long', 'like twenty to thirty feet long And', 'twenty to thirty feet long And they', 'twenty to thirty feet long And they weigh', 'thirty feet long And they weigh about', 'feet long And they weigh about six', 'long And they weigh about six tons', 'six tons They', 'tons They typically', 'They typically travel around seventy', 'typically travel around seventy five', 'travel around seventy five miles', 'travel around seventy five miles in', 'travel around seventy five miles in a', 'seventy five miles in a day', 'in small concrete tanks', 'unprecedented Three years', 'Three years ago', 'Three years ago in', 'Three years ago in Argentina', 'She was being held in a tiny', 'Schwarz made it thirty seconds', 'Schwarz made it thirty seconds into', 'Schwarz made it thirty seconds into his', 'made it thirty seconds into his oral', 'seconds into his oral argument', 'seconds into his oral argument before', 'three then a few minutes later Judge', 'minutes later Judge Randy', 'author for twenty minutes', 'for twenty minutes before', 'twenty minutes before handing', 'minutes before handing the', 'minutes before handing the floor', 'minutes before handing the floor to', 'And after two', 'And after two years', 'And after two years of', 'after two years of research', 'after two years of research and', 'After two years', 'After two years the', 'After two years the humans', 'two years the humans managed', 'two years the humans managed to', 'he agreed to donate twenty five', 'agreed to donate twenty five percent', 'agreed to donate twenty five percent of', 'agreed to donate twenty five percent of the', 'donate twenty five percent of the proceeds', 'donate twenty five percent of the proceeds of', 'twenty five percent of the proceeds of the', 'percent of the proceeds of the photos', 'percent of the proceeds of the photos to', 'characteristic to spend hundreds', 'characteristic to spend hundreds of', 'to spend hundreds of thousands', 'to spend hundreds of thousands of', 'hundreds of thousands of dollars,', 'thousands of dollars, years', 'of dollars, years of', 'dollars, years of our']\n",
      "Does the input contain a number? ['Sulawesi A few years', 'There was one shot', 'There was one shot David', 'So after two', 'So after two days', 'So after two days of', 'So after two days of these', 'So after two days of these near', 'two days of these near misses', 'while some monkeys trying or two', 'while some monkeys trying or two or', 'some monkeys trying or two or three', 'monkeys trying or two or three monkeys', 'monkeys trying or two or three monkeys are', 'trying or two or three monkeys are trying', 'trying or two or three monkeys are trying to', 'or two or three monkeys are trying to play', 'or two or three monkeys are trying to play with', 'two or three monkeys are trying to play with the', 'three monkeys are trying to play with the button', 'There was one', 'There was one shot', 'There was one shot in', 'There was one shot in particular', \"wild They've lost eighty\", \"They've lost eighty percent\", \"They've lost eighty percent of\", \"They've lost eighty percent of their\", 'eighty percent of their population', 'eighty percent of their population in', 'eighty percent of their population in the', 'population in the last forty', 'population in the last forty years', 'the last forty years because', 'last forty years because of', 'selfies This was in two', 'selfies This was in two thousand', 'This was in two thousand eleven', 'thousand eleven Within', 'thousand eleven Within an', 'eleven Within an hour', 'crested black macaque one', 'a license fee Um could you', 'A4 sized white envelope', 'A4 sized white envelope in', 'A4 sized white envelope in my', 'approximately twenty', 'approximately twenty fourteen', 'twenty fourteen um', 'fourteen um when', 'fourteen um when the', 'Bumble profile with whatever I get', 'copyright There are like twenty', 'copyright There are like twenty French', 'There are like twenty French tourists', 'There are like twenty French tourists that', 'like twenty French tourists that I', 'twenty French tourists that I need', 'twenty French tourists that I need to', \"All these years I've been practicing for like twenty\", \"these years I've been practicing for like twenty five\", \"I've been practicing for like twenty five years\", \"I've been practicing for like twenty five years and\", \"practicing for like twenty five years and I've\", \"practicing for like twenty five years and I've never\", \"twenty five years and I've never been\", \"five years and I've never been able\", \"years and I've never been able to\", 'Andrew the case was a one banana', 'case was a one banana problem', 'on a precedent set in two thousand', 'precedent set in two thousand four', 'two thousand four in', 'two thousand four in a', 'thousand four in a case', 'four in a case called', 'case the Ninth Circuit Court of', 'the Ninth Circuit Court of Appeals', 'Calling civil matter fifteen', 'civil matter fifteen dash', 'matter fifteen dash four', 'fifteen dash four three', 'fifteen dash four three two', 'dash four three two four', 'two four Naruto', 'four Naruto versus', 'Um one', 'adoption of the fourteenth Amendment', 'fourteenth Amendment the', 'courts In twenty', 'courts In twenty eleven', 'courts In twenty eleven they', 'In twenty eleven they sued', 'twenty eleven they sued SeaWorld', 'eleven they sued SeaWorld on', 'SeaWorld on behalf of five', 'on behalf of five orcas', 'of five orcas at SeaWorld', 'Orcas are huge like twenty', 'are huge like twenty to', 'huge like twenty to thirty', 'huge like twenty to thirty feet', 'like twenty to thirty feet long', 'like twenty to thirty feet long And', 'twenty to thirty feet long And they', 'twenty to thirty feet long And they weigh', 'thirty feet long And they weigh about', 'feet long And they weigh about six', 'long And they weigh about six tons', 'six tons They', 'They typically travel around seventy', 'typically travel around seventy five', 'travel around seventy five miles', 'travel around seventy five miles in', 'travel around seventy five miles in a', 'seventy five miles in a day', 'program after the last of their twenty', 'after the last of their twenty seven', 'last of their twenty seven orcas', 'their twenty seven orcas dies', 'unprecedented Three', 'unprecedented Three years', 'Three years ago', 'Three years ago in', 'Three years ago in Argentina', 'part of the court case', 'The case went before three judges', 'went before three judges on the Ninth', 'before three judges on the Ninth Circuit', 'three judges on the Ninth Circuit Court', 'the Ninth Circuit Court of Appeals', \"Trump's travel ban twice this year\", 'travel ban twice this year case', 'Schwarz made it thirty', 'Schwarz made it thirty seconds', 'Schwarz made it thirty seconds into', 'Schwarz made it thirty seconds into his', 'made it thirty seconds into his oral', 'seconds into his oral argument', 'article three And then a few minutes later', 'three then a few minutes later Judge', 'definition of author for twenty', 'author for twenty minutes', 'for twenty minutes before', 'twenty minutes before handing', 'And after two', 'And after two years', 'And after two years of', 'after two years of research', 'after two years of research and', 'it and you see three', 'three judges there And then', 'After two', 'After two years', 'After two years the', 'After two years the humans', 'two years the humans managed', 'two years the humans managed to', 'September David and PETA announced', 'And he agreed to donate twenty', 'he agreed to donate twenty five', 'agreed to donate twenty five percent', 'agreed to donate twenty five percent of', 'agreed to donate twenty five percent of the', 'donate twenty five percent of the proceeds', 'donate twenty five percent of the proceeds of', 'twenty five percent of the proceeds of the', \"There's just one\", 'characteristic to spend hundreds', 'to spend hundreds of thousands', 'to spend hundreds of thousands of', 'hundreds of thousands of dollars,']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "easy_qs = ['Is time mentioned in the input?',\n",
    "           'Does the input contain a measurement?', 'Does the input contain a number?']\n",
    "for checkpoint in ensemble1:\n",
    "    print(checkpoint)\n",
    "    df = ensemble_checkpoint_story_dict[checkpoint][story_fnames[0]][easy_qs]\n",
    "    for k in df.columns:\n",
    "        print(k, df[k][df[k] > 0].index.tolist())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10_000 * 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
