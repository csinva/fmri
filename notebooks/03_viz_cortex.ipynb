{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "viz_cortex = __import__('03_viz_cortex')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "import h5py\n",
    "import pickle as pkl\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "fit_decoding = __import__('02_fit_decoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset rotten_tomatoes (/home/chansingh/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8740f0f2219444ca4136172ab46cf8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset rotten_tomatoes (/home/chansingh/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c12d2138df4c11998d3d84bbeffee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39massert\u001b[39;00m args\u001b[39m.\u001b[39mperc_threshold_fmri \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mShould run this script with perc=0!\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      7\u001b[0m X_train, y_train, X_test, y_test \u001b[39m=\u001b[39m fit_decoding\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mget_dsets(\n\u001b[1;32m      8\u001b[0m         args\u001b[39m.\u001b[39mdset, seed\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mseed, subsample_frac\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m feats_train, feats_test \u001b[39m=\u001b[39m fit_decoding\u001b[39m.\u001b[39;49mget_feats(\n\u001b[1;32m     11\u001b[0m         args\u001b[39m.\u001b[39;49mmodel, X_train, X_test,\n\u001b[1;32m     12\u001b[0m         subject_fmri\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49msubject, perc_threshold_fmri\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mperc_threshold_fmri, args\u001b[39m=\u001b[39;49margs)\n\u001b[1;32m     14\u001b[0m norms \u001b[39m=\u001b[39m {\n\u001b[1;32m     15\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mfeats_train_mean\u001b[39m\u001b[39m'\u001b[39m: feats_train\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m),\n\u001b[1;32m     16\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mfeats_train_std\u001b[39m\u001b[39m'\u001b[39m: feats_train\u001b[39m.\u001b[39mstd(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m),\n\u001b[1;32m     17\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mfeats_test_mean\u001b[39m\u001b[39m'\u001b[39m: feats_test\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m),\n\u001b[1;32m     18\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mfeats_test_std\u001b[39m\u001b[39m'\u001b[39m: feats_test\u001b[39m.\u001b[39mstd(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m),\n\u001b[1;32m     19\u001b[0m }\n",
      "File \u001b[0;32m~/fmri/notebooks/../02_fit_decoding.py:126\u001b[0m, in \u001b[0;36mget_feats\u001b[0;34m(model, X, X_test, subject_fmri, perc_threshold_fmri, args)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39mfmri\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    124\u001b[0m     save_dir_fmri \u001b[39m=\u001b[39m join(\n\u001b[1;32m    125\u001b[0m         results_dir, \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m, mod, subject_fmri)\n\u001b[0;32m--> 126\u001b[0m     feats_train \u001b[39m=\u001b[39m get_embs_fmri(\n\u001b[1;32m    127\u001b[0m         X, mod, save_dir_fmri, perc_threshold\u001b[39m=\u001b[39;49mperc_threshold_fmri)\n\u001b[1;32m    128\u001b[0m     feats_test \u001b[39m=\u001b[39m get_embs_fmri(\n\u001b[1;32m    129\u001b[0m         X_test, mod, save_dir_fmri, perc_threshold\u001b[39m=\u001b[39mperc_threshold_fmri)\n\u001b[1;32m    130\u001b[0m \u001b[39melif\u001b[39;00m model\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39mvecs\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/fmri/notebooks/../02_fit_decoding.py:58\u001b[0m, in \u001b[0;36mget_embs_fmri\u001b[0;34m(X, model, save_dir_fmri, perc_threshold)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_embs_fmri\u001b[39m(X: List[\u001b[39mstr\u001b[39m], model, save_dir_fmri, perc_threshold\u001b[39m=\u001b[39m\u001b[39m98\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m     57\u001b[0m     \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mbert-\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m model\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mroberta\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m---> 58\u001b[0m         feats \u001b[39m=\u001b[39m get_ngram_vecs(X, model\u001b[39m=\u001b[39;49mmodel)\n\u001b[1;32m     59\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m         feats \u001b[39m=\u001b[39m get_word_vecs(X, model\u001b[39m=\u001b[39mmodel)\n",
      "File \u001b[0;32m~/fmri/notebooks/../02_fit_decoding.py:47\u001b[0m, in \u001b[0;36mget_ngram_vecs\u001b[0;34m(X, model)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39melif\u001b[39;00m model\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mroberta\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     46\u001b[0m     checkpoint \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mroberta-large\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 47\u001b[0m pipe \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39mfeature-extraction\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     48\u001b[0m                 model\u001b[39m=\u001b[39;49mcheckpoint,\n\u001b[1;32m     49\u001b[0m                 truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     50\u001b[0m                 device\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     51\u001b[0m ngram_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(model\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m__\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m])\n\u001b[1;32m     52\u001b[0m \u001b[39mreturn\u001b[39;00m feature_spaces\u001b[39m.\u001b[39mget_embs_from_text(\n\u001b[1;32m     53\u001b[0m     X, embedding_function\u001b[39m=\u001b[39mpipe, ngram_size\u001b[39m=\u001b[39mngram_size)\n",
      "File \u001b[0;32m~/.embgam/lib/python3.8/site-packages/transformers/pipelines/__init__.py:856\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    854\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m device\n\u001b[0;32m--> 856\u001b[0m \u001b[39mreturn\u001b[39;00m pipeline_class(model\u001b[39m=\u001b[39;49mmodel, framework\u001b[39m=\u001b[39;49mframework, task\u001b[39m=\u001b[39;49mtask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.embgam/lib/python3.8/site-packages/transformers/pipelines/base.py:778\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, modelcard, framework, task, args_parser, device, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[39m# Special handling\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 778\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    780\u001b[0m \u001b[39m# Update config with task specific parameters\u001b[39;00m\n\u001b[1;32m    781\u001b[0m task_specific_params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mtask_specific_params\n",
      "File \u001b[0;32m~/.embgam/lib/python3.8/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    925\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 927\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/.embgam/lib/python3.8/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.embgam/lib/python3.8/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.embgam/lib/python3.8/site-packages/torch/nn/modules/module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 602\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    603\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    604\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.embgam/lib/python3.8/site-packages/torch/nn/modules/module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    923\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 925\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoding_result='/home/chansingh/mntv1/deep-fMRI/results/encoding/bert-10__ndel=4/UTS03/',\n",
    "decoding_result = join('/home/chansingh/mntv1/deep-fMRI/results/linear_models/oct25',\n",
    "                         'coef_rotten_tomatoes_bert-10__ndel=4fmri_perc=0_seed=1.pkl')\n",
    "args = pkl.load(open(decoding_result.replace('coef_', ''), 'rb')).reset_index()\n",
    "args = args.iloc[0]\n",
    "assert args.perc_threshold_fmri == 0, 'Should run this script with perc=0!'\n",
    "X_train, y_train, X_test, y_test = fit_decoding.data.get_dsets(\n",
    "        args.dset, seed=args.seed, subsample_frac=0)\n",
    "\n",
    "feats_train, feats_test = fit_decoding.get_feats(\n",
    "        args.model, X_train, X_test,\n",
    "        subject_fmri=args.subject, perc_threshold_fmri=args.perc_threshold_fmri, args=args)\n",
    "\n",
    "norms = {\n",
    "        'feats_train_mean': feats_train.mean(axis=0),\n",
    "        'feats_train_std': feats_train.std(axis=0),\n",
    "        'feats_test_mean': feats_test.mean(axis=0),\n",
    "        'feats_test_std': feats_test.std(axis=0),\n",
    "}\n",
    "pkl.dump(norms, open)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_thresh, coefs = viz_cortex.load_corrs_and_coefs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(corrs_thresh, coefs, '.')\n",
    "plt.xlabel('corrs')\n",
    "plt.ylabel('coefs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.zeros((5, 3)).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.embgam')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "559535f78d940c882783b39501b2581b5193373045707e5f8a51d046029cfd38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
