{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import feature_spaces\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import encoding_utils, feature_spaces\n",
    "import pickle as pkl\n",
    "from collections import defaultdict\n",
    "from feature_spaces import *\n",
    "NUM_VOXELS = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stories, test_stories, allstories = encoding_utils.get_allstories([1, 2, 3, 4, 5])\n",
    "wordseqs = feature_spaces.get_story_wordseqs(test_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 307.34it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_words_for_story(wordseq, max_running_words=3) -> List[str]:\n",
    "    running_words = []\n",
    "    \n",
    "    words = np.array(wordseq.data)\n",
    "    TRIM = 5\n",
    "    tr_times = wordseq.tr_times[5+TRIM: -TRIM]\n",
    "    for i, tr_time in enumerate(tr_times):\n",
    "        valid_times = wordseq.data_times <= tr_time\n",
    "        # print(valid_times)\n",
    "        running_words.append(' '.join(words[valid_times][-max_running_words:]))\n",
    "    #     print(tr_time, running_words)\n",
    "    return running_words\n",
    "\n",
    "texts_list_test = []\n",
    "for story_name in tqdm(test_stories):\n",
    "    wordseq = wordseqs[story_name]\n",
    "    texts_list_test.append(get_words_for_story(wordseq))\n",
    "texts_test = sum(texts_list_test, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76296981 0.74258237 0.72107898 0.71616266 0.71508206]\n"
     ]
    }
   ],
   "source": [
    "subj = 'UTS03'\n",
    "\n",
    "# select top_idxs\n",
    "save_dir = '/home/chansingh/mntv1/deep-fMRI/results/encoding/bert-10__ndel=4/UTS03'\n",
    "corrs_val = np.load(join(save_dir, 'corrs.npz'))['arr_0']\n",
    "top_idxs = np.argsort(corrs_val)[::-1][:NUM_VOXELS]\n",
    "print(corrs_val[top_idxs][:5])\n",
    "\n",
    "# load responses (n_time_points x n_voxels)\n",
    "resp_test = encoding_utils.get_response(test_stories, subj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 18841.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# get the top ngrams for each voxel\n",
    "NUM_NGRAMS = 30\n",
    "top_ngrams = {}\n",
    "for i, voxel in enumerate(tqdm(top_idxs)):\n",
    "    top_resp_idxs = np.argsort(resp_test[:, i])[::-1]\n",
    "    top_ngrams[voxel] = [texts_test[idx][:NUM_NGRAMS] for idx in top_resp_idxs]\n",
    "top_ngrams = pd.DataFrame.from_dict(top_ngrams)\n",
    "top_ngrams.columns = [f'voxel_top_{i}' for i in range(NUM_VOXELS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>voxel_top_0</th>\n",
       "      <th>voxel_top_1</th>\n",
       "      <th>voxel_top_2</th>\n",
       "      <th>voxel_top_3</th>\n",
       "      <th>voxel_top_4</th>\n",
       "      <th>voxel_top_5</th>\n",
       "      <th>voxel_top_6</th>\n",
       "      <th>voxel_top_7</th>\n",
       "      <th>voxel_top_8</th>\n",
       "      <th>voxel_top_9</th>\n",
       "      <th>...</th>\n",
       "      <th>voxel_top_240</th>\n",
       "      <th>voxel_top_241</th>\n",
       "      <th>voxel_top_242</th>\n",
       "      <th>voxel_top_243</th>\n",
       "      <th>voxel_top_244</th>\n",
       "      <th>voxel_top_245</th>\n",
       "      <th>voxel_top_246</th>\n",
       "      <th>voxel_top_247</th>\n",
       "      <th>voxel_top_248</th>\n",
       "      <th>voxel_top_249</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>have my cigarettes</td>\n",
       "      <td>of his cigarette</td>\n",
       "      <td>and he sees</td>\n",
       "      <td>of light and</td>\n",
       "      <td>person comes closer</td>\n",
       "      <td>our thirty second</td>\n",
       "      <td>see her again</td>\n",
       "      <td>she had horses</td>\n",
       "      <td>you a ride</td>\n",
       "      <td>this suburban street</td>\n",
       "      <td>...</td>\n",
       "      <td>i ran i</td>\n",
       "      <td>i had no</td>\n",
       "      <td>at least once</td>\n",
       "      <td>want any part</td>\n",
       "      <td>the girl reaches</td>\n",
       "      <td>the neighborhood this</td>\n",
       "      <td>cigarettes for me</td>\n",
       "      <td>both need this</td>\n",
       "      <td>does the opposite</td>\n",
       "      <td>driver's license yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>see me and</td>\n",
       "      <td>you know let's</td>\n",
       "      <td>you know let's</td>\n",
       "      <td>pulls out a</td>\n",
       "      <td>and i walk</td>\n",
       "      <td>when i see</td>\n",
       "      <td>in the front</td>\n",
       "      <td>like we would</td>\n",
       "      <td>one match inside</td>\n",
       "      <td>cigarette and we</td>\n",
       "      <td>...</td>\n",
       "      <td>wider and wider</td>\n",
       "      <td>she says never</td>\n",
       "      <td>still understands how</td>\n",
       "      <td>someone who has</td>\n",
       "      <td>say you got</td>\n",
       "      <td>i was crying</td>\n",
       "      <td>that was close</td>\n",
       "      <td>i'm a waitress</td>\n",
       "      <td>someone who has</td>\n",
       "      <td>carl's efficiency apartments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>now to my</td>\n",
       "      <td>at home this</td>\n",
       "      <td>want any part</td>\n",
       "      <td>be friends i</td>\n",
       "      <td>and emergecenters and</td>\n",
       "      <td>and i walk</td>\n",
       "      <td>pulls out a</td>\n",
       "      <td>or lonely or</td>\n",
       "      <td>and i follow</td>\n",
       "      <td>when i see</td>\n",
       "      <td>...</td>\n",
       "      <td>and i say</td>\n",
       "      <td>i know it's</td>\n",
       "      <td>there is one</td>\n",
       "      <td>checking and it's</td>\n",
       "      <td>want any part</td>\n",
       "      <td>was ok because</td>\n",
       "      <td>our cigarettes she</td>\n",
       "      <td>one bedroom apartment</td>\n",
       "      <td>seven cigarettes on</td>\n",
       "      <td>have my cigarettes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>person comes closer</td>\n",
       "      <td>checking and it's</td>\n",
       "      <td>i'm not a</td>\n",
       "      <td>be we are</td>\n",
       "      <td>checking and it's</td>\n",
       "      <td>be friends i</td>\n",
       "      <td>very very bad</td>\n",
       "      <td>i ran i</td>\n",
       "      <td>we share some</td>\n",
       "      <td>and he says</td>\n",
       "      <td>...</td>\n",
       "      <td>walking and uh</td>\n",
       "      <td>still understands how</td>\n",
       "      <td>i ran i</td>\n",
       "      <td>great to meet</td>\n",
       "      <td>i'm a waitress</td>\n",
       "      <td>want any part</td>\n",
       "      <td>leads me to</td>\n",
       "      <td>learn this time</td>\n",
       "      <td>be friends i</td>\n",
       "      <td>i had no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i'm going home</td>\n",
       "      <td>was fun and</td>\n",
       "      <td>cigarette and we</td>\n",
       "      <td>me going she</td>\n",
       "      <td>has fifty million</td>\n",
       "      <td>saw you yeah</td>\n",
       "      <td>when i see</td>\n",
       "      <td>has fifty million</td>\n",
       "      <td>and i walk</td>\n",
       "      <td>who the fuck</td>\n",
       "      <td>...</td>\n",
       "      <td>i had no</td>\n",
       "      <td>ever tell people</td>\n",
       "      <td>i was crying</td>\n",
       "      <td>what our lives</td>\n",
       "      <td>i think no</td>\n",
       "      <td>stories about our</td>\n",
       "      <td>going on there's</td>\n",
       "      <td>be real nice</td>\n",
       "      <td>had my cigarettes</td>\n",
       "      <td>was gonna happen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           voxel_top_0        voxel_top_1       voxel_top_2   voxel_top_3  \\\n",
       "0   have my cigarettes   of his cigarette       and he sees  of light and   \n",
       "1           see me and     you know let's    you know let's   pulls out a   \n",
       "2            now to my       at home this     want any part  be friends i   \n",
       "3  person comes closer  checking and it's         i'm not a     be we are   \n",
       "4       i'm going home        was fun and  cigarette and we  me going she   \n",
       "\n",
       "             voxel_top_4        voxel_top_5    voxel_top_6        voxel_top_7  \\\n",
       "0    person comes closer  our thirty second  see her again     she had horses   \n",
       "1             and i walk         when i see   in the front      like we would   \n",
       "2  and emergecenters and         and i walk    pulls out a       or lonely or   \n",
       "3      checking and it's       be friends i  very very bad            i ran i   \n",
       "4      has fifty million       saw you yeah     when i see  has fifty million   \n",
       "\n",
       "        voxel_top_8           voxel_top_9  ...    voxel_top_240  \\\n",
       "0        you a ride  this suburban street  ...          i ran i   \n",
       "1  one match inside      cigarette and we  ...  wider and wider   \n",
       "2      and i follow            when i see  ...        and i say   \n",
       "3     we share some           and he says  ...   walking and uh   \n",
       "4        and i walk          who the fuck  ...         i had no   \n",
       "\n",
       "           voxel_top_241          voxel_top_242      voxel_top_243  \\\n",
       "0               i had no          at least once      want any part   \n",
       "1         she says never  still understands how    someone who has   \n",
       "2            i know it's           there is one  checking and it's   \n",
       "3  still understands how                i ran i      great to meet   \n",
       "4       ever tell people           i was crying     what our lives   \n",
       "\n",
       "      voxel_top_244          voxel_top_245       voxel_top_246  \\\n",
       "0  the girl reaches  the neighborhood this   cigarettes for me   \n",
       "1       say you got           i was crying      that was close   \n",
       "2     want any part         was ok because  our cigarettes she   \n",
       "3    i'm a waitress          want any part         leads me to   \n",
       "4        i think no      stories about our    going on there's   \n",
       "\n",
       "           voxel_top_247        voxel_top_248                 voxel_top_249  \n",
       "0         both need this    does the opposite          driver's license yet  \n",
       "1         i'm a waitress      someone who has  carl's efficiency apartments  \n",
       "2  one bedroom apartment  seven cigarettes on            have my cigarettes  \n",
       "3        learn this time         be friends i                      i had no  \n",
       "4           be real nice    had my cigarettes              was gonna happen  \n",
       "\n",
       "[5 rows x 250 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_ngrams.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ngrams.to_pickle('top_ngrams.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.embgam')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "559535f78d940c882783b39501b2581b5193373045707e5f8a51d046029cfd38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
