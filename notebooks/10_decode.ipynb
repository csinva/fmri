{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ridge_utils.DataSequence import DataSequence\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import dirname\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from neuro.features import qa_questions, feature_spaces\n",
    "from neuro.data import story_names, response_utils\n",
    "from neuro.features.stim_utils import load_story_wordseqs, load_story_wordseqs_huge\n",
    "import neuro.config\n",
    "import seaborn as sns\n",
    "from neuro.features.questions.gpt4 import QS_35_STABLE\n",
    "import numpy as np\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "from os.path import join\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from neuro import analyze_helper\n",
    "import dvu\n",
    "import sasc.viz\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import numpy as np\n",
    "from neuro.analyze_helper import abbrev_question\n",
    "from neuro.config import repo_dir, PROCESSED_DIR, setup_freesurfer\n",
    "from copy import deepcopy\n",
    "dvu.set_style()\n",
    "data_dir = join(neuro.config.repo_dir, 'data', 'decoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fmri_and_labs(data_dir, story_name='onapproachtopluto', train_or_test='test', subject='uts03'):\n",
    "    '''\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        The fMRI features, with columns corresponding to the principal components\n",
    "        of the fMRI data.\n",
    "    labs : pd.DataFrame\n",
    "        Binary labeled annotations for each of the texts\n",
    "    texts: \n",
    "        The texts corresponding to the rows of df\n",
    "    '''\n",
    "    df = joblib.load(\n",
    "        join(data_dir, subject, train_or_test, story_name + '.pkl'))\n",
    "    dfs = []\n",
    "    for offset in [1, 2, 3, 4]:\n",
    "        df_offset = df.shift(-offset)\n",
    "        df_offset.columns = [col + f'_{offset}' for col in df.columns]\n",
    "        dfs.append(df_offset)\n",
    "    df = pd.concat(dfs, axis=1)  # .dropna()  # would want to dropna here\n",
    "\n",
    "    # load labels\n",
    "    labs = joblib.load(\n",
    "        join(data_dir, 'labels', train_or_test, story_name + '_labels.pkl'))\n",
    "\n",
    "    # drop rows with nans\n",
    "    idxs_na = df.isna().sum(axis=1).values > 0\n",
    "    df = df[~idxs_na]\n",
    "    labs = labs[~idxs_na]\n",
    "    texts = pd.Series(df.index)\n",
    "    return df, labs, texts\n",
    "\n",
    "\n",
    "def concatenate_running_texts(texts, frac=1/2):\n",
    "    '''When decoding, you might want to concatenate \n",
    "    the text of the current and surrounding texts\n",
    "    to deal with the temporal imprecision of the fMRI signal.\n",
    "    '''\n",
    "    texts_before = (\n",
    "        texts.shift(1)\n",
    "        .str.split().apply(  # only keep second half of words\n",
    "            lambda l: ' '.join(l[int(-len(l) * frac):]) if l else '')\n",
    "    )\n",
    "\n",
    "    texts_after = (\n",
    "        texts.shift(-1)\n",
    "        .str.split().apply(  # only keep first half of words\n",
    "            lambda l: ' '.join(l[:int(len(l) * frac)]) if l else '')\n",
    "    )\n",
    "\n",
    "    return texts_before + ' ' + texts + ' ' + texts_after\n",
    "\n",
    "# example get data\n",
    "# df_orig, labs, texts = get_fmri_and_labs(data_dir)\n",
    "# texts = concatenate_running_texts(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run decode for all questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the data for a single subject\n",
    "data_by_subject = {}\n",
    "for subject in ['uts01', 'uts02', 'uts03']:\n",
    "    data = defaultdict(list)\n",
    "    for train_or_test in ['test', 'train']:\n",
    "        story_names_list = os.listdir(join(data_dir, subject, train_or_test))\n",
    "        for story_name in story_names_list:\n",
    "            df, labs, texts = get_fmri_and_labs(\n",
    "                data_dir, story_name.replace('.pkl', ''), train_or_test, subject)\n",
    "            data['df_' + train_or_test].append(df)\n",
    "            data['labs_' + train_or_test].append(labs)\n",
    "            data['texts_' + train_or_test].append(texts)\n",
    "\n",
    "    for k in data:\n",
    "        data[k] = pd.concat(data[k], axis=0)\n",
    "    data_by_subject[subject] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to None to keep all coefs\n",
    "num_coefs_to_keep = None\n",
    "\n",
    "\n",
    "def _keep_only_few_coefs(X, num_coefs_to_keep, num_features=200, num_delays=4):\n",
    "    if num_coefs_to_keep is None:\n",
    "        return X\n",
    "    else:\n",
    "        idxs_to_keep = np.arange(num_coefs_to_keep)\n",
    "        idxs_to_keep = np.concatenate(\n",
    "            [idxs_to_keep + i * num_features for i in range(num_delays)])\n",
    "        return X[:, idxs_to_keep]\n",
    "\n",
    "\n",
    "for subject in ['uts01', 'uts02', 'uts03'][::-1]:\n",
    "    data = data_by_subject[subject]\n",
    "\n",
    "    # example fit linear decoder\n",
    "    r = defaultdict(list)\n",
    "    for label_num in tqdm(range(data['labs_train'].shape[1])):\n",
    "        X_train, y_train = data['df_train'].values, data['labs_train'].values[:, label_num]\n",
    "        X_test, y_test = data['df_test'].values, data['labs_test'].values[:, label_num]\n",
    "        print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "        # balance the binary class imbalance\n",
    "        try:\n",
    "            rus = RandomUnderSampler(random_state=42)\n",
    "            X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "            X_test, y_test = rus.fit_resample(X_test, y_test)\n",
    "\n",
    "            # only keep 5 feats\n",
    "            X_train = _keep_only_few_coefs(X_train, num_coefs_to_keep)\n",
    "            X_test = _keep_only_few_coefs(X_test, num_coefs_to_keep)\n",
    "\n",
    "            if len(y_test) < 30:\n",
    "                print('too few positive labels', label_num)\n",
    "                continue\n",
    "\n",
    "            print('label', label_num,\n",
    "                  data['labs_train'].columns[label_num], X_train.shape, X_test.shape)\n",
    "            m = LogisticRegressionCV(random_state=42)\n",
    "            m.fit(X_train, y_train)\n",
    "            test_acc = m.score(X_test, y_test)\n",
    "            print(\n",
    "                f\"\"\"\\ttest acc {test_acc:.3f}\"\"\")  # \\n\\tnaive acc {1 -y_test.mean():.3f}\"\"\")\n",
    "            r['label'].append(data['labs_train'].columns[label_num])\n",
    "            # y_pred = m.predict(X_test)\n",
    "            # balanced_accuracy_score(y_test, y_pred))\n",
    "            r['test_acc'].append(test_acc)\n",
    "            r['num_test'].append(len(y_test))\n",
    "            r['coef'].append(m.coef_.copy())\n",
    "\n",
    "            # extra test data from another subject ##########\n",
    "            test_acc_ood = []\n",
    "            for subject_ood in ['uts01', 'uts02', 'uts03']:\n",
    "                if subject_ood == subject:\n",
    "                    continue\n",
    "                X_test_ood, y_test_ood = data_by_subject[subject_ood][\n",
    "                    'df_test'].values, data_by_subject[subject_ood]['labs_test'].values[:, label_num]\n",
    "\n",
    "                # balance the binary class imbalance\n",
    "                rus = RandomUnderSampler(random_state=42)\n",
    "                X_test_ood, y_test_ood = rus.fit_resample(\n",
    "                    X_test_ood, y_test_ood)\n",
    "\n",
    "                X_test_ood = _keep_only_few_coefs(\n",
    "                    X_test_ood, num_coefs_to_keep)\n",
    "\n",
    "                test_acc_ood_subject = m.score(X_test_ood, y_test_ood)\n",
    "                test_acc_ood.append(test_acc_ood_subject)\n",
    "            r['test_acc_ood'].append(np.mean(test_acc_ood))\n",
    "\n",
    "            print(f\"\"\"\\ttest acc ood {np.mean(test_acc_ood):.3f}\"\"\")\n",
    "            ###############################################\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('error for', label_num)\n",
    "            continue\n",
    "    r_df = pd.DataFrame(r)\n",
    "    # .sort_values(\n",
    "    # metric, ascending=False).reset_index()\n",
    "    r_df.to_pickle(join(data_dir, f'r_df_{subject}.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot learned decoding scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_result(subject):\n",
    "    if subject == 'mean':\n",
    "        dfs = [pd.read_pickle(join(data_dir, f'r_df_{subject}.pkl'))\n",
    "               for subject in ['uts01', 'uts02', 'uts03']]\n",
    "        return pd.concat(dfs, axis=0).groupby('label').mean().reset_index()\n",
    "    else:\n",
    "        return pd.read_pickle(join(data_dir, f'r_df_{subject}.pkl'))\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(13, 13))\n",
    "plt.figure(figsize=(8, 8))\n",
    "fig, ax = plt.subplots()\n",
    "colors = {\n",
    "    'uts01': 'C0',\n",
    "    'uts02': 'C1',\n",
    "    'uts03': 'C2',\n",
    "    'mean': 'black'\n",
    "}\n",
    "metric = 'test_acc'\n",
    "# metric = 'test_acc_ood'\n",
    "for subject in ['mean', 'uts01', 'uts02', 'uts03']:\n",
    "    r_df = _load_result(subject)\n",
    "    if subject == 'mean':\n",
    "        idx_sort = r_df[metric].sort_values(ascending=False).index\n",
    "\n",
    "    r_df = r_df.loc[idx_sort]\n",
    "    r_df = r_df[r_df['label'].isin(QS_35_STABLE)]\n",
    "\n",
    "    # plot accuracy with binomial error bars\n",
    "    if subject == 'mean':\n",
    "        plt.errorbar(\n",
    "            r_df[metric],\n",
    "            range(len(r_df)),\n",
    "            color='black',\n",
    "            fmt='o',\n",
    "            ms=4,\n",
    "            zorder=1000,\n",
    "            label=subject.capitalize(),\n",
    "        )\n",
    "        plt.axvline(r_df[metric].mean(), color=colors[subject])\n",
    "    else:\n",
    "        plt.errorbar(\n",
    "            r_df[metric],\n",
    "            range(len(r_df)),\n",
    "            xerr=np.sqrt(\n",
    "                r_df[metric] * (1-r_df[metric])\n",
    "                / r_df['num_test']),\n",
    "            alpha=0.5,\n",
    "            label=subject.upper(),\n",
    "            fmt='o',\n",
    "            ms=4,\n",
    "        )\n",
    "        plt.axvline(r_df[metric].mean(),\n",
    "                    linestyle='--', color=colors[subject])\n",
    "\n",
    "    print('mean acc', r_df[metric].mean())\n",
    "\n",
    "fs = 'x-small'\n",
    "# add horizontal bars\n",
    "labs = [analyze_helper.abbrev_question(q) for q in r_df['label']]\n",
    "plt.yticks(range(len(r_df)), labs, fontsize=fs)\n",
    "plt.xlabel(\n",
    "    'Decoding accuracy', fontsize=fs)\n",
    "plt.xticks(fontsize=fs)\n",
    "# ax.yaxis.set_major_locator(ticker.MultipleLocator(2))\n",
    "# Get the current tick locations\n",
    "xticks = ax.get_xticks()\n",
    "yticks = ax.get_yticks()\n",
    "\n",
    "# Select every other tick\n",
    "# ax.set_xticks(xticks[::2])\n",
    "ax.set_yticks(yticks[::2], minor=True)\n",
    "ax.grid(True, which='major', linewidth=0.7, alpha=0.6)\n",
    "\n",
    "\n",
    "# annotate with baseline and text label\n",
    "plt.axvline(0.5, color='darkgray', linestyle='--')\n",
    "plt.text(0.51, 0, 'Chance', color='darkgray', fontsize=fs, ha='left')\n",
    "plt.legend(fontsize='small')\n",
    "plt.tight_layout()\n",
    "plt.savefig('linear_decoding.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize learned coefs on cortex map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in ['uts01', 'uts02', 'uts03']:\n",
    "    # subject = 'uts03'\n",
    "    pca_comps = joblib.load(join(data_dir, f'{subject}/pca_components.pkl'))\n",
    "    # vertically stack pca_comps 4 times\n",
    "    pca_comps = np.vstack([pca_comps]*4)\n",
    "    r_df = _load_result(subject)\n",
    "\n",
    "    for i in tqdm(range(len(r_df))):\n",
    "        pc_coef = r_df.iloc[i]['coef']\n",
    "        question = r_df.iloc[i]['label']\n",
    "        if question not in QS_35_STABLE:\n",
    "            continue\n",
    "        voxel_coefs = (pc_coef @ pca_comps).squeeze()\n",
    "        sasc.viz.quickshow(\n",
    "            voxel_coefs,\n",
    "            subject,\n",
    "            fname_save=join(data_dir, 'flatmaps_decoding', subject, f'{question}.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames_arr = []\n",
    "\n",
    "# make grid of plots\n",
    "nrows = 6\n",
    "ncols = 35\n",
    "axes = plt.subplots(nrows=nrows, ncols=ncols,\n",
    "                    figsize=(4.5 * ncols, 3 * nrows))[1]\n",
    "\n",
    "for i, subject in enumerate(['uts01', 'uts02', 'uts03']):\n",
    "    for j, label in enumerate(tqdm(QS_35_STABLE)):\n",
    "        fname = join(data_dir, 'flatmaps_decoding', subject, f'{label}.png')\n",
    "        axes[i, j].imshow(mpimg.imread(fname))\n",
    "        axes[i, j].axis('off')\n",
    "        if i == 0:\n",
    "            axes[i, j].set_title(abbrev_question(label), fontsize='x-small')\n",
    "        if j == 0:\n",
    "            # axes[i, j].set_ylabel(\n",
    "            # subject.upper() + ' Decode', fontsize='x-small')\n",
    "            # show rotated text where ylabel would be\n",
    "            axes[i, j].text(-0.5, 0.5, subject.upper() + ' Decode',\n",
    "                            fontsize='x-small', rotation=90, ha='center', va='center')\n",
    "\n",
    "        fname_enc = join(repo_dir, 'qa_results', 'figs',\n",
    "                         'flatmaps_export', subject.upper().replace('UT', ''), label + '.png')\n",
    "        axes[3 + i, j].imshow(mpimg.imread(fname_enc))\n",
    "        axes[3 + i, j].axis('off')\n",
    "        if j == 0:\n",
    "            # axes[3 + i, j].set_ylabel(subject.upper() +\n",
    "            #   ' Encode', fontsize='x-small')\n",
    "            # show rotated text where ylabel would be\n",
    "            axes[3 + i, j].text(-0.5, 0.5, subject.upper() + ' Encode',\n",
    "                                fontsize='x-small', rotation=90, ha='center', va='center')\n",
    "\n",
    "plt.savefig(join(data_dir, 'flatmaps_decoding', 'all.pdf'))  # , dpi=300)\n",
    "# for subject in ['uts01', 'uts02', 'uts03']:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare learned coef distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in ['S01', 'S02', 'S03']:\n",
    "    subject_dec = 'ut' + subject.lower()\n",
    "\n",
    "    # subject = 'uts03'\n",
    "    pca_comps = joblib.load(\n",
    "        join(data_dir, f'{subject_dec}/pca_components.pkl'))\n",
    "    # vertically stack pca_comps 4 times\n",
    "    pca_comps = np.vstack([pca_comps]*4)\n",
    "    r_df = _load_result(subject).loc[QS_35_STABLE]\n",
    "\n",
    "    for i, q in tqdm(QS_35_STABLE):\n",
    "\n",
    "        settings = ['individual_gpt4_ndel=1_pc_new']\n",
    "        flatmap_qa = joblib.load(\n",
    "            join(PROCESSED_DIR, subject.replace('UT', ''), 'individual_gpt4_ndel=1_pc_new.pkl'))\n",
    "\n",
    "        pc_coef = r_df.iloc[i]['coef']\n",
    "        flatmap_decode = (pc_coef @ pca_comps).squeeze()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
