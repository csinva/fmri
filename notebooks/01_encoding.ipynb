{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import dvu\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from matplotlib import pyplot as plt\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import imodelsx.process_results\n",
    "import qa_questions\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import analyze_helper\n",
    "fit_encoding = __import__('01_fit_encoding')\n",
    "dvu.set_style()\n",
    "\n",
    "results_dir = '/home/chansingh/mntv1/deep-fMRI/encoding/results_apr7'\n",
    "r, cols_varied, mets = analyze_helper.load_clean_results(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imodelsx.process_results.delete_runs_in_dataframe(\n",
    "    r[r.seed != 1], actually_delete=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = r\n",
    "d = d[d.feature_selection_alpha_index < 0]\n",
    "d = d[d.distill_model_path == 'None']\n",
    "cols_varied = [c for c in cols_varied if not c in [\n",
    "    'distill_model_path', 'feature_selection_alpha_index']]\n",
    "# d = d[(d.qa_questions_version == 'v1') *\n",
    "#   (d.qa_embedding_model == 'mistral 7B')]\n",
    "if len(cols_varied) > 0:\n",
    "    d = d.groupby(cols_varied)[mets].mean()\n",
    "else:\n",
    "    d = d[mets]\n",
    "\n",
    "(\n",
    "    d\n",
    "    # .sort_values(by='corrs_test_mean', ascending=False)\n",
    "    .sort_values(by='corrs_tune_pc_mean', ascending=False)\n",
    "    .rename(columns=lambda x: x.replace('_', ' ').replace('corrs', ''))\n",
    "    .style\n",
    "    .background_gradient(cmap='magma', axis=0)\n",
    "    .format(precision=3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-24 13:19:04 llm_engine.py:87] Initializing an LLM engine with config: model='gpt2', tokenizer='gpt2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "INFO 04-24 13:19:07 weight_utils.py:163] Using model weights format ['*.safetensors']\n",
      "INFO 04-24 13:19:08 llm_engine.py:357] # GPU blocks: 71336, # CPU blocks: 7281\n",
      "INFO 04-24 13:19:10 model_runner.py:684] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-24 13:19:10 model_runner.py:688] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 04-24 13:19:15 model_runner.py:756] Graph capturing finished in 5 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 4/4 [00:00<00:00, 67.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Hello, my name is', Generated text: \" Scott. I'm a social media manager at Amazon, working as a Data Scientist\"\n",
      "Prompt: 'The president of the United States is', Generated text: ' not sitting right now.\\n\\nDemocrats made a concerted effort to make his name'\n",
      "Prompt: 'The capital of France is', Generated text: ' now home to one of the most powerful Muslim governments in the world, but its'\n",
      "Prompt: 'The future of AI is', Generated text: ' a mess of algorithms and algorithms designed to be faster, more flexible and better.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(prompts, sampling_params, prefix_pos)\n",
    "\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # look at distill_model_path\n",
    "# d = r[(r.feature_space == 'qa_embedder-10') * (r.pc_components == 100)]\n",
    "# d = d.groupby(cols_varied)[mets].mean()\n",
    "# d.pivot_table(index=[c for c in cols_varied if not c == 'distill_model_path'],\n",
    "#               columns='distill_model_path', values='corrs_test_mean', aggfunc='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = r[(r.feature_space == 'qa_embedder-10') * (r.pc_components == 100)]\n",
    "d = d[d.ndelays == 8]\n",
    "d = d.groupby(cols_varied)[mets].mean()\n",
    "(\n",
    "    d.pivot_table(index=[c for c in cols_varied if not c == 'qa_embedding_model'],\n",
    "                  columns='qa_embedding_model', values='corrs_test_mean', aggfunc='mean')\n",
    "    .style.background_gradient(cmap='magma')  # , axis=0)\n",
    "    .format(precision=3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare performance of a few different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa = r[(r.feature_space == 'qa_embedder-5')\n",
    "# ].sort_values(by='corrs_tune_pc_mean', ascending=False).iloc[0]\n",
    "qa = r.iloc[0]\n",
    "eng1000 = r[(r.feature_space == 'eng1000')].sort_values(\n",
    "    by='corrs_tune_pc_mean', ascending=False).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(qa['corrs_test'], eng1000['corrs_test'], '.', ms=1)\n",
    "plt.xlabel(f'QA Embedder (mean: {qa[\"corrs_test\"].mean():0.3f})')\n",
    "plt.ylabel(f'Eng1000 (mean: {eng1000[\"corrs_test\"].mean():0.3f})')\n",
    "plt.title('Test Correlations')\n",
    "m_max = max(qa['corrs_test'].max(), eng1000['corrs_test'].max())\n",
    "m_min = min(qa['corrs_test'].min(), eng1000['corrs_test'].min())\n",
    "plt.plot([m_min, m_max], [m_min, m_max], 'k--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check parameters for rerunning expts (alphas, delays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = r[(r.pc_components == -1) * (r.ndelays == 8)].iloc[0]\n",
    "args = r.sort_values(by='corrs_test_mean').iloc[-1]\n",
    "model_params_to_save = joblib.load(\n",
    "    join(args.save_dir_unique, 'model_params.pkl'))\n",
    "print(args.feature_space, args.pc_components, args.ndelays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print which alphas are being used\n",
    "pd.Series(model_params_to_save['alphas_best']).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = r[(r.pc_components == -1) * (r.feature_space == 'qa_embedder-5')\n",
    "         ].sort_values(by='corrs_tune_mean', ascending=False).iloc[0]\n",
    "args2 = r[(r.pc_components > 0) * (r.feature_space == 'qa_embedder-5')\n",
    "          ].sort_values(by='corrs_tune_mean', ascending=False).iloc[0]\n",
    "# args = r[]\n",
    "\n",
    "# args2 = r[(r.feature_space == 'eng1000')].iloc[0]\n",
    "# args = r[(r.pc_components == -1) * (r.feature_space == 8)].iloc[0]\n",
    "# args = r[(r.pc_components == -1) * (r.ndelays == 8)].iloc[0]\n",
    "# model_params_to_save = joblib.load(\n",
    "# join(args.save_dir_unique, 'model_params.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_thresholds = np.arange(0, 100, 1)\n",
    "corrs_tune_individual = args['corrs_tune']\n",
    "corrs_test_individual = args['corrs_test']\n",
    "corrs_test_pca = args2['corrs_test']\n",
    "res = []\n",
    "for percentile_threshold in percentile_thresholds:\n",
    "    args_top_thresh = np.where(corrs_tune_individual > np.percentile(\n",
    "        corrs_tune_individual, percentile_threshold))[0]\n",
    "    args_non_top_thresh = np.where(corrs_tune_individual <= np.percentile(\n",
    "        corrs_tune_individual, percentile_threshold))[0]\n",
    "    args_total = np.concatenate([args_top_thresh, args_non_top_thresh])\n",
    "    mean_corr_weighted = (corrs_test_individual[args_top_thresh].mean() * args_top_thresh.size +\n",
    "                          corrs_test_pca[args_non_top_thresh].mean() * args_non_top_thresh.size) / args_total.size\n",
    "    # print('mean corr weighted',\n",
    "    #       (corrs_test_individual[args_top_thresh].mean() * args_top_thresh.size +\n",
    "    #        corrs_test_pca[args_non_top_thresh].mean() * args_non_top_thresh.size) / args_total.size\n",
    "    #       )\n",
    "    res.append(mean_corr_weighted)\n",
    "plt.plot(percentile_thresholds, res)\n",
    "plt.axhline(corrs_test_individual.mean(), color='k', linestyle='--')\n",
    "plt.axhline(corrs_test_pca.mean(), color='r', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate model weights usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = r.iloc[0]\n",
    "story_names_train, story_names_test = fit_encoding.get_story_names(args)\n",
    "stim_test_delayed, resp_test = fit_encoding.get_data(\n",
    "    args, story_names_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test stim is all 0?\n",
    "np.unique(stim_test_delayed, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_corrs(preds, resp):\n",
    "    corrs = []\n",
    "    for i in tqdm(range(preds.shape[1])):\n",
    "        corrs.append(np.corrcoef(\n",
    "            preds[:, i], resp[:, i])[0, 1])\n",
    "    return np.array(corrs)\n",
    "\n",
    "\n",
    "wt = model_params_to_save['weights']\n",
    "preds_test = stim_test_delayed @ wt\n",
    "corrs_test = _calc_corrs(preds_test, resp_test)\n",
    "print(np.mean(corrs_test))\n",
    "print(args.corrs_test_mean)\n",
    "assert np.allclose(corrs_test, args['corrs_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate PC models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = r[(r.pc_components == 100) * (r.feature_space == 'qa_embedder-5')\n",
    "         ].sort_values(by='corrs_tune_mean', ascending=False).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_names_train, story_names_test = fit_encoding.get_story_names(args)\n",
    "stim_test_delayed, resp_test = fit_encoding.get_data(args, story_names_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params_to_save = joblib.load(\n",
    "    join(args.save_dir_unique, 'model_params.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = model_params_to_save['weights']\n",
    "# + model_params_to_save['bias'] (not needed for just calculating corr, but needed for predictions)\n",
    "preds_test = stim_test_delayed @ wt\n",
    "\n",
    "corrs_test = _calc_corrs(preds_test, resp_test)\n",
    "print(np.mean(corrs_test))\n",
    "print(args.corrs_test_mean)\n",
    "assert np.allclose(corrs_test, args['corrs_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original setup, before we had unpacked weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_pc = model_params_to_save['weights_pc']\n",
    "pca = model_params_to_save['pca']\n",
    "scaler_test = model_params_to_save['scaler_test']\n",
    "preds_pc = stim_test_delayed @ wt_pc\n",
    "preds_pc_scaled = scaler_test.inverse_transform(preds_pc)\n",
    "preds_voxels = pca.inverse_transform(preds_pc_scaled)\n",
    "corrs_test = _calc_corrs(preds_voxels, resp_test)\n",
    "assert np.allclose(corrs_test, args['corrs_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary number of pcs included\n",
    "num_pcs = [5, 25, 50, 100, 200, 500, 1000]\n",
    "corrs = []\n",
    "for num_pc in num_pcs:\n",
    "    wt_pc = model_params_to_save['weights_pc']\n",
    "    pca = model_params_to_save['pca']\n",
    "    scaler_test = model_params_to_save['scaler_test']\n",
    "    preds_pc = stim_test_delayed @ wt_pc\n",
    "    preds_pc_scaled = scaler_test.inverse_transform(preds_pc)\n",
    "\n",
    "    pca_subset = deepcopy(pca)\n",
    "    pca_subset.components_[num_pc:] = 0\n",
    "    preds_voxels = pca_subset.inverse_transform(preds_pc_scaled)\n",
    "\n",
    "    corrs_test = _calc_corrs(preds_voxels, resp_test)\n",
    "    # assert np.allclose(corrs_test, args['corrs_test'])\n",
    "    print(num_pc, np.mean(corrs_test))\n",
    "    corrs.append(np.mean(corrs_test))\n",
    "plt.plot(num_pcs, corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_test = fit_encoding.evaluate_pc_model_on_each_voxel(\n",
    "    args, stim_test_delayed, resp_test,\n",
    "    model_params_to_save, pca, scaler_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = model_params_to_save['weights']\n",
    "\n",
    "# multiply\n",
    "preds_pc = stim_test_delayed @ wt\n",
    "preds_pc_unscaled = preds_pc * scaler_test.scale_ + scaler_test.mean_\n",
    "preds_voxels2 = preds_pc_unscaled @ pca.components_ + pca.mean_\n",
    "\n",
    "# rewrite the above as a multiplication of a single weight matrix\n",
    "preds_voxels2 = (stim_test_delayed @ wt * scaler_test.scale_ +\n",
    "                 scaler_test.mean_) @ pca.components_ + pca.mean_\n",
    "weight_full = wt * scaler_test.scale_ @ pca.components_\n",
    "bias_full = scaler_test.mean_ @ pca.components_ + pca.mean_\n",
    "preds_voxels2 = stim_test_delayed @ weight_full + bias_full\n",
    "\n",
    "assert np.allclose(preds_voxels, preds_voxels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert corrs_test.mean() == args.corrs_test_mean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.embgam')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "559535f78d940c882783b39501b2581b5193373045707e5f8a51d046029cfd38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
