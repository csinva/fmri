{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from os.path import join, expanduser\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import sys\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from neuro.features.feat_select import get_alphas\n",
    "from neuro.features.qa_questions import get_questions, get_merged_questions_v3_boostexamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_feats_dir = expanduser('~/mntv1/deep-fMRI/qa/sparse_feats_shared/')\n",
    "os.listdir(sparse_feats_dir)\n",
    "qa_sparse_feats_dir = join(\n",
    "    sparse_feats_dir,\n",
    "    # 'qa_embedder___qa_questions_version=v3_boostexamples___ensemble1')\n",
    "    'qa_embedder___qa_questions_version=v3_boostexamples_merged___ensemble2')\n",
    "# 'eng1000___qa_questions_version=v1___mistralai-Mistral-7B-Instruct-v0.2')\n",
    "# alphas = np.logspace(0, -3, 20)\n",
    "alphas = get_alphas('qa_embedder')\n",
    "# alphas = get_alphas('eng1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/chansingh/mntv1/deep-fMRI/qa/sparse_feats_shared/qa_embedder___qa_questions_version=v3_boostexamples_merged___ensemble2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m nonzeros \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(index\u001b[38;5;241m=\u001b[39mseeds, columns\u001b[38;5;241m=\u001b[39malphas)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m      3\u001b[0m enets \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(index\u001b[38;5;241m=\u001b[39mseeds, columns\u001b[38;5;241m=\u001b[39malphas)\n\u001b[0;32m----> 4\u001b[0m fnames \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqa_sparse_feats_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m tqdm(seeds):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m tqdm(alphas):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/chansingh/mntv1/deep-fMRI/qa/sparse_feats_shared/qa_embedder___qa_questions_version=v3_boostexamples_merged___ensemble2'"
     ]
    }
   ],
   "source": [
    "seeds = range(5)\n",
    "nonzeros = pd.DataFrame(index=seeds, columns=alphas).astype(float)\n",
    "enets = pd.DataFrame(index=seeds, columns=alphas)\n",
    "fnames = os.listdir(qa_sparse_feats_dir)\n",
    "for seed in tqdm(seeds):\n",
    "    for alpha in tqdm(alphas):\n",
    "        template = f'seed={seed}___feature_selection_frac=0.50___feature_selection_alpha={alpha:.2e}.joblib'\n",
    "        if template in fnames:\n",
    "            coef_enet = joblib.load(join(qa_sparse_feats_dir, template))\n",
    "            coef_enet_selected = deepcopy(\n",
    "                np.any(np.abs(coef_enet) > 0, axis=0).squeeze())\n",
    "            print(f'{coef_enet.shape=}')\n",
    "            enets.loc[seed, alpha] = coef_enet_selected\n",
    "            nonzeros.loc[seed, alpha] = coef_enet_selected.sum()\n",
    "# template = f'seed={seed}___feature_selection_frac=0.50___feature_selection_alpha={feature_selection_alpha:.2e}.joblib'\n",
    "# os.listdir(qa_sparse_feats_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(coefs_list)=5\n",
      "len(coefs_list)=5\n",
      "len(coefs_list)=5\n",
      "len(coefs_list)=5\n",
      "len(coefs_list)=5\n",
      "len(coefs_list)=5\n",
      "len(coefs_list)=5\n",
      "len(coefs_list)=5\n",
      "len(coefs_list)=5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_c769d_row0_col0, #T_c769d_row1_col0, #T_c769d_row2_col0, #T_c769d_row3_col0, #T_c769d_row4_col0, #T_c769d_row5_col0 {\n",
       "  background-color: #440154;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c769d_row0_col1, #T_c769d_row4_col1 {\n",
       "  background-color: #450559;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c769d_row0_col2, #T_c769d_row1_col2 {\n",
       "  background-color: #460b5e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c769d_row0_col3 {\n",
       "  background-color: #481a6c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c769d_row0_col4 {\n",
       "  background-color: #482677;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c769d_row0_col5, #T_c769d_row4_col5 {\n",
       "  background-color: #39558c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c769d_row0_col6 {\n",
       "  background-color: #1f988b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c769d_row0_col7 {\n",
       "  background-color: #77d153;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c769d_row0_col8, #T_c769d_row2_col8 {\n",
       "  background-color: #fde725;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c769d_row1_col1, #T_c769d_row2_col1, #T_c769d_row3_col1 {\n",
       "  background-color: #46075a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c769d_row1_col3, #T_c769d_row4_col3 {\n",
       "  background-color: #48186a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c769d_row1_col4, #T_c769d_row3_col4 {\n",
       "  background-color: #472a7a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c769d_row1_col5 {\n",
       "  background-color: #375a8c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c769d_row1_col6 {\n",
       "  background-color: #1f978b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c769d_row1_col7 {\n",
       "  background-color: #7ad151;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c769d_row1_col8, #T_c769d_row4_col8 {\n",
       "  background-color: #000000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c769d_row2_col2, #T_c769d_row3_col2, #T_c769d_row4_col2 {\n",
       "  background-color: #470d60;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c769d_row2_col3, #T_c769d_row3_col3 {\n",
       "  background-color: #481769;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c769d_row2_col4 {\n",
       "  background-color: #472e7c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c769d_row2_col5 {\n",
       "  background-color: #38598c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c769d_row2_col6 {\n",
       "  background-color: #1e9b8a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c769d_row2_col7 {\n",
       "  background-color: #73d056;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c769d_row3_col5 {\n",
       "  background-color: #39568c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c769d_row3_col6 {\n",
       "  background-color: #1fa088;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c769d_row3_col7 {\n",
       "  background-color: #6ece58;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c769d_row3_col8 {\n",
       "  background-color: #fbe723;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c769d_row4_col4 {\n",
       "  background-color: #482979;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c769d_row4_col6 {\n",
       "  background-color: #1f998a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c769d_row4_col7 {\n",
       "  background-color: #7cd250;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c769d_row5_col1 {\n",
       "  background-color: #450457;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c769d_row5_col2 {\n",
       "  background-color: #46085c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c769d_row5_col3 {\n",
       "  background-color: #470e61;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c769d_row5_col4 {\n",
       "  background-color: #481b6d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c769d_row5_col5 {\n",
       "  background-color: #424086;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c769d_row5_col6 {\n",
       "  background-color: #29798e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c769d_row5_col7 {\n",
       "  background-color: #37b878;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c769d_row5_col8 {\n",
       "  background-color: #e5e419;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_c769d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >alpha</th>\n",
       "      <th id=\"T_c769d_level0_col0\" class=\"col_heading level0 col0\" >0.483293</th>\n",
       "      <th id=\"T_c769d_level0_col1\" class=\"col_heading level0 col1\" >0.400000</th>\n",
       "      <th id=\"T_c769d_level0_col2\" class=\"col_heading level0 col2\" >0.335982</th>\n",
       "      <th id=\"T_c769d_level0_col3\" class=\"col_heading level0 col3\" >0.280000</th>\n",
       "      <th id=\"T_c769d_level0_col4\" class=\"col_heading level0 col4\" >0.233572</th>\n",
       "      <th id=\"T_c769d_level0_col5\" class=\"col_heading level0 col5\" >0.162378</th>\n",
       "      <th id=\"T_c769d_level0_col6\" class=\"col_heading level0 col6\" >0.112884</th>\n",
       "      <th id=\"T_c769d_level0_col7\" class=\"col_heading level0 col7\" >0.078476</th>\n",
       "      <th id=\"T_c769d_level0_col8\" class=\"col_heading level0 col8\" >0.054556</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >seed</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "      <th class=\"blank col8\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c769d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_c769d_row0_col0\" class=\"data row0 col0\" >9</td>\n",
       "      <td id=\"T_c769d_row0_col1\" class=\"data row0 col1\" >17</td>\n",
       "      <td id=\"T_c769d_row0_col2\" class=\"data row0 col2\" >25</td>\n",
       "      <td id=\"T_c769d_row0_col3\" class=\"data row0 col3\" >49</td>\n",
       "      <td id=\"T_c769d_row0_col4\" class=\"data row0 col4\" >73</td>\n",
       "      <td id=\"T_c769d_row0_col5\" class=\"data row0 col5\" >164</td>\n",
       "      <td id=\"T_c769d_row0_col6\" class=\"data row0 col6\" >326</td>\n",
       "      <td id=\"T_c769d_row0_col7\" class=\"data row0 col7\" >480</td>\n",
       "      <td id=\"T_c769d_row0_col8\" class=\"data row0 col8\" >603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c769d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_c769d_row1_col0\" class=\"data row1 col0\" >9</td>\n",
       "      <td id=\"T_c769d_row1_col1\" class=\"data row1 col1\" >19</td>\n",
       "      <td id=\"T_c769d_row1_col2\" class=\"data row1 col2\" >26</td>\n",
       "      <td id=\"T_c769d_row1_col3\" class=\"data row1 col3\" >46</td>\n",
       "      <td id=\"T_c769d_row1_col4\" class=\"data row1 col4\" >80</td>\n",
       "      <td id=\"T_c769d_row1_col5\" class=\"data row1 col5\" >174</td>\n",
       "      <td id=\"T_c769d_row1_col6\" class=\"data row1 col6\" >322</td>\n",
       "      <td id=\"T_c769d_row1_col7\" class=\"data row1 col7\" >483</td>\n",
       "      <td id=\"T_c769d_row1_col8\" class=\"data row1 col8\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c769d_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_c769d_row2_col0\" class=\"data row2 col0\" >10</td>\n",
       "      <td id=\"T_c769d_row2_col1\" class=\"data row2 col1\" >18</td>\n",
       "      <td id=\"T_c769d_row2_col2\" class=\"data row2 col2\" >27</td>\n",
       "      <td id=\"T_c769d_row2_col3\" class=\"data row2 col3\" >45</td>\n",
       "      <td id=\"T_c769d_row2_col4\" class=\"data row2 col4\" >85</td>\n",
       "      <td id=\"T_c769d_row2_col5\" class=\"data row2 col5\" >171</td>\n",
       "      <td id=\"T_c769d_row2_col6\" class=\"data row2 col6\" >332</td>\n",
       "      <td id=\"T_c769d_row2_col7\" class=\"data row2 col7\" >476</td>\n",
       "      <td id=\"T_c769d_row2_col8\" class=\"data row2 col8\" >602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c769d_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_c769d_row3_col0\" class=\"data row3 col0\" >8</td>\n",
       "      <td id=\"T_c769d_row3_col1\" class=\"data row3 col1\" >18</td>\n",
       "      <td id=\"T_c769d_row3_col2\" class=\"data row3 col2\" >27</td>\n",
       "      <td id=\"T_c769d_row3_col3\" class=\"data row3 col3\" >44</td>\n",
       "      <td id=\"T_c769d_row3_col4\" class=\"data row3 col4\" >80</td>\n",
       "      <td id=\"T_c769d_row3_col5\" class=\"data row3 col5\" >167</td>\n",
       "      <td id=\"T_c769d_row3_col6\" class=\"data row3 col6\" >344</td>\n",
       "      <td id=\"T_c769d_row3_col7\" class=\"data row3 col7\" >472</td>\n",
       "      <td id=\"T_c769d_row3_col8\" class=\"data row3 col8\" >599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c769d_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_c769d_row4_col0\" class=\"data row4 col0\" >9</td>\n",
       "      <td id=\"T_c769d_row4_col1\" class=\"data row4 col1\" >17</td>\n",
       "      <td id=\"T_c769d_row4_col2\" class=\"data row4 col2\" >27</td>\n",
       "      <td id=\"T_c769d_row4_col3\" class=\"data row4 col3\" >47</td>\n",
       "      <td id=\"T_c769d_row4_col4\" class=\"data row4 col4\" >77</td>\n",
       "      <td id=\"T_c769d_row4_col5\" class=\"data row4 col5\" >164</td>\n",
       "      <td id=\"T_c769d_row4_col6\" class=\"data row4 col6\" >327</td>\n",
       "      <td id=\"T_c769d_row4_col7\" class=\"data row4 col7\" >485</td>\n",
       "      <td id=\"T_c769d_row4_col8\" class=\"data row4 col8\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c769d_level0_row5\" class=\"row_heading level0 row5\" >stable</th>\n",
       "      <td id=\"T_c769d_row5_col0\" class=\"data row5 col0\" >8</td>\n",
       "      <td id=\"T_c769d_row5_col1\" class=\"data row5 col1\" >14</td>\n",
       "      <td id=\"T_c769d_row5_col2\" class=\"data row5 col2\" >20</td>\n",
       "      <td id=\"T_c769d_row5_col3\" class=\"data row5 col3\" >31</td>\n",
       "      <td id=\"T_c769d_row5_col4\" class=\"data row5 col4\" >51</td>\n",
       "      <td id=\"T_c769d_row5_col5\" class=\"data row5 col5\" >120</td>\n",
       "      <td id=\"T_c769d_row5_col6\" class=\"data row5 col6\" >249</td>\n",
       "      <td id=\"T_c769d_row5_col7\" class=\"data row5 col7\" >406</td>\n",
       "      <td id=\"T_c769d_row5_col8\" class=\"data row5 col8\" >579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f75436bcfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# v3_boost_examples has 674, eng1000 has 985\n",
    "nonzeros.columns.name = 'alpha'\n",
    "nonzeros.index.name = 'seed'\n",
    "# nonzeros.columns = np.arange(len(nonzeros.columns))\n",
    "# enets.columns = nonzeros.columns\n",
    "# nonzeros.columns = nonzeros.columns.round(4)\n",
    "\n",
    "coefs_stable_dict = {}\n",
    "coefs_all_dict = {}\n",
    "# add row for fracs\n",
    "for col in nonzeros.columns:\n",
    "    coefs_list = enets[col]\n",
    "    coefs_list = coefs_list[coefs_list.notna()]\n",
    "\n",
    "    if len(coefs_list) > 0:\n",
    "        # get fraction of times each element is True\n",
    "        coefs_arr = np.vstack(coefs_list.values)\n",
    "        coefs_all = coefs_arr.max(axis=0)\n",
    "        coefs_arr = coefs_arr.min(axis=0)\n",
    "        nonzeros_stable = np.sum(coefs_arr)\n",
    "    else:\n",
    "        nonzeros_stable = np.nan\n",
    "    nonzeros.loc['stable', col] = nonzeros_stable\n",
    "    coefs_all_dict[col] = deepcopy(coefs_all)\n",
    "    coefs_stable_dict[col] = deepcopy(coefs_arr)\n",
    "\n",
    "display(\n",
    "    nonzeros\n",
    "    .style\n",
    "    .background_gradient(cmap='viridis', axis=None)\n",
    "    .format('{:.0f}')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = np.array(get_merged_questions_v3_boostexamples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 num questions: 8\n",
      "----STABLE---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Does the input include a philosophical or reflective thought?',\n",
       " 'Does the sentence contain a proper noun?',\n",
       " 'Does the sentence describe a personal or social interaction that leads to a change or revelation?',\n",
       " 'Does the sentence describe a physical action?',\n",
       " 'Does the sentence describe a relationship between people?',\n",
       " 'Does the sentence involve a description of physical environment or setting?',\n",
       " 'Does the sentence involve the mention of a specific object or item?',\n",
       " 'Does the sentence reference a specific location or place?']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----UNSTABLE---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Does the sentence describe a personal reflection or thought?',\n",
       " 'Does the sentence describe a visual experience or scene?',\n",
       " 'Does the sentence involve a social or interpersonal interaction?']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 num questions: 14\n",
      "----STABLE---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Does the input involve planning or organizing?',\n",
       " 'Does the sentence describe a visual experience or scene?',\n",
       " \"Does the sentence express the narrator's opinion or judgment about an event or character?\",\n",
       " 'Does the sentence include dialogue?',\n",
       " 'Is the input related to a specific industry or profession?',\n",
       " 'Is time mentioned in the input?']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----UNSTABLE---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Does the input describe a sensory experience?',\n",
       " 'Does the sentence contain a cultural reference?',\n",
       " 'Does the sentence describe a personal reflection or thought?',\n",
       " 'Does the sentence describe a sensory experience?',\n",
       " 'Does the sentence include numerical information?',\n",
       " 'Does the sentence involve a description of an interpersonal misunderstanding or dispute?',\n",
       " 'Does the sentence involve spatial reasoning?']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 num questions: 20\n",
      "----STABLE---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Does the sentence contain a cultural reference?',\n",
       " 'Does the sentence describe a personal reflection or thought?',\n",
       " 'Does the sentence describe a sensory experience?',\n",
       " 'Does the sentence include numerical information?',\n",
       " 'Does the sentence include technical or specialized terminology?',\n",
       " 'Does the sentence involve spatial reasoning?']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----UNSTABLE---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Does the input describe a sensory experience?',\n",
       " 'Does the input describe a specific texture or sensation?',\n",
       " 'Does the sentence contain a negation?',\n",
       " 'Does the sentence describe a physical sensation?',\n",
       " 'Does the sentence describe a specific sensation or feeling?',\n",
       " 'Does the sentence include a direct speech quotation?',\n",
       " 'Does the sentence include a personal anecdote or story?',\n",
       " 'Does the sentence include dialogue or thoughts directed towards another character?',\n",
       " 'Does the sentence involve a description of an interpersonal misunderstanding or dispute?',\n",
       " 'Does the sentence involve a discussion about personal or social values?',\n",
       " 'Does the sentence involve an expression of personal values or beliefs?',\n",
       " 'Does the sentence mention a specific location or place?',\n",
       " 'Is the sentence abstract rather than concrete?']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 num questions: 31\n",
      "----STABLE---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Does the input describe a specific texture or sensation?',\n",
       " 'Does the sentence contain a negation?',\n",
       " 'Does the sentence describe a change in a physical or emotional state?',\n",
       " 'Does the sentence describe a physical sensation?',\n",
       " 'Does the sentence include a number or statistic?',\n",
       " 'Does the sentence include a personal anecdote or story?',\n",
       " 'Does the sentence include an account of a miscommunication or misunderstanding?',\n",
       " 'Does the sentence involve an expression of personal values or beliefs?',\n",
       " 'Does the sentence mention a specific location or place?',\n",
       " 'Does the text describe a journey?',\n",
       " 'Is the input about a discovery or realization?']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----UNSTABLE---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Does the input describe a problem or challenge?',\n",
       " 'Does the input describe a sensory experience?',\n",
       " 'Does the input discuss a societal issue or social justice topic?',\n",
       " 'Does the input include a comparison or metaphor?',\n",
       " 'Does the sentence convey a sense of urgency or haste?',\n",
       " 'Does the sentence describe a moment of relief or resolution of tension?',\n",
       " 'Does the sentence describe a specific sensation or feeling?',\n",
       " 'Does the sentence describe an activity related to daily life or routine?',\n",
       " 'Does the sentence express a sense of belonging or connection to a place or community?',\n",
       " 'Does the sentence include a direct speech quotation?',\n",
       " 'Does the sentence include a recounting of an impactful or emotional dialogue?',\n",
       " 'Does the sentence include a specific sound or auditory description?',\n",
       " 'Does the sentence include dialogue or thoughts directed towards another character?',\n",
       " 'Does the sentence involve a decision-making process?',\n",
       " 'Does the sentence involve a description of an interpersonal misunderstanding or dispute?',\n",
       " 'Does the sentence involve a discussion about personal or social values?',\n",
       " 'Does the sentence involve a recounting of a memorable event or anecdote?',\n",
       " 'Does the sentence involve a safety or security concern?',\n",
       " 'Does the sentence involve an unexpected incident or accident?',\n",
       " 'Does the sentence reference a specific time or date?',\n",
       " 'Does the sentence use a unique or unusual word?',\n",
       " 'Does the sentence use past tense?',\n",
       " 'Does the text describe a mode of communication?',\n",
       " 'Does the text include a planning or decision-making process?',\n",
       " 'Is the sentence abstract rather than concrete?',\n",
       " \"Is the sentence conveying the narrator's physical movement or action in detail?\",\n",
       " 'Is the sentence emotionally positive?',\n",
       " 'Is the sentence expressing skepticism or disbelief towards something or someone?',\n",
       " 'Is the sentence part of a narrative?',\n",
       " 'Is the sentence providing an explanation or rationale?',\n",
       " 'Is there a discussion of a social issue?',\n",
       " 'Is there a mention of a scientific fact or concept?',\n",
       " 'Is there mention of a city, country, or geographic feature?']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 num questions: 51\n",
      "----STABLE---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Does the input discuss a societal issue or social justice topic?',\n",
       " 'Does the sentence convey a decision or choice made by the narrator?',\n",
       " 'Does the sentence convey a sense of urgency or haste?',\n",
       " \"Does the sentence convey the narrator's sense of humor or wit?\",\n",
       " 'Does the sentence describe a moment of relief or resolution of tension?',\n",
       " 'Does the sentence describe a specific sensation or feeling?',\n",
       " 'Does the sentence express a sense of belonging or connection to a place or community?',\n",
       " 'Does the sentence include a direct speech quotation?',\n",
       " 'Does the sentence include a specific sound or auditory description?',\n",
       " 'Does the sentence involve a discussion about personal or social values?',\n",
       " 'Does the sentence involve a safety or security concern?',\n",
       " 'Does the sentence involve an unexpected incident or accident?',\n",
       " 'Does the sentence reference a specific time or date?',\n",
       " 'Does the sentence use a unique or unusual word?',\n",
       " 'Does the text describe a mode of communication?',\n",
       " \"Is the sentence conveying the narrator's physical movement or action in detail?\",\n",
       " 'Is the sentence emotionally positive?',\n",
       " 'Is the sentence expressing skepticism or disbelief towards something or someone?',\n",
       " 'Is the sentence providing an explanation or rationale?',\n",
       " 'Is there mention of a city, country, or geographic feature?']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----UNSTABLE---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Does the input contain a measurement?',\n",
       " 'Does the input describe a moment of realization or epiphany?',\n",
       " 'Does the input describe a sensory experience?',\n",
       " 'Does the input discuss a health-related issue or wellness?',\n",
       " 'Does the input include a comparison or metaphor?',\n",
       " 'Does the input include a description of clothing?',\n",
       " 'Does the sentence describe a character in a story?',\n",
       " 'Does the sentence describe a health-related topic or concern?',\n",
       " 'Does the sentence describe a physical sensation (e.g., touch, taste)?',\n",
       " 'Does the sentence describe a routine activity?',\n",
       " 'Does the sentence describe an activity related to daily life or routine?',\n",
       " 'Does the sentence describe an experience of learning or gaining new knowledge?',\n",
       " 'Does the sentence describe an indoor setting?',\n",
       " 'Does the sentence express anger or frustration directed at a specific situation or person?',\n",
       " 'Does the sentence include a conditional clause?',\n",
       " 'Does the sentence include a recounting of an impactful or emotional dialogue?',\n",
       " 'Does the sentence include a threat or warning?',\n",
       " 'Does the sentence include dialogue or thoughts directed towards another character?',\n",
       " 'Does the sentence involve a decision-making process?',\n",
       " 'Does the sentence involve a description of a group activity or social gathering?',\n",
       " 'Does the sentence involve a description of a journey or travel?',\n",
       " 'Does the sentence involve a description of a personal crisis or urgent situation?',\n",
       " 'Does the sentence involve a description of an interpersonal misunderstanding or dispute?',\n",
       " 'Does the sentence involve a personal anecdote about family or friends?',\n",
       " 'Does the sentence involve a recount of a social or community event?',\n",
       " 'Does the sentence involve a recounting of a memorable event or anecdote?',\n",
       " 'Does the sentence mention a past or historical event?',\n",
       " 'Does the sentence relate to personal health or bodily functions?',\n",
       " 'Does the sentence use past tense?',\n",
       " 'Does the story include a lesson or moral?',\n",
       " 'Does the story involve a close call or narrow escape?',\n",
       " 'Does the story involve a personal project or creation?',\n",
       " 'Does the story mention a craft or DIY project?',\n",
       " 'Does the story mention a health issue or injury?',\n",
       " 'Does the text describe an act of kindness?',\n",
       " 'Does the text include a planning or decision-making process?',\n",
       " 'Does the text include a reference to a past era or time period?',\n",
       " 'Does the text mention a piece of advice?',\n",
       " 'Is a personal goal or objective mentioned?',\n",
       " 'Is a specific color or pattern described?',\n",
       " 'Is a specific tool or equipment mentioned?',\n",
       " 'Is the input expressing a wish or desire?',\n",
       " 'Is the sentence a command?',\n",
       " 'Is the sentence abstract rather than concrete?',\n",
       " 'Is the sentence conveying a strategic or tactical thought by the narrator?',\n",
       " 'Is the sentence describing a moment of realization or epiphany?',\n",
       " 'Is the sentence designed to persuade or convince?',\n",
       " 'Is the sentence expressing admiration or praise?',\n",
       " 'Is the sentence in the passive voice?',\n",
       " 'Is the sentence intended to be motivational or inspirational?',\n",
       " 'Is the sentence part of a narrative?',\n",
       " 'Is the sentence reflective, involving self-analysis or introspection?',\n",
       " 'Is the sentence structured as a list?',\n",
       " 'Is there a description of a landscape or natural scene?',\n",
       " 'Is there a discussion of a social issue?',\n",
       " 'Is there a mention of a scientific fact or concept?']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "questions = np.array(get_questions('v3_boostexamples', full=True))\n",
    "# questions = np.array(get_merged_questions_v3_boostexamples())\n",
    "# print(len(questions))\n",
    "qs_prev = []\n",
    "for i in range(5):\n",
    "    print(i, 'num questions:', len(\n",
    "        questions[list(coefs_stable_dict.values())[i]]))\n",
    "\n",
    "    print('----STABLE---')\n",
    "    qs = sorted(questions[list(coefs_stable_dict.values())[i]].tolist())\n",
    "    qs_unstable = sorted(questions[list(coefs_all_dict.values())[i]].tolist())\n",
    "    display([q for q in qs if q not in qs_prev])\n",
    "    for q in qs_prev:\n",
    "        if not q in qs:\n",
    "            print('[DROPPED]', q)\n",
    "\n",
    "    print('----UNSTABLE---')\n",
    "    display([q for q in qs_unstable if not q in qs])\n",
    "\n",
    "    qs_prev = qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually clean up questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = np.array(get_questions('v3_boostexamples', full=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(questions)\n",
    "df.index.name = 'question_num'\n",
    "df.rename(columns={0: 'question'}, inplace=True)\n",
    "df['stable_idx'] = 10\n",
    "coefs_stable_vals = list(coefs_stable_dict.values())\n",
    "for i in range(len(coefs_stable_vals) - 1, -1, -1):\n",
    "    df.loc[coefs_stable_vals[i], 'stable_idx'] = i\n",
    "df = df.sort_values(by='stable_idx')\n",
    "\n",
    "# display full questions no truncation\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.max_colwidth', None):\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ''\n",
    "for i in range(len(df)):\n",
    "    s += f'{i}. {questions[i]}\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Below is a numbered list of questions. Group and list any questions (along with their numbers) that are very similar to each other'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select questions to run with gpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df[df.stable_idx <= 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop anything that was merged\n",
    "from neuro.features.questions.merge_v3_boostexamples import DICT_MERGE_V3_BOOSTEXAMPLES\n",
    "questions_to_drop = [k for k in sum(DICT_MERGE_V3_BOOSTEXAMPLES.values(), [\n",
    "]) if not k in DICT_MERGE_V3_BOOSTEXAMPLES]\n",
    "d = d[~d.question.isin(questions_to_drop)]\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.question.values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.embgam')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "559535f78d940c882783b39501b2581b5193373045707e5f8a51d046029cfd38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
