{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from matplotlib import pyplot as plt\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import imodelsx.process_results\n",
    "import qa_questions\n",
    "import random\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import feature_spaces\n",
    "fit_encoding = __import__('01_fit_encoding')\n",
    "import encoding_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A:\n",
    "    use_test_setup = False\n",
    "    subject = 'UTS03'\n",
    "\n",
    "\n",
    "args = A()\n",
    "story_names_train, story_names_test = fit_encoding.get_story_names(args)\n",
    "wordseqs = feature_spaces.get_story_wordseqs(story_names_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate examples for questions v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 43  # 42, 43\n",
    "ngrams_examples = []\n",
    "ngram_size = 10\n",
    "num_examples_per_story = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "for story_name in story_names_train:\n",
    "    words_list = wordseqs[story_name].data\n",
    "    ngrams_list = feature_spaces._get_ngrams_list_from_words_list(\n",
    "        words_list, ngram_size=ngram_size)[ngram_size + 2:]\n",
    "    ngrams_examples += np.random.choice(ngrams_list,\n",
    "                                        num_examples_per_story).tolist()\n",
    "print('\\n'.join(['- ' + ngram for ngram in ngrams_examples]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate examples for boosted questions v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load top model to boost\n",
    "results_dir = '/home/chansingh/mntv1/deep-fMRI/encoding/results_apr1'\n",
    "r = imodelsx.process_results.get_results_df(results_dir)\n",
    "for k in ['save_dir', 'save_dir_unique']:\n",
    "    r[k] = r[k].map(lambda x: x if x.startswith('/home')\n",
    "                    else x.replace('/mntv1', '/home/chansingh/mntv1'))\n",
    "\n",
    "args_top = r[\n",
    "    (r.feature_space.str.contains('qa_embedder')) *\n",
    "    (r.pc_components == 100) *\n",
    "    (r.ndelays == 4)\n",
    "].sort_values(\n",
    "    by='corrs_tune_pc_mean',\n",
    "    ascending=False).iloc[0]\n",
    "print(f'{args_top.feature_space=} {args_top.ndelays=}')\n",
    "print(f'{args_top.corrs_test_mean=:.3f} {args_top.corrs_tune_pc_mean=:3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params_to_save = joblib.load(\n",
    "    join(args_top.save_dir_unique, 'model_params.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = defaultdict(list)\n",
    "for story_name in tqdm(story_names_train):\n",
    "    # ngram for 2 trs preceding the current TR\n",
    "    chunks = wordseqs[story_name].chunks()\n",
    "    ngrams_list = feature_spaces._get_ngrams_list_from_chunks(\n",
    "        chunks, num_trs=2)\n",
    "    ngrams_list = np.array(ngrams_list[10:-5])\n",
    "\n",
    "    stim_train_delayed, resp_train = fit_encoding.get_data(\n",
    "        args_top, [story_name])\n",
    "\n",
    "    preds_test = stim_train_delayed @ model_params_to_save['weights'] + \\\n",
    "        model_params_to_save['bias']\n",
    "\n",
    "    # calculate correlation at each timepoint\n",
    "    corrs_time = np.array([np.corrcoef(resp_train[i, :], preds_test[i, :])[0, 1]\n",
    "                           for i in range(resp_train.shape[0])])\n",
    "    corrs_time[:10] = 100  # don't pick first 10 TRs\n",
    "    # get worst 3 idxs\n",
    "    corrs_worst_idxs = np.argsort(corrs_time)[:3]\n",
    "\n",
    "    for i in range(3):\n",
    "        r['story_name'].append(story_name)\n",
    "        r['corrs'].append(corrs_time[corrs_worst_idxs[i]])\n",
    "        r['ngram'].append(ngrams_list[corrs_worst_idxs[i]])\n",
    "        r['tr'].append(corrs_worst_idxs[i])\n",
    "\n",
    "    joblib.dump(r, 'ngrams_worst.pkl')  # saved as 04_ngrams_boost.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_boost = pd.DataFrame(joblib.load('04_ngrams_boost.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- on top of that my my wife got pregnant at the time and i realized as that\n",
      "- off guard but when he got two feet away\n",
      "- pounds of baby that i'm analyzing her skull and she goes\n",
      "- therapy he's had shock treatments he's had every combination\n",
      "- feeling creepy for staring at these strangers and also envious\n",
      "- by he was also a bipolar and if\n",
      "- to play and every so often i turn around to look at mike course\n",
      "- unsettled i sensed i was now exactly where i should be\n",
      "- and we would he taught me how to chop wood and he bought me um\n",
      "- me to the town's ramshackle internet cafe so i could check my e-mail\n",
      "- we were happy we kept all the time in touch with the family\n",
      "- just lost his wife\n",
      "- straight at me and almost\n",
      "- and the snacks that doesn't even start with the clothing so we overpacked\n",
      "- over blueprints and the building\n",
      "- in the morning and the streets of delhi were quiet i'd never seen them quiet before they're normally\n",
      "- education director and i asked him which class he thought i should take and he\n",
      "- god's gift to the planet you know or\n",
      "- trail and he says well there's a water fountain right there and\n",
      "- resist we called him puff like puff the magic dragon he\n",
      "- to the car park and i ask to be let down they offer\n",
      "- about how really happy adoption is and we are\n",
      "- and i saw that it was only a few miles\n",
      "- not feel like somebody who had suffered liked someone\n",
      "- of church that was trying to be really hip and modern it met in a strip mall and\n",
      "- that water was starting to well up in her eyes and i said\n",
      "- chloe kept making remarks about how only a few of our friends have been killed by customers\n",
      "- graduated on to a two room school house in the center of chilmark\n",
      "- an evangelical christian i grew up as far away from that as possible my parents\n",
      "- uh to this day you know i still think that he died on purpose\n",
      "- through it and i'm just like this is so cool i i say\n",
      "- it's like super cool and um i'm really excited so\n",
      "- thing and she came out with her very prized possession which was a\n",
      "- intense feelings can make you feel small in their grasp or\n",
      "- so we feel good about the joke but but we still need a birthday card so uh one\n",
      "- ones that make you lose sleep and\n",
      "- and you know and i\n",
      "- the weirdest thing is that his friends all went naked\n",
      "- i shut the door i call out to my wife i said hey babe\n",
      "- my clothes my books my bookcases\n",
      "- me twice and i never replied and then he called and\n",
      "- oh one and we're waiting outside the room\n",
      "- your space and your family\n",
      "- rarely up to me and i go new places and i meet new people\n",
      "- he was reading this book because he needed to understand\n",
      "- ride back he's like fucking like you know that dude from leave it to beaver\n",
      "- numbers so you can imagine my joy when i heard\n",
      "- there bitch with no one writing about you in the new yorker\n",
      "- seconds and i just fell to the side somebody took my place\n",
      "- please don't send any more please don't send any more because my\n",
      "- and my classmate right in front of me is stephanie wilson\n",
      "- now we have this moment where we're laughing together at this image of my mom like digging through the\n",
      "- waiting for me from my adult female open minded\n",
      "- in a matter of days how could i say no to him\n",
      "- german concentration camp and then\n",
      "- we should be very grateful that ancient catholic\n",
      "- i went home that night and i tore a page out of the program that listed all the different\n",
      "- i got to do at the white house uh some of the proudest moments of my life\n",
      "- so i had no option but to find a job\n",
      "- our gear into a small bering airplane\n",
      "- about keeping promises and about self sacrifice\n",
      "- he's done his cop training he's like we have evidence\n",
      "- in distress style caught by surprise\n",
      "- me to be there that she in fact needed me there for support my brothers too\n",
      "- i laughed\n",
      "- with his friends we would just find some little nook or a corner or like go in the backyard\n",
      "- thinking this conversation about being\n",
      "- i think it's very difficult to really live any day as\n",
      "- it was all of a sudden i started feeling things i started feeling hot\n",
      "- justified or even worse satisfyingly resolved\n",
      "- where we had something called hot seating which meant that forty eight of us came\n",
      "- seems like seconds four or five jeeps full of mps\n",
      "- fifth grade was the last year i can remember believing that the world needed\n",
      "- outside were very impatient uh waiting for\n",
      "- i do the camp thing i do what's at camp and i don't know if you know this but there are a lot\n",
      "- like out cold alright and the surgeon's like do gives him like\n",
      "- he was no help and um somehow though sam and i survived our\n",
      "- now probably making sure everyone she shares the afterlife\n",
      "- eddie heka told you about and\n",
      "- well where i was from but i felt that somehow or other this\n",
      "- chicken fights but it's kinda weird because there's only three of us so\n",
      "- rid of the fear one problem is i don't actually\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(['- ' + x for x in ngrams_boost.iloc[1::3]\n",
    "      ['ngram'].values if len(x.strip()) > 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.embgam')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "559535f78d940c882783b39501b2581b5193373045707e5f8a51d046029cfd38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
