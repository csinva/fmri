{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from matplotlib import pyplot as plt\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import imodelsx.process_results\n",
    "import qa_questions\n",
    "import random\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import feature_spaces\n",
    "fit_encoding = __import__('01_fit_encoding')\n",
    "import encoding_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A:\n",
    "    use_test_setup = False\n",
    "    subject = 'UTS03'\n",
    "\n",
    "\n",
    "args = A()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_names_train, story_names_test = fit_encoding.get_story_names(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordseqs = feature_spaces.get_story_wordseqs(story_names_train)\n",
    "# chunks = wordseqs['theclosetthatateeverything'].chunks()\n",
    "# chunk_lens = [len(chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 43  # 42\n",
    "ngrams_examples = []\n",
    "ngram_size = 10\n",
    "num_examples_per_story = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "for story_name in story_names_train:\n",
    "    words_list = wordseqs[story_name].data\n",
    "    ngrams_list = feature_spaces._get_ngrams_list_from_words_list(\n",
    "        words_list, ngram_size=ngram_size)[ngram_size + 2:]\n",
    "    ngrams_examples += np.random.choice(ngrams_list,\n",
    "                                        num_examples_per_story).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(['- ' + ngram for ngram in ngrams_examples]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordseqs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordseqs = feature_spaces.get_story_wordseqs(['sloth'])\n",
    "# chunks = wordseqs['theclosetthatateeverything'].chunks()\n",
    "# chunk_lens = [len(chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = wordseqs['sloth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ws.data_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws.data_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def _get_ngrams_list_from_words_list_and_times(words_list: List[str], times_list: np.ndarray[float], sec_offset: float = 4) -> List[str]:\n",
    "\n",
    "    words_arr = np.array(words_list)\n",
    "    ngrams_list = []\n",
    "    for i in range(len(times_list)):\n",
    "        t = times_list[i]\n",
    "        t_off = t - sec_offset\n",
    "        idxs = np.where(np.logical_and(\n",
    "            times_list >= t_off, times_list <= t))[0]\n",
    "        ngrams_list.append(' '.join(words_arr[idxs]))\n",
    "    return ngrams_list\n",
    "\n",
    "\n",
    "ngrams_list = _get_ngrams_list_from_words_list_and_times(\n",
    "    ws.data, ws.data_times, sec_offset=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -9.,  -7.,  -5.,  -3.,  -1.,   1.,   3.,   5.,   7.,   9.,  11.,\n",
       "        13.,  15.,  17.,  19.,  21.,  23.,  25.,  27.,  29.,  31.,  33.,\n",
       "        35.,  37.,  39.,  41.,  43.,  45.,  47.,  49.,  51.,  53.,  55.,\n",
       "        57.,  59.,  61.,  63.,  65.,  67.,  69.,  71.,  73.,  75.,  77.,\n",
       "        79.,  81.,  83.,  85.,  87.,  89.,  91.,  93.,  95.,  97.,  99.,\n",
       "       101., 103., 105., 107., 109., 111., 113., 115., 117., 119., 121.,\n",
       "       123., 125., 127., 129., 131., 133., 135., 137., 139., 141., 143.,\n",
       "       145., 147., 149., 151., 153., 155., 157., 159., 161., 163., 165.,\n",
       "       167., 169., 171., 173., 175., 177., 179., 181., 183., 185., 187.,\n",
       "       189., 191., 193., 195., 197., 199., 201., 203., 205., 207., 209.,\n",
       "       211., 213., 215., 217., 219., 221., 223., 225., 227., 229., 231.,\n",
       "       233., 235., 237., 239., 241., 243., 245., 247., 249., 251., 253.,\n",
       "       255., 257., 259., 261., 263., 265., 267., 269., 271., 273., 275.,\n",
       "       277., 279., 281., 283., 285., 287., 289., 291., 293., 295., 297.,\n",
       "       299., 301., 303., 305., 307., 309., 311., 313., 315., 317., 319.,\n",
       "       321., 323., 325., 327., 329., 331., 333., 335., 337., 339., 341.,\n",
       "       343., 345., 347., 349., 351., 353., 355., 357., 359., 361., 363.,\n",
       "       365., 367., 369., 371., 373., 375., 377., 379., 381., 383., 385.,\n",
       "       387., 389., 391., 393., 395., 397., 399., 401., 403., 405., 407.,\n",
       "       409., 411., 413., 415., 417., 419., 421., 423., 425., 427., 429.,\n",
       "       431., 433., 435., 437., 439., 441., 443., 445., 447., 449., 451.,\n",
       "       453., 455., 457., 459., 461., 463., 465., 467., 469., 471., 473.,\n",
       "       475., 477., 479., 481., 483., 485., 487., 489., 491., 493., 495.,\n",
       "       497., 499., 501., 503., 505., 507., 509., 511., 513., 515., 517.,\n",
       "       519., 521., 523., 525., 527., 529., 531., 533., 535., 537., 539.,\n",
       "       541., 543., 545., 547., 549., 551., 553., 555., 557., 559., 561.,\n",
       "       563., 565., 567., 569., 571., 573., 575., 577., 579., 581., 583.,\n",
       "       585., 587., 589., 591., 593., 595., 597., 599., 601., 603., 605.,\n",
       "       607., 609., 611., 613., 615., 617., 619., 621., 623., 625., 627.,\n",
       "       629., 631., 633., 635., 637., 639., 641., 643., 645., 647., 649.,\n",
       "       651., 653., 655., 657., 659., 661., 663., 665., 667., 669., 671.,\n",
       "       673., 675., 677., 679., 681., 683., 685., 687., 689., 691., 693.,\n",
       "       695., 697., 699., 701., 703., 705., 707., 709., 711., 713., 715.,\n",
       "       717., 719., 721., 723., 725., 727., 729., 731., 733., 735., 737.,\n",
       "       739., 741., 743., 745., 747., 749., 751., 753., 755., 757., 759.,\n",
       "       761., 763., 765., 767., 769., 771., 773., 775., 777., 779., 781.,\n",
       "       783., 785., 787., 789., 791., 793., 795., 797., 799., 801., 803.,\n",
       "       805., 807., 809., 811., 813., 815., 817., 819., 821., 823., 825.,\n",
       "       827., 829., 831., 833., 835., 837., 839., 841., 843., 845., 847.,\n",
       "       849., 851., 853., 855., 857., 859., 861., 863., 865., 867., 869.,\n",
       "       871., 873., 875., 877., 879., 881., 883., 885., 887., 889., 891.,\n",
       "       893.])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws.tr_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "    \"\"\"Concatenate running list of words into grams with spaces in between\n",
    "    \"\"\"\n",
    "    ngrams_list = []\n",
    "    for i in range(len(words_list)):\n",
    "        l = max(0, i - ngram_size)\n",
    "        ngram = ' '.join(words_list[l: i + 1])\n",
    "        ngrams_list.append(ngram.strip())\n",
    "    return ngrams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ws.chunks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_spaces._get_ngrams_list_from_chunks(ws.chunks()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = encoding_utils.get_response(['sloth'], args.subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = wo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wordseqs['wheretheressmoke'].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wordseqs['wheretheressmoke'].chunks()[10:-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordseqs['wheretheressmoke'].chunks()\n",
    "\n",
    "\n",
    "def get_ngrams_list_from_chunks(chunks, num_trs=2):\n",
    "    ngrams_list = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # print(chunks[i - num_trs:i])\n",
    "        # sum(chunks[i - num_trs:i], [])\n",
    "        chunk_block = chunks[i - num_trs:i]\n",
    "        if len(chunk_block) == 0:\n",
    "            ngrams_list.append('')\n",
    "        else:\n",
    "            chunk_block = np.concatenate(chunk_block)\n",
    "            ngrams_list.append(' '.join(chunk_block))\n",
    "\n",
    "    return ngrams_list\n",
    "\n",
    "\n",
    "ngrams_list = get_ngrams_list_from_chunks(\n",
    "    wordseqs['wheretheressmoke'].chunks(), num_trs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load fitted models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = '/home/chansingh/mntv1/deep-fMRI/encoding/results_mar30'\n",
    "experiment_filename = '../experiments/01_fit_encoding.py'\n",
    "\n",
    "# load the results in to a pandas dataframe\n",
    "r = imodelsx.process_results.get_results_df(results_dir)\n",
    "r = imodelsx.process_results.fill_missing_args_with_default(\n",
    "    r, experiment_filename)\n",
    "# imodelsx.process_results.delete_runs_in_dataframe(\n",
    "# r[r.feature_space == 'qa_embedder-10'], actually_delete=True)\n",
    "cols_varied = imodelsx.process_results.get_experiment_keys(\n",
    "    r, experiment_filename)\n",
    "print('experiment varied these params:', cols_varied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = r[(r.feature_space == 'qa_embedder-5') * (r.pc_components == 100) * (r.ndelays == 4)\n",
    "         ].sort_values(by='corrs_tune_mean', ascending=False).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['corrs_test_pc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_names_train, story_names_test = fit_encoding.get_story_names(args)\n",
    "stim_test_delayed, resp_test = fit_encoding.get_data(args, story_names_test)\n",
    "stim_train_delayed, resp_train = fit_encoding.get_data(args, story_names_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.embgam')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "559535f78d940c882783b39501b2581b5193373045707e5f8a51d046029cfd38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
