{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ridge_utils.DataSequence import DataSequence\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import dirname\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from neuro.features import qa_questions, feature_spaces\n",
    "from neuro.data import story_names, response_utils\n",
    "from neuro.features.stim_utils import load_story_wordseqs, load_story_wordseqs_huge\n",
    "import neuro.config\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "from os.path import join\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import decode\n",
    "data_dir = join(neuro.config.repo_dir, 'data', 'decoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptMapper(nn.Module):\n",
    "    def __init__(self, input_dim, prompt_length, hidden_size):\n",
    "        super(PromptMapper, self).__init__()\n",
    "        # A simple MLP that outputs a flattened prompt vector which is reshaped into (prompt_length, hidden_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_size * prompt_length),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size * prompt_length, hidden_size * prompt_length)\n",
    "        )\n",
    "        self.prompt_length = prompt_length\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape (batch_size, d)\n",
    "        out = self.fc(x)  # (batch_size, prompt_length * hidden_size)\n",
    "        out = out.view(-1, self.prompt_length, self.hidden_size)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PromptDataset(Dataset):\n",
    "    def __init__(self, vectors, sentences, tokenizer, max_length=50):\n",
    "        self.vectors = vectors\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vectors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vec = torch.tensor(self.vectors[idx])\n",
    "        sentence = self.sentences[idx]\n",
    "        # Tokenize the sentence. We use padding/truncation to a fixed length.\n",
    "        tokenized = self.tokenizer(sentence,\n",
    "                                   return_tensors=\"pt\",\n",
    "                                   padding=\"max_length\",\n",
    "                                   max_length=self.max_length,\n",
    "                                   truncation=True)\n",
    "        input_ids = tokenized.input_ids.squeeze(\n",
    "            0)        # Shape: (max_length,)\n",
    "        attention_mask = tokenized.attention_mask.squeeze(\n",
    "            0)  # Shape: (max_length,)\n",
    "        return vec.to('cuda'), input_ids.to('cuda'), attention_mask.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b97e174ceb424cd9b85eaa5447b6e5f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data\n",
    "n = 32\n",
    "data_by_subject = decode.load_data_by_subject(data_dir)\n",
    "data = data_by_subject['uts03']\n",
    "vectors = data['df_train'].values.astype(np.float32)\n",
    "sentences = data['texts_train'].values.tolist()\n",
    "vectors = vectors[:n]\n",
    "\n",
    "# pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B\", device_map=\"auto\",)\n",
    "model.eval()  # We'll freeze the model so we only update the mapping network\n",
    "\n",
    "# model params\n",
    "d = 800\n",
    "prompt_length = 6\n",
    "batch_size = 32\n",
    "hidden_size = model.config.hidden_size\n",
    "mapper = PromptMapper(d, prompt_length, hidden_size).to('cuda')\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "dataset = PromptDataset(vectors, sentences, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "optimizer = torch.optim.Adam(mapper.parameters(), lr=1e-4)\n",
    "\n",
    "# We'll compute the loss only for the target sentence tokens.\n",
    "# To do so, we create labels that are -100 for the soft prompt positions.\n",
    "num_epochs = 10\n",
    "max_length = max([len(s) for s in sentences])  # from our dataset\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for vecs, input_ids, attention_mask in tqdm(dataloader):\n",
    "        batch_size = input_ids.shape[0]\n",
    "\n",
    "        # Get the soft prompt embeddings from our mapper network.\n",
    "        # soft_prompt: (batch_size, prompt_length, hidden_size)\n",
    "        soft_prompt = mapper(vecs)\n",
    "\n",
    "        # Obtain the input token embeddings from the LLaMA model.\n",
    "        token_embeddings = model.get_input_embeddings()(\n",
    "            input_ids)  # (batch_size, max_length, hidden_size)\n",
    "\n",
    "        # Concatenate the soft prompt with the token embeddings.\n",
    "        # New input: [soft prompt tokens] + [tokenized sentence]\n",
    "        inputs_embeds = torch.cat(\n",
    "            [soft_prompt, token_embeddings], dim=1)  # .to('cuda')\n",
    "\n",
    "        # Adjust the attention mask: add ones for the soft prompt positions.\n",
    "        prompt_mask = torch.ones(\n",
    "            (batch_size, prompt_length), dtype=attention_mask.dtype, device=attention_mask.device)\n",
    "        extended_attention_mask = torch.cat(\n",
    "            [prompt_mask, attention_mask], dim=1)\n",
    "\n",
    "        # Create labels: we ignore the soft prompt positions (set them to -100) so that loss is only computed for the sentence tokens.\n",
    "        labels = torch.cat([\n",
    "            torch.full((batch_size, prompt_length), -100,\n",
    "                       dtype=input_ids.dtype, device=input_ids.device),\n",
    "            input_ids\n",
    "        ], dim=1)\n",
    "\n",
    "        # Forward pass through the model.\n",
    "        outputs = model(inputs_embeds=inputs_embeds,\n",
    "                        attention_mask=extended_attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print decoded sentence examples\n",
    "        if epoch % 5 == 0:\n",
    "            with torch.no_grad():\n",
    "                soft_prompt = mapper(vecs)\n",
    "                inputs_embeds = torch.cat(\n",
    "                    [soft_prompt, token_embeddings], dim=1)\n",
    "                outputs = model.generate(\n",
    "                    inputs_embeds=inputs_embeds,\n",
    "                    attention_mask=extended_attention_mask,\n",
    "                    max_length=max_length,\n",
    "                    min_length=4,\n",
    "                    do_sample=False)\n",
    "                decoded_sentences = tokenizer.batch_decode(outputs[:, prompt_length:],\n",
    "                                                           skip_special_tokens=True)\n",
    "                for i in range(min(3, len(decoded_sentences))):\n",
    "                    print(f\"GT sentence {i+1}: {sentences[i]}\")\n",
    "                    print(f\"Decoded sentence {i+1}: {decoded_sentences[i]}\")\n",
    "                    print()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # save the model\n",
    "    torch.save(mapper.state_dict(), f\"mapper_epoch_{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[128001],\n",
       "        [128001],\n",
       "        [128001],\n",
       "        [128001],\n",
       "        [128001],\n",
       "        [128001],\n",
       "        [128001],\n",
       "        [128001],\n",
       "        [128001],\n",
       "        [128001],\n",
       "        [128001],\n",
       "        [128001]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(\n",
    "    inputs_embeds=inputs_embeds,\n",
    "    attention_mask=extended_attention_mask,\n",
    "    max_length=max_length,\n",
    "    min_length=20,\n",
    "    do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5. Inference Example\n",
    "# -----------------------------\n",
    "# To use the learned mapping:\n",
    "# Given a new vector, map it to a continuous prompt and generate the corresponding sentence.\n",
    "mapper.eval()\n",
    "with torch.no_grad():\n",
    "    vecs_test = data['df_test'].values.astype(np.float32)\n",
    "    sentences_test = data['texts_train'].values.tolist()\n",
    "    new_vector = torch.tensor(vecs_test[0]).unsqueeze(\n",
    "        0).to('cuda')  # shape (1, d)\n",
    "\n",
    "    # new_vector = torch.tensor(np.random.randn(d).astype(\n",
    "    # np.float32)).unsqueeze(0)  # shape (1, d)\n",
    "    soft_prompt = mapper(new_vector)  # (1, prompt_length, hidden_size)\n",
    "\n",
    "    # Start with an empty input (or a beginning-of-sentence token, if desired).\n",
    "    # Here we assume the model will generate based on the soft prompt.\n",
    "    dummy_input = torch.tensor(\n",
    "        [[tokenizer.bos_token_id]], device=soft_prompt.device)\n",
    "    token_embeddings = model.get_input_embeddings()(dummy_input)\n",
    "\n",
    "    inputs_embeds = torch.cat([soft_prompt, token_embeddings], dim=1)\n",
    "\n",
    "    prompt_mask = torch.ones(\n",
    "        (1, prompt_length), dtype=torch.long, device=soft_prompt.device)\n",
    "    dummy_mask = torch.ones_like(dummy_input)\n",
    "    extended_attention_mask = torch.cat([prompt_mask, dummy_mask], dim=1)\n",
    "\n",
    "    # Generate output tokens (adjust generation parameters as needed)\n",
    "    generated_ids = model.generate(inputs_embeds=inputs_embeds,\n",
    "                                   attention_mask=extended_attention_mask,\n",
    "                                   max_length=max_length + prompt_length)\n",
    "\n",
    "    # Decode the generated tokens (skipping the soft prompt positions)\n",
    "    # Since our soft prompt is continuous, the generated text typically follows after the dummy token.\n",
    "    output_text = tokenizer.decode(\n",
    "        generated_ids[0][prompt_length:], skip_special_tokens=True)\n",
    "    print('Original text:', sentences_test[0])\n",
    "    print(\"Generated sentence:\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(generated_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
