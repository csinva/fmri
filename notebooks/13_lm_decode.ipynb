{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ridge_utils.DataSequence import DataSequence\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import dirname\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from neuro.features import qa_questions, feature_spaces\n",
    "from neuro.data import story_names, response_utils\n",
    "from neuro.features.stim_utils import load_story_wordseqs, load_story_wordseqs_huge\n",
    "import neuro.config\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "from os.path import join\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import decode\n",
    "data_dir = join(neuro.config.repo_dir, 'data', 'decoding')\n",
    "\n",
    "data_by_subject = decode.load_data_by_subject(data_dir)\n",
    "data = data_by_subject['uts03']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimize prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "class PromptTuningModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, model_name,\n",
    "            # prefix_text=\"Repeat this word:\",\n",
    "            # suffix_text=\"\\nRepeated word:\",\n",
    "            prefix_text='<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nDecode this message:',\n",
    "            suffix_text='<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nDecoded message:',\n",
    "            prompt_length=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Load tokenizer and model.\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, device_map=\"auto\")\n",
    "        self.model.eval()  # freeze model weights\n",
    "        # self.model.model.tokenizer.pad_token_id = self.tokenizer.pad_token_id\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Determine the embedding dimension from the model.\n",
    "        self.embedding_dim = self.model.model.embed_tokens.embedding_dim\n",
    "        self.prompt_length = prompt_length\n",
    "\n",
    "        # Create a learnable parameter for the continuous prompt (4 tokens).\n",
    "        self.prompt_embeddings = nn.Parameter(torch.randn(\n",
    "            prompt_length, self.embedding_dim)).to('cuda')\n",
    "\n",
    "        # Tokenize the fixed prefix text.\n",
    "        self.prefix_text = prefix_text\n",
    "        self.prefix_ids = self.tokenizer.encode(\n",
    "            prefix_text, return_tensors=\"pt\").squeeze(0).to('cuda')\n",
    "\n",
    "        # Define suffix text and tokenize it.\n",
    "        self.suffix_text = suffix_text\n",
    "        self.suffix_ids = self.tokenizer.encode(\n",
    "            self.suffix_text, return_tensors=\"pt\").squeeze(0).to('cuda')\n",
    "\n",
    "    def forward(self, target_sentence):\n",
    "        \"\"\"\n",
    "        For a given target sentence (string), build an input that is:\n",
    "            \"decode this message:\" tokens +\n",
    "            learned continuous prompt embeddings +\n",
    "            \"\\nDecoded message:\" tokens +\n",
    "            target sentence tokens.\n",
    "        The loss is computed only on the target sentence portion.\n",
    "        \"\"\"\n",
    "        # Tokenize the target sentence.\n",
    "        target_ids = self.tokenizer.encode(\n",
    "            target_sentence, return_tensors=\"pt\").squeeze(0)\n",
    "\n",
    "        # Get embeddings for the fixed prefix from the model's embedding layer.\n",
    "        prefix_embeds = self.model.model.embed_tokens(\n",
    "            self.prefix_ids.unsqueeze(0))  # shape: [1, prefix_len, emb_dim]\n",
    "\n",
    "        # Get embeddings for the suffix.\n",
    "        suffix_embeds = self.model.model.embed_tokens(\n",
    "            self.suffix_ids.unsqueeze(0))  # shape: [1, suffix_len, emb_dim]\n",
    "\n",
    "        # Get embeddings for the target sentence tokens.\n",
    "        target_embeds = self.model.model.embed_tokens(\n",
    "            target_ids.unsqueeze(0))  # shape: [1, target_len, emb_dim]\n",
    "\n",
    "        # Expand learned prompt embeddings to batch size.\n",
    "        learned_prompt = self.prompt_embeddings.unsqueeze(\n",
    "            0)  # shape: [1, prompt_length, emb_dim]\n",
    "\n",
    "        # Concatenate: [prefix embeddings] + [learned prompt] + [suffix embeddings] + [target embeddings]\n",
    "        inputs_embeds = torch.cat(\n",
    "            [prefix_embeds, learned_prompt, suffix_embeds, target_embeds], dim=1)\n",
    "\n",
    "        # Build attention mask (all ones).\n",
    "        attention_mask = torch.ones(inputs_embeds.shape[:-1], dtype=torch.long)\n",
    "\n",
    "        # Create labels so that only the target sentence tokens contribute to the loss.\n",
    "        prefix_len = prefix_embeds.shape[1]\n",
    "        prompt_len = learned_prompt.shape[1]\n",
    "        suffix_len = suffix_embeds.shape[1]\n",
    "        total_len = prefix_len + prompt_len + \\\n",
    "            suffix_len + target_embeds.shape[1]\n",
    "        labels = torch.full((1, total_len), -100, dtype=torch.long)\n",
    "        labels[0, prefix_len + prompt_len + suffix_len:] = target_ids\n",
    "\n",
    "        # Forward pass through the model using inputs_embeds.\n",
    "        outputs = self.model(inputs_embeds=inputs_embeds,\n",
    "                             attention_mask=attention_mask,\n",
    "                             labels=labels)\n",
    "        return outputs.loss, outputs.logits\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "\n",
    "# adjust to your model checkpoint as needed\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # updated model checkpoint\n",
    "target_sentence = \" hello<|eot_id|>\"  # the sentence to be decoded by the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize our prompt tuning model.\n",
    "pt_model = PromptTuningModel(model_name)  # .to(device)\n",
    "pt_model.prompt_embeddings = nn.Parameter(\n",
    "    pt_model.prompt_embeddings.detach().clone())\n",
    "\n",
    "# Set up an optimizer to update only the prompt embeddings.\n",
    "optimizer = torch.optim.Adam([pt_model.prompt_embeddings], lr=1e-4)\n",
    "\n",
    "# Training loop: optimize the continuous prompt so that the modelâ€™s output\n",
    "# (when given \"decode this message:\" + learned prompt) reproduces the target sentence.\n",
    "num_steps = 1000\n",
    "for step in range(num_steps):\n",
    "    optimizer.zero_grad()\n",
    "    loss, _ = pt_model(target_sentence)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # example generation\n",
    "        with torch.no_grad():\n",
    "            # Build input embeddings for generation: prefix + learned prompt.\n",
    "            prefix_ids = pt_model.prefix_ids.to(device)\n",
    "            prefix_embeds = pt_model.model.model.embed_tokens(\n",
    "                prefix_ids.unsqueeze(0))\n",
    "            learned_prompt = pt_model.prompt_embeddings.unsqueeze(0)\n",
    "            inputs_embeds = torch.cat([prefix_embeds, learned_prompt], dim=1)\n",
    "            attention_mask = torch.ones(\n",
    "                inputs_embeds.shape[:-1], dtype=torch.long).to(device)\n",
    "\n",
    "            # Generate text (note: adjust generation parameters as needed).\n",
    "            generated_ids = pt_model.model.generate(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=5\n",
    "            )\n",
    "            generated_text = pt_model.tokenizer.decode(\n",
    "                generated_ids[0], skip_special_tokens=True)\n",
    "            print(repr(generated_text))\n",
    "\n",
    "# After training, test the learned prompt via generation.\n",
    "with torch.no_grad():\n",
    "    # Build input embeddings for generation: prefix + learned prompt.\n",
    "    prefix_ids = pt_model.prefix_ids.to(device)\n",
    "    prefix_embeds = pt_model.model.model.embed_tokens(prefix_ids.unsqueeze(0))\n",
    "    learned_prompt = pt_model.prompt_embeddings.unsqueeze(0)\n",
    "    inputs_embeds = torch.cat([prefix_embeds, learned_prompt], dim=1)\n",
    "    attention_mask = torch.ones(\n",
    "        inputs_embeds.shape[:-1], dtype=torch.long).to(device)\n",
    "\n",
    "    # Generate text (note: adjust generation parameters as needed).\n",
    "    generated_ids = pt_model.model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=10\n",
    "    )\n",
    "    generated_text = pt_model.tokenizer.decode(\n",
    "        generated_ids[0], skip_special_tokens=True)\n",
    "    print(repr(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    # {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Decode this message:\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Decoded message:\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "tokenizer.batch_decode(input_ids)\n",
    "\n",
    "\n",
    "prefix = '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nDecode this message:'\n",
    "suffix = '<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nDecoded message:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E2E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptMapper(nn.Module):\n",
    "    def __init__(self, input_dim, prompt_length, hidden_size):\n",
    "        super(PromptMapper, self).__init__()\n",
    "        # A simple MLP that outputs a flattened prompt vector which is reshaped into (prompt_length, hidden_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_size * prompt_length),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size * prompt_length, hidden_size * prompt_length)\n",
    "        )\n",
    "        self.prompt_length = prompt_length\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape (batch_size, d)\n",
    "        out = self.fc(x)  # (batch_size, prompt_length * hidden_size)\n",
    "        out = out.view(-1, self.prompt_length, self.hidden_size)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PromptDataset(Dataset):\n",
    "    def __init__(self, vectors, sentences, tokenizer, max_length=50):\n",
    "        self.vectors = vectors\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vectors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vec = torch.tensor(self.vectors[idx])\n",
    "        sentence = self.sentences[idx]\n",
    "        # Tokenize the sentence. We use padding/truncation to a fixed length.\n",
    "        tokenized = self.tokenizer(sentence,\n",
    "                                   return_tensors=\"pt\",\n",
    "                                   padding=\"max_length\",\n",
    "                                   max_length=self.max_length,\n",
    "                                   truncation=True)\n",
    "        input_ids = tokenized.input_ids.squeeze(\n",
    "            0)        # Shape: (max_length,)\n",
    "        attention_mask = tokenized.attention_mask.squeeze(\n",
    "            0)  # Shape: (max_length,)\n",
    "        return vec.to('cuda'), input_ids.to('cuda'), attention_mask.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "n = 32\n",
    "vectors = data['df_train'].values.astype(np.float32)\n",
    "sentences = data['texts_train'].values.tolist()\n",
    "vectors = vectors[:n]\n",
    "\n",
    "# pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B\", device_map=\"auto\",)\n",
    "model.eval()  # We'll freeze the model so we only update the mapping network\n",
    "\n",
    "# model params\n",
    "d = 800\n",
    "prompt_length = 6\n",
    "batch_size = 32\n",
    "hidden_size = model.config.hidden_size\n",
    "mapper = PromptMapper(d, prompt_length, hidden_size).to('cuda')\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "dataset = PromptDataset(vectors, sentences, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "optimizer = torch.optim.Adam(mapper.parameters(), lr=1e-4)\n",
    "\n",
    "# We'll compute the loss only for the target sentence tokens.\n",
    "# To do so, we create labels that are -100 for the soft prompt positions.\n",
    "num_epochs = 10\n",
    "max_length = max([len(s) for s in sentences])  # from our dataset\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for vecs, input_ids, attention_mask in tqdm(dataloader):\n",
    "        batch_size = input_ids.shape[0]\n",
    "\n",
    "        # Get the soft prompt embeddings from our mapper network.\n",
    "        # soft_prompt: (batch_size, prompt_length, hidden_size)\n",
    "        soft_prompt = mapper(vecs)\n",
    "\n",
    "        # Obtain the input token embeddings from the LLaMA model.\n",
    "        token_embeddings = model.get_input_embeddings()(\n",
    "            input_ids)  # (batch_size, max_length, hidden_size)\n",
    "\n",
    "        # Concatenate the soft prompt with the token embeddings.\n",
    "        # New input: [soft prompt tokens] + [tokenized sentence]\n",
    "        inputs_embeds = torch.cat(\n",
    "            [soft_prompt, token_embeddings], dim=1)  # .to('cuda')\n",
    "\n",
    "        # Adjust the attention mask: add ones for the soft prompt positions.\n",
    "        prompt_mask = torch.ones(\n",
    "            (batch_size, prompt_length), dtype=attention_mask.dtype, device=attention_mask.device)\n",
    "        extended_attention_mask = torch.cat(\n",
    "            [prompt_mask, attention_mask], dim=1)\n",
    "\n",
    "        # Create labels: we ignore the soft prompt positions (set them to -100) so that loss is only computed for the sentence tokens.\n",
    "        labels = torch.cat([\n",
    "            torch.full((batch_size, prompt_length), -100,\n",
    "                       dtype=input_ids.dtype, device=input_ids.device),\n",
    "            input_ids\n",
    "        ], dim=1)\n",
    "\n",
    "        # Forward pass through the model.\n",
    "        outputs = model(inputs_embeds=inputs_embeds,\n",
    "                        attention_mask=extended_attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print decoded sentence examples\n",
    "        if epoch % 5 == 0:\n",
    "            with torch.no_grad():\n",
    "                soft_prompt = mapper(vecs)\n",
    "                inputs_embeds = torch.cat(\n",
    "                    [soft_prompt, token_embeddings], dim=1)\n",
    "                outputs = model.generate(\n",
    "                    inputs_embeds=inputs_embeds,\n",
    "                    attention_mask=extended_attention_mask,\n",
    "                    max_length=max_length,\n",
    "                    min_length=4,\n",
    "                    do_sample=False)\n",
    "                decoded_sentences = tokenizer.batch_decode(outputs[:, prompt_length:],\n",
    "                                                           skip_special_tokens=True)\n",
    "                for i in range(min(3, len(decoded_sentences))):\n",
    "                    print(f\"GT sentence {i+1}: {sentences[i]}\")\n",
    "                    print(f\"Decoded sentence {i+1}: {decoded_sentences[i]}\")\n",
    "                    print()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # save the model\n",
    "    torch.save(mapper.state_dict(), f\"mapper_epoch_{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(\n",
    "    inputs_embeds=inputs_embeds,\n",
    "    attention_mask=extended_attention_mask,\n",
    "    max_length=max_length,\n",
    "    min_length=20,\n",
    "    do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5. Inference Example\n",
    "# -----------------------------\n",
    "# To use the learned mapping:\n",
    "# Given a new vector, map it to a continuous prompt and generate the corresponding sentence.\n",
    "mapper.eval()\n",
    "with torch.no_grad():\n",
    "    vecs_test = data['df_test'].values.astype(np.float32)\n",
    "    sentences_test = data['texts_train'].values.tolist()\n",
    "    new_vector = torch.tensor(vecs_test[0]).unsqueeze(\n",
    "        0).to('cuda')  # shape (1, d)\n",
    "\n",
    "    # new_vector = torch.tensor(np.random.randn(d).astype(\n",
    "    # np.float32)).unsqueeze(0)  # shape (1, d)\n",
    "    soft_prompt = mapper(new_vector)  # (1, prompt_length, hidden_size)\n",
    "\n",
    "    # Start with an empty input (or a beginning-of-sentence token, if desired).\n",
    "    # Here we assume the model will generate based on the soft prompt.\n",
    "    dummy_input = torch.tensor(\n",
    "        [[tokenizer.bos_token_id]], device=soft_prompt.device)\n",
    "    token_embeddings = model.get_input_embeddings()(dummy_input)\n",
    "\n",
    "    inputs_embeds = torch.cat([soft_prompt, token_embeddings], dim=1)\n",
    "\n",
    "    prompt_mask = torch.ones(\n",
    "        (1, prompt_length), dtype=torch.long, device=soft_prompt.device)\n",
    "    dummy_mask = torch.ones_like(dummy_input)\n",
    "    extended_attention_mask = torch.cat([prompt_mask, dummy_mask], dim=1)\n",
    "\n",
    "    # Generate output tokens (adjust generation parameters as needed)\n",
    "    generated_ids = model.generate(inputs_embeds=inputs_embeds,\n",
    "                                   attention_mask=extended_attention_mask,\n",
    "                                   max_length=max_length + prompt_length)\n",
    "\n",
    "    # Decode the generated tokens (skipping the soft prompt positions)\n",
    "    # Since our soft prompt is continuous, the generated text typically follows after the dummy token.\n",
    "    output_text = tokenizer.decode(\n",
    "        generated_ids[0][prompt_length:], skip_special_tokens=True)\n",
    "    print('Original text:', sentences_test[0])\n",
    "    print(\"Generated sentence:\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(generated_ids[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
