{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import feature_spaces\n",
    "import encoding_utils\n",
    "import dvu\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import List\n",
    "from matplotlib import pyplot as plt\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import encoding_utils, feature_spaces\n",
    "import pickle as pkl\n",
    "from datasets import Dataset, DatasetDict\n",
    "from feature_spaces import *\n",
    "dvu.set_style()\n",
    "NUM_VOXELS = 250"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataset in standard format (csv)\n",
    "- 'text': Last 20 words as input text\n",
    "- 'vox1'...'vox250': regression response for each voxel\n",
    "- 'corr_test': correlation between predicted and actual response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at narrative stories\n",
    "train_stories, test_stories, allstories = encoding_utils.get_allstories([1, 2, 3, 4, 5])\n",
    "wordseqs = feature_spaces.get_story_wordseqs(allstories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 170.47it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 203.65it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_words_for_story(wordseq, max_running_words=30) -> List[str]:\n",
    "    running_words = []\n",
    "    \n",
    "    words = np.array(wordseq.data)\n",
    "    TRIM = 5\n",
    "    tr_times = wordseq.tr_times[5+TRIM: -TRIM]\n",
    "    for i, tr_time in enumerate(tr_times):\n",
    "        valid_times = wordseq.data_times <= tr_time\n",
    "        # print(valid_times)\n",
    "        running_words.append(' '.join(words[valid_times][-max_running_words:]))\n",
    "    #     print(tr_time, running_words)\n",
    "    return running_words\n",
    "\n",
    "texts_list_train = []\n",
    "for story_name in tqdm(train_stories):\n",
    "    wordseq = wordseqs[story_name]\n",
    "    texts_list_train.append(get_words_for_story(wordseq))\n",
    "texts_train = sum(texts_list_train, [])\n",
    "\n",
    "texts_list_test = []\n",
    "for story_name in tqdm(test_stories):\n",
    "    wordseq = wordseqs[story_name]\n",
    "    texts_list_test.append(get_words_for_story(wordseq))\n",
    "texts_test = sum(texts_list_test, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resp_train.shape=(9461, 95556), resp_test.shape=(291, 95556)\n"
     ]
    }
   ],
   "source": [
    "subj = 'UTS03'\n",
    "# (n_time_points x n_voxels)\n",
    "resp_train = encoding_utils.get_response(train_stories, subj)\n",
    "resp_test = encoding_utils.get_response(test_stories, subj)\n",
    "print(f\"{resp_train.shape=}, {resp_test.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76296981 0.74258237 0.72107898 0.71616266 0.71508206]\n",
      "resp_train_voxel.shape=(9461, 250), resp_test_voxel.shape=(291, 250)\n"
     ]
    }
   ],
   "source": [
    "# select top_idxs\n",
    "save_dir = '/home/chansingh/mntv1/deep-fMRI/results/encoding/bert-10__ndel=4/UTS03'\n",
    "corrs_val = np.load(join(save_dir, 'corrs.npz'))['arr_0']\n",
    "top_idxs = np.argsort(corrs_val)[::-1][:NUM_VOXELS]\n",
    "print(corrs_val[top_idxs][:5])\n",
    "\n",
    "# select top voxels\n",
    "resp_train_voxel = resp_train[:, top_idxs]\n",
    "resp_test_voxel = resp_test[:, top_idxs]\n",
    "print(f\"{resp_train_voxel.shape=}, {resp_test_voxel.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_train_voxel = StandardScaler().fit_transform(resp_train_voxel)\n",
    "resp_test_voxel = StandardScaler().fit_transform(resp_test_voxel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = {\n",
    "    'text': texts_train,\n",
    "}\n",
    "df_test = {\n",
    "    'text': texts_test,\n",
    "}\n",
    "for i in range(NUM_VOXELS):\n",
    "    df_train[f'voxel_{i}'] = resp_train_voxel[:, i]\n",
    "    df_test[f'voxel_{i}'] = resp_test_voxel[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_train = Dataset.from_pandas(pd.DataFrame.from_dict(df_train))\n",
    "dset_test = Dataset.from_pandas(pd.DataFrame.from_dict(df_test))\n",
    "ds = DatasetDict()\n",
    "ds['train'] = dset_train\n",
    "ds['test'] = dset_test\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split train to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8e6457d98b403abbc1ef2f7fdd04bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864d3f6f6b6c4f84bce3fe9b1b04550a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split test to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e628473bb3d742af9b1b6067cee66b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a35ff4b7a1046869b3189bbfad8f7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds.push_to_hub('csinva/fmri_language_responses')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.embgam')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "559535f78d940c882783b39501b2581b5193373045707e5f8a51d046029cfd38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
