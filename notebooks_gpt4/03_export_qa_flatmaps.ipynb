{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from ridge_utils.DataSequence import DataSequence\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cortex\n",
    "import seaborn as sns\n",
    "from os.path import join\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import sasc.viz\n",
    "import joblib\n",
    "import dvu\n",
    "import sys\n",
    "sys.path.append('../notebooks')\n",
    "from tqdm import tqdm\n",
    "from sasc.config import FMRI_DIR, STORIES_DIR, RESULTS_DIR\n",
    "from neuro.config import repo_dir, PROCESSED_DIR\n",
    "from neuro import analyze_helper, viz\n",
    "from neuro.features.qa_questions import get_questions, get_merged_questions_v3_boostexamples\n",
    "flatmaps_per_question = __import__('06_flatmaps_per_question')\n",
    "from neurosynth import term_dict, term_dict_rev\n",
    "import viz\n",
    "from load_coef_flatmaps import _load_coefs_35questions, _load_coefs_individual, \\\n",
    "_load_coefs_individual_wordrate, _load_coefs_wordrate, _load_coefs_shapley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = '/home/chansingh/mntv1/deep-fMRI/encoding/aug14_neurosynth_gemv'\n",
    "rr_shapley, cols_varied, mets = analyze_helper.load_clean_results(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_dir = '/home/chansingh/mntv1/deep-fMRI/encoding/jun16_gpt4'\n",
    "# rr, cols_varied, mets = analyze_helper.load_clean_results(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'S03'\n",
    "df_w_shapley35 = _load_coefs_shapley(rr_shapley,\n",
    "                                     subject=subject, qa_questions_version='v3_boostexamples_merged')\n",
    "\n",
    "df_w_shapley_neurosynth = _load_coefs_shapley(rr_shapley,\n",
    "                                              subject, qa_questions_version='v1neurosynth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _load_coefs_35_full(subject='S02'):\n",
    "#     # # load questions model weights\n",
    "#     # results_dir = analyze_helper.best_results_dir\n",
    "#     # rr, cols_varied, mets = analyze_helper.load_clean_results(results_dir)\n",
    "#     # metric_sort = 'corrs_tune_pc_weighted_mean'\n",
    "\n",
    "#     # # pick model to interpret\n",
    "#     # r = rr\n",
    "#     # r = r[r.qa_questions_version == 'v3_boostexamples_merged']\n",
    "#     # r = r[r.num_stories == -1]\n",
    "#     # r = r[r.weight_enet_mask_num_nonzero == 35]\n",
    "#     # r = r[r.feature_space == 'qa_embedder']\n",
    "#     # cols_varied = [c for c in cols_varied if not c in ['num_stories',\n",
    "#     #                                                    'feature_selection_alpha', 'feature_selection_stability_seeds']]\n",
    "#     # args0 = r[r.subject == subject].iloc[0]\n",
    "#     # weights, weights_pc = flatmaps_per_question.get_weights_top(args0)\n",
    "#     # qs_selected = questions[args0['weight_enet_mask']]\n",
    "#     # df_w_selected35 = pd.DataFrame({'question': qs_selected, 'weights': [\n",
    "#     #                     w for w in weights]}).set_index('question')\n",
    "#     # joblib.dump((args0, qs_selected, df_w_selected35),\n",
    "#     # '../qa_results/processed/selected_weights.pkl')\n",
    "\n",
    "#     args0, qs_selected, df_w_selected35 = joblib.load(\n",
    "#         '../qa_results/processed/selected_weights.pkl')\n",
    "#     return df_w_selected35\n",
    "\n",
    "subject = 'S02'\n",
    "r = rr_shapley\n",
    "r = r[r.qa_questions_version == 'v3_boostexamples_merged']\n",
    "r = r[r.weight_enet_mask_num_nonzero == 35]\n",
    "r = r[r.feature_space == 'qa_embedder']\n",
    "r = r[r.subject == subject]\n",
    "r = r[r.use_random_subset_features == 0]\n",
    "\n",
    "corrs_test = r['corrs_test']\n",
    "joblib.dump(corrs_test, join(PROCESSED_DIR,\n",
    "                             subject, 'corrs_test_35.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject = 'S02'\n",
    "for subject in ['S01', 'S02', 'S03']:\n",
    "\n",
    "    df_w_shapley_35 = _load_coefs_shapley(rr_shapley,\n",
    "                                          subject, qa_questions_version='v3_boostexamples_merged')\n",
    "    joblib.dump(df_w_shapley_35, join(\n",
    "        PROCESSED_DIR, subject, 'shapley_35.pkl'))\n",
    "\n",
    "    df_w_shapley_neurosynth = _load_coefs_shapley(rr_shapley,\n",
    "                                                  subject, qa_questions_version='v1neurosynth')\n",
    "    joblib.dump(df_w_shapley_neurosynth, join(PROCESSED_DIR,\n",
    "                subject, 'shapley_neurosynth.pkl'))\n",
    "\n",
    "    ########### use old models ###################\n",
    "    # jointly fitted 35-question model\n",
    "    # df_w_selected35 = _load_coefs_35questions(subject=subject)\n",
    "\n",
    "    # individually fitted question models\n",
    "    # df_w_individual = _load_coefs_individual(rr_shapley, subject=subject)\n",
    "    # joblib.dump(df_w_individual, join(PROCESSED_DIR,\n",
    "    # subject, 'individual.pkl'))\n",
    "\n",
    "    # individually fitted question models *with wordrate\n",
    "    # df_w_individual_wordrate = _load_coefs_individual_wordrate(\n",
    "    # subject=subject)\n",
    "\n",
    "    # wordrate\n",
    "    # df_w_wordrate_alone = _load_coefs_wordrate(subject=subject)\n",
    "\n",
    "    # # collate individual dfs #########################\n",
    "    # # average weights for df_w_selected35 and df_w_individual\n",
    "    # if subject == 'S02':\n",
    "    #     df_avg = df_w_selected35.merge(df_w_individual, on='question')\n",
    "    #     df_avg['weights'] = df_avg.apply(\n",
    "    #         lambda x: np.mean([x['weights_x'], x['weights_y']], axis=0), axis=1)\n",
    "\n",
    "    # df_avg_individual = df_w_individual.merge(\n",
    "    #     df_w_individual_wordrate, on='question')\n",
    "    # df_avg_individual['weights'] = df_avg_individual.apply(\n",
    "    #     lambda x: np.mean([x['weights_x'], x['weights_y']], axis=0), axis=1)\n",
    "\n",
    "    # df_qa_dict = {\n",
    "    #     'selected35': df_w_selected35,\n",
    "    #     'individual': df_w_individual,\n",
    "    #     'individual_wordrate': df_w_individual_wordrate,\n",
    "    #     'wordrate_alone': df_w_wordrate_alone,\n",
    "    #     # 'avg': df_avg,\n",
    "    #     'shapley_neurosynth': df_w_shapley_neurosynth,\n",
    "    #     'shapley35': df_w_shapley35,\n",
    "    #     'avg_individual': df_avg_individual\n",
    "    # }\n",
    "    # joblib.dump(df_qa_dict, f'df_qa_dict_{subject}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.embgam')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "559535f78d940c882783b39501b2581b5193373045707e5f8a51d046029cfd38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
