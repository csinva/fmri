{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from ridge_utils.DataSequence import DataSequence\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cortex\n",
    "import seaborn as sns\n",
    "from os.path import join\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import sasc.viz\n",
    "import joblib\n",
    "import dvu\n",
    "import sys\n",
    "sys.path.append('../notebooks')\n",
    "from tqdm import tqdm\n",
    "from sasc.config import FMRI_DIR, STORIES_DIR, RESULTS_DIR\n",
    "from neuro.config import repo_dir, PROCESSED_DIR\n",
    "from neuro import analyze_helper, viz\n",
    "from neuro.features.qa_questions import get_questions, get_merged_questions_v3_boostexamples\n",
    "flatmaps_per_question = __import__('06_flatmaps_per_question')\n",
    "from neurosynth import term_dict, term_dict_rev\n",
    "import viz\n",
    "from load_coef_flatmaps import _load_coefs_individual, _load_coefs_full, \\\n",
    "_load_coefs_individual_wordrate, _load_coefs_wordrate, _load_coefs_shapley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [00:57<00:00,  9.17it/s]\n",
      "/home/chansingh/imodelsx/imodelsx/process_results.py:92: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[k] = df[k].fillna(np.nan)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment varied these params: ['subject', 'feature_selection_alpha', 'qa_questions_version', 'use_random_subset_features', 'single_question_idx', 'seed']\n"
     ]
    }
   ],
   "source": [
    "results_dir = '/home/chansingh/mntv1/deep-fMRI/encoding/aug14_neurosynth_gemv'\n",
    "rr, cols_varied, mets = analyze_helper.load_clean_results(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_dir = '/home/chansingh/mntv1/deep-fMRI/encoding/jun16_gpt4'\n",
    "# rr, cols_varied, mets = analyze_helper.load_clean_results(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'S03'\n",
    "df_w_shapley35 = _load_coefs_individual(rr,\n",
    "                                        subject=subject, qa_questions_version='v3_boostexamples_merged')\n",
    "\n",
    "df_w_shapley_neurosynth = _load_coefs_individual(rr,\n",
    "                                                 subject, qa_questions_version='v1neurosynth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:09<00:00,  3.74it/s]\n",
      "100%|██████████| 35/35 [00:07<00:00,  4.85it/s]\n",
      "100%|██████████| 35/35 [00:08<00:00,  4.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# subject = 'S02'\n",
    "for subject in ['S01', 'S02', 'S03']:\n",
    "    # df = _load_coefs_individual(\n",
    "    # rr, subject=subject, qa_questions_version='v1neurosynth')\n",
    "    # joblib.dump(df, join(PROCESSED_DIR, subject, 'individual_neurosynth.pkl'))\n",
    "\n",
    "    df = _load_coefs_individual(\n",
    "        rr, subject=subject, qa_questions_version='v3_boostexamples_merged')\n",
    "    joblib.dump(df, join(PROCESSED_DIR, subject, 'individual_35.pkl'))\n",
    "\n",
    "    df = _load_coefs_full(\n",
    "        rr, subject=subject, qa_questions_version='v3_boostexamples_merged')\n",
    "    joblib.dump(df, join(PROCESSED_DIR, subject, 'full_35.pkl'))\n",
    "\n",
    "    df = _load_coefs_full(\n",
    "        rr, subject=subject, qa_questions_version='v1neurosynth')\n",
    "    joblib.dump(df, join(PROCESSED_DIR, subject, 'full_neurosynth.pkl'))\n",
    "\n",
    "    df = _load_coefs_shapley(\n",
    "        rr, subject, qa_questions_version='v3_boostexamples_merged')\n",
    "    joblib.dump(df, join(\n",
    "        PROCESSED_DIR, subject, 'shapley_35.pkl'))\n",
    "\n",
    "    df = _load_coefs_shapley(\n",
    "        rr, subject, qa_questions_version='v1neurosynth')\n",
    "    joblib.dump(df, join(PROCESSED_DIR,\n",
    "                subject, 'shapley_neurosynth.pkl'))\n",
    "\n",
    "    ########### use old models ###################\n",
    "    # jointly fitted 35-question model\n",
    "    # df_w_selected35 = _load_coefs_35questions(subject=subject)\n",
    "\n",
    "    # individually fitted question models\n",
    "    # df_w_individual = _load_coefs_individual(rr_shapley, subject=subject)\n",
    "    # joblib.dump(df_w_individual, join(PROCESSED_DIR,\n",
    "    # subject, 'individual.pkl'))\n",
    "\n",
    "    # individually fitted question models *with wordrate\n",
    "    # df_w_individual_wordrate = _load_coefs_individual_wordrate(\n",
    "    # subject=subject)\n",
    "\n",
    "    # wordrate\n",
    "    # df_w_wordrate_alone = _load_coefs_wordrate(subject=subject)\n",
    "\n",
    "    # # collate individual dfs #########################\n",
    "    # # average weights for df_w_selected35 and df_w_individual\n",
    "    # if subject == 'S02':\n",
    "    #     df_avg = df_w_selected35.merge(df_w_individual, on='question')\n",
    "    #     df_avg['weights'] = df_avg.apply(\n",
    "    #         lambda x: np.mean([x['weights_x'], x['weights_y']], axis=0), axis=1)\n",
    "\n",
    "    # df_avg_individual = df_w_individual.merge(\n",
    "    #     df_w_individual_wordrate, on='question')\n",
    "    # df_avg_individual['weights'] = df_avg_individual.apply(\n",
    "    #     lambda x: np.mean([x['weights_x'], x['weights_y']], axis=0), axis=1)\n",
    "\n",
    "    # df_qa_dict = {\n",
    "    #     'selected35': df_w_selected35,\n",
    "    #     'individual': df_w_individual,\n",
    "    #     'individual_wordrate': df_w_individual_wordrate,\n",
    "    #     'wordrate_alone': df_w_wordrate_alone,\n",
    "    #     # 'avg': df_avg,\n",
    "    #     'shapley_neurosynth': df_w_shapley_neurosynth,\n",
    "    #     'shapley35': df_w_shapley35,\n",
    "    #     'avg_individual': df_avg_individual\n",
    "    # }\n",
    "    # joblib.dump(df_qa_dict, f'df_qa_dict_{subject}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.embgam')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "559535f78d940c882783b39501b2581b5193373045707e5f8a51d046029cfd38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
