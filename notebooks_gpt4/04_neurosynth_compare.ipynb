{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cortex\n",
    "from os.path import join\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('../notebooks')\n",
    "from neuro.config import repo_dir, PROCESSED_DIR\n",
    "from neuro import viz\n",
    "from neurosynth import term_dict, term_dict_rev\n",
    "import viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, this notebook requires first running `03_export_qa_flatmaps.ipynb` into `df_qa_dict.pkl` files for each subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load neurosynth flatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neurosynth_flatmaps(subject, neurosynth_dir='/home/chansingh/mntv1/deep-fMRI/qa/neurosynth_data'):\n",
    "    subject_s = subject.replace('UT', '')\n",
    "    # neurosynth_dir = '/home/chansingh/mntv1/deep-fMRI/qa/neurosynth_data/all_association-test_z'\n",
    "\n",
    "    term_names = [k.replace('.nii.gz', '').replace(\n",
    "        '_association-test_z', '') for k in os.listdir(join(neurosynth_dir, f'all_in_{subject_s}-BOLD'))]\n",
    "\n",
    "    # filter dict for files that were in neurosynth\n",
    "    term_dict_ = {k: v for k, v in term_dict.items() if k in term_names}\n",
    "    # for k in term_dict.keys():\n",
    "    #     if k not in term_names:\n",
    "    #         print(k)\n",
    "\n",
    "    # filter dict for files that had questions run\n",
    "    questions_run = [k.replace('.pkl', '') for k in os.listdir(\n",
    "        '/home/chansingh/mntv1/deep-fMRI/qa/cache_gpt')]\n",
    "    term_dict_ = {k: v for k, v in term_dict_.items() if v in questions_run}\n",
    "\n",
    "    def _load_flatmap(term, neurosynth_dir, subject):\n",
    "        # output_file = join(neurosynth_dir, f'{term}_association-test_z.nii.gz')\n",
    "        output_file = join(\n",
    "            neurosynth_dir, f'all_in_{subject_s}-BOLD/{term}.nii.gz')\n",
    "        vol = cortex.Volume(output_file, subject, subject + '_auto').data\n",
    "        mask = cortex.db.get_mask(subject, subject + '_auto')\n",
    "        return vol[mask]\n",
    "\n",
    "    return {q: _load_flatmap(\n",
    "        term, neurosynth_dir, subject) for (term, q) in term_dict_.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute correlations with qa flatmaps and plot avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting = 'shapley_neurosynth'\n",
    "# setting = 'full_neurosynth'\n",
    "\n",
    "apply_mask = True\n",
    "frac_voxels_to_keep = 1  # 0.10\n",
    "\n",
    "\n",
    "corrs_df_list = defaultdict(list)\n",
    "for subject in tqdm(['UTS01', 'UTS02', 'UTS03']):\n",
    "    flatmaps_gt = get_neurosynth_flatmaps(subject)\n",
    "\n",
    "    flatmaps_qa_list = defaultdict(list)\n",
    "    # , 'individual_neurosynth']:\n",
    "    # for setting in ['shapley_neurosynth', 'full_neurosynth', 'individual_gpt4']:\n",
    "    # , 'individual_gpt4', 'shapley_neurosynth']:\n",
    "    for setting in ['shapley_neurosynth']:\n",
    "        flatmaps_qa = joblib.load(\n",
    "            join(PROCESSED_DIR, subject.replace('UT', ''), setting + '.pkl'))\n",
    "        for q in flatmaps_qa.keys():\n",
    "            flatmaps_qa_list[q].append(flatmaps_qa[q])\n",
    "    flatmaps_qa = {\n",
    "        q: np.mean(flatmaps_qa_list[q], axis=0)\n",
    "        for q in flatmaps_qa_list.keys()}\n",
    "\n",
    "    if apply_mask:\n",
    "        corrs_test = joblib.load(join(PROCESSED_DIR, subject.replace(\n",
    "            'UT', ''), 'corrs_test_35.pkl')).values[0]\n",
    "        # threshold\n",
    "        if frac_voxels_to_keep < 1:\n",
    "            corrs_test_mask = (corrs_test > np.percentile(\n",
    "                corrs_test, 100 * (1 - frac_voxels_to_keep))).astype(bool)\n",
    "        else:\n",
    "            corrs_test_mask = np.ones_like(corrs_test).astype(bool)\n",
    "        flatmaps_qa = {k: flatmaps_qa[k][corrs_test_mask]\n",
    "                       for k in flatmaps_qa.keys()}\n",
    "        flatmaps_gt = {k: flatmaps_gt[k][corrs_test_mask]\n",
    "                       for k in flatmaps_gt.keys()}\n",
    "\n",
    "    # get common flatmaps and put into d\n",
    "    common_keys = set(flatmaps_gt.keys()) & set(\n",
    "        flatmaps_qa.keys())\n",
    "    d = defaultdict(list)\n",
    "    for k in common_keys:\n",
    "        d['questions'].append(k)\n",
    "        d['corr'].append(np.corrcoef(flatmaps_qa[k],\n",
    "                                     flatmaps_gt[k])[0, 1])\n",
    "        d['flatmap_qa'].append(flatmaps_qa[k])\n",
    "        d['flatmap_neurosynth'].append(flatmaps_gt[k])\n",
    "    d = pd.DataFrame(d).sort_values('corr', ascending=False)\n",
    "\n",
    "    corrs = viz._calc_corrs(\n",
    "        d['flatmap_qa'].values,\n",
    "        d['flatmap_neurosynth'].values,\n",
    "        titles_qa=d['questions'].values,\n",
    "        titles_gt=d['questions'].values,\n",
    "    )\n",
    "\n",
    "    corrs_df_list['corrs'].extend(np.diag(corrs).tolist())\n",
    "    corrs_df_list['questions'].extend(d['questions'].values.tolist())\n",
    "    corrs_df_list['subject'].extend([subject] * len(d['questions'].values))\n",
    "\n",
    "    # viz.corr_bars(\n",
    "    #     corrs,\n",
    "    #     out_dir_save=join(repo_dir, 'qa_results', 'neurosynth', setting),\n",
    "    #     xlab='Neurosynth',\n",
    "    # )\n",
    "\n",
    "    # save flatmaps\n",
    "    # for i in tqdm(range(len(d))):\n",
    "    #     sasc.viz.quickshow(\n",
    "    #         d.iloc[i]['flatmap_qa'],\n",
    "    #         subject=subject,\n",
    "    #         fname_save=join(repo_dir, 'qa_results', 'neurosynth', subject,\n",
    "    #                         setting, f'{d.iloc[i][\"questions\"]}.png')\n",
    "    #     )\n",
    "\n",
    "    #     sasc.viz.quickshow(\n",
    "    #         d.iloc[i]['flatmap_neurosynth'],\n",
    "    #         subject=subject,\n",
    "    #         fname_save=join(repo_dir, 'qa_results', 'neurosynth', subject,\n",
    "    #                         'neurosynth', f'{d.iloc[i][\"questions\"]}.png')\n",
    "    #     )\n",
    "corrs_df = pd.DataFrame(corrs_df_list)\n",
    "# corrs_df.to_pickle(join(repo_dir, 'qa_results',\n",
    "#    'neurosynth', setting + '_corrs_df.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng1000 = joblib.load(\n",
    "    join(PROCESSED_DIR, subject.replace('UT', ''), 'eng1000_weights.pkl'))\n",
    "masks_eng1000 = [\n",
    "    np.abs(eng1000[i_]) >= np.percentile(\n",
    "        np.abs(eng1000[i_]), 100 * (1 - frac_voxels_to_keep))\n",
    "    for i_ in range(eng1000.shape[0])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrs_df = pd.DataFrame(corrs_df_dict)\n",
    "frac_voxels_to_keep_list = [0.01, 0.05, 0.1, 0.25, 0.5, 1]\n",
    "for frac_voxels_to_keep in tqdm(frac_voxels_to_keep_list):\n",
    "    pvals = []\n",
    "    for i in range(len(flatmaps_qa)):\n",
    "        corrs_perm_eng1000 = np.array(list(flatmaps_gt.values()))[\n",
    "            :, masks_eng1000[i]] @ eng1000[:, masks_eng1000[i]].T\n",
    "        pvals.append((corrs_perm_eng1000 >\n",
    "                     corrs_df[f'corrs_{frac_voxels_to_keep}'].values[i]).mean())\n",
    "\n",
    "    # get what fraction of 'corrs_perm_eng1000' column is greater than 'corrs'\n",
    "    corrs_df[f'pval_{frac_voxels_to_keep}'] = pvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_masked = np.array(list(flatmaps_gt.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_masked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_eng1000[i].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_perm_eng1000[:, masks_eng1000[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot correlations in corrs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = corrs_df\n",
    "xlab = f'Flatmap correlation (Top-{int(100*frac_voxels_to_keep)}% best-predicted voxels)'\n",
    "plt.figure(figsize=(7, 5))\n",
    "colors = {\n",
    "    'UTS01': 'C0',\n",
    "    'UTS02': 'C1',\n",
    "    'UTS03': 'C2',\n",
    "    'mean': 'black'\n",
    "}\n",
    "\n",
    "d_mean = pd.DataFrame(c.groupby('questions')[\n",
    "    'corrs'].mean()).reset_index()\n",
    "d_mean['subject'] = 'mean'\n",
    "c = pd.concat([c, d_mean])\n",
    "c = c.set_index('questions')\n",
    "\n",
    "for subject in ['mean', 'UTS01', 'UTS02', 'UTS03']:\n",
    "    r_df = c[c['subject'] == subject]\n",
    "    if subject == 'mean':\n",
    "        idx_sort = r_df['corrs'].sort_values(ascending=False).index\n",
    "    r_df = r_df.loc[idx_sort]\n",
    "\n",
    "    # plot corrs\n",
    "    if subject == 'mean':\n",
    "        plt.errorbar(\n",
    "            r_df['corrs'],\n",
    "            range(len(r_df)),\n",
    "            color='black',\n",
    "            fmt='o',\n",
    "            zorder=1000,\n",
    "            label=subject.capitalize(),\n",
    "        )\n",
    "    else:\n",
    "        plt.errorbar(\n",
    "            r_df['corrs'],\n",
    "            range(len(r_df)),\n",
    "            # xerr=np.sqrt(\n",
    "            # r_df['corrs'] * (1-r_df['corrs'])\n",
    "            # / r_df['num_test']),\n",
    "            alpha=0.5,\n",
    "            label=subject.upper(),\n",
    "            fmt='o')\n",
    "    plt.axvline(r_df['corrs'].mean(),\n",
    "                linestyle='--', color=colors[subject], zorder=-1000)\n",
    "\n",
    "    print('mean corr', r_df['corrs'].mean())\n",
    "\n",
    "# add horizontal bars\n",
    "plt.yticks(range(len(r_df)), [term_dict_rev[k] for k in idx_sort])\n",
    "plt.xlabel(xlab, fontsize='large')\n",
    "plt.grid(axis='y', alpha=0.2)\n",
    "plt.axvline(0, color='gray')\n",
    "\n",
    "abs_lim = max(np.abs(plt.xlim()))\n",
    "plt.xlim(-abs_lim, abs_lim)\n",
    "\n",
    "# annotate with baseline and text label\n",
    "plt.legend(fontsize='large')\n",
    "plt.tight_layout()\n",
    "plt.savefig(join(repo_dir, 'qa_results',\n",
    "            'neurosynth', 'corrs_' + setting + '.png'), dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at merged flatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_df = pd.read_pickle(join(repo_dir, 'qa_results',\n",
    "                               'neurosynth', setting + '_corrs_df.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = 'shapley_neurosynth'\n",
    "for subject in ['UTS01', 'UTS02', 'UTS03']:\n",
    "    img_dir1 = join(repo_dir, 'qa_results', 'neurosynth',\n",
    "                    subject, 'neurosynth')\n",
    "    img_dir2 = join(repo_dir, 'qa_results', 'neurosynth',\n",
    "                    subject, setting)\n",
    "\n",
    "    # read images and combine them with their filenames on a single plot\n",
    "    # fnames = os.listdir(img_dir1)\n",
    "    # fnames = [f for f in fnames if f.endswith('.png')]\n",
    "    # only keep the ones that are in both directories\n",
    "    # fnames = [f for f in fnames if f in os.listdir(img_dir2)]\n",
    "\n",
    "    corrs = corrs_df[corrs_df['subject'] == subject]\n",
    "    # corrs = corrs.sort_values('corrs', ascending=False)\n",
    "    fnames = [v + '.png' for v in corrs['questions'].values]\n",
    "\n",
    "    n = len(fnames)\n",
    "    C = 4\n",
    "    R = int(np.ceil(n / C))\n",
    "\n",
    "    fig, axs = plt.subplots(R, C, figsize=(C * 3.2, R * 1))\n",
    "    axs = axs.flatten()\n",
    "    for i in range(len(axs)):\n",
    "        axs[i].axis('off')\n",
    "    for i, fname in enumerate(fnames):\n",
    "        img1 = plt.imread(join(img_dir1, fname))\n",
    "        img2 = plt.imread(join(img_dir2, fname))\n",
    "        axs[i].imshow(np.concatenate([img1, img2], axis=1))\n",
    "        axs[i].set_title(\n",
    "            f'{term_dict_rev[fname[:-4]]} ({corrs[\"corrs\"].values[i]:0.3f})', fontsize=8)\n",
    "\n",
    "    # add text in bottom right of figure\n",
    "    fig.text(0.99, 0.01, f'{subject}\\nNeurosynth on left, QA on right',\n",
    "             ha='right', va='bottom', fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(join(repo_dir, 'qa_results', 'neurosynth',\n",
    "                subject, f'flatmaps_{setting}_{subject}.png'), dpi=300)\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
