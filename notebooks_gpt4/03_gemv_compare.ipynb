{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from ridge_utils.DataSequence import DataSequence\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cortex\n",
    "import seaborn as sns\n",
    "from os.path import join\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import sasc.viz\n",
    "import joblib\n",
    "import dvu\n",
    "import sys\n",
    "sys.path.append('../notebooks')\n",
    "from tqdm import tqdm\n",
    "from sasc.config import FMRI_DIR, STORIES_DIR, RESULTS_DIR\n",
    "from neuro.config import repo_dir\n",
    "from neuro import analyze_helper, viz\n",
    "from neuro.features.qa_questions import get_questions, get_merged_questions_v3_boostexamples\n",
    "flatmaps_per_question = __import__('06_flatmaps_per_question')\n",
    "from neurosynth import term_dict, term_dict_rev\n",
    "import viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_coef_flatmaps import _load_coefs_35questions, _load_coefs_individual, _load_coefs_individual_wordrate, _load_coefs_wordrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load fitted models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 633.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment varied these params: ['subject', 'feature_selection_alpha', 'qa_questions_version', 'seed']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/chansingh/imodelsx/imodelsx/process_results.py:92: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[k] = df[k].fillna(np.nan)\n"
     ]
    }
   ],
   "source": [
    "results_dir = '/home/chansingh/mntv1/deep-fMRI/encoding/aug12_neurosynth_gemv'\n",
    "rr, cols_varied, mets = analyze_helper.load_clean_results(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'S02'\n",
    "\n",
    "r = rr[rr['qa_questions_version'] == 'v3_boostexamples_merged']\n",
    "r = r[r['subject'] == subject]\n",
    "row = r.iloc[0]\n",
    "\n",
    "questions = get_merged_questions_v3_boostexamples()\n",
    "questions = np.array(questions)[row.weight_enet_mask]\n",
    "\n",
    "for i in range(len(r)):\n",
    "    flatmaps_shapley = defaultdict(list)\n",
    "    weights, weights_pc = flatmaps_per_question.get_weights_top(row)\n",
    "    question_indexes = np.arange(len(questions))[\n",
    "        row['weight_random_mask'][:len(questions)]]\n",
    "\n",
    "    for i, q_idx in enumerate(question_indexes):\n",
    "        flatmaps_shapley[questions[q_idx]].append(weights[i])\n",
    "for q, w in flatmaps_shapley.items():\n",
    "    flatmaps_shapley[q] = np.vstack(w).shape\n",
    "flatmaps_shapley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject = 'S02'\n",
    "for subject in ['S01', 'S02', 'S03']:\n",
    "    # jointly fitted 35-question model\n",
    "    df_w_selected35 = _load_coefs_35questions(subject=subject)\n",
    "\n",
    "    # individually fitted question models\n",
    "    df_w_individual = _load_coefs_individual(subject=subject)\n",
    "\n",
    "    # individually fitted question models *with wordrate\n",
    "    df_w_individual_wordrate = _load_coefs_individual_wordrate(\n",
    "        subject=subject)\n",
    "\n",
    "    # wordrate\n",
    "    df_w_wordrate_alone = _load_coefs_wordrate(subject=subject)\n",
    "\n",
    "    # collate individual dfs #########################\n",
    "    # average weights for df_w_selected35 and df_w_individual\n",
    "    if subject == 'S02':\n",
    "        df_avg = df_w_selected35.merge(df_w_individual, on='question')\n",
    "        df_avg['weights'] = df_avg.apply(\n",
    "            lambda x: np.mean([x['weights_x'], x['weights_y']], axis=0), axis=1)\n",
    "\n",
    "    df_avg_individual = df_w_individual.merge(\n",
    "        df_w_individual_wordrate, on='question')\n",
    "    df_avg_individual['weights'] = df_avg_individual.apply(\n",
    "        lambda x: np.mean([x['weights_x'], x['weights_y']], axis=0), axis=1)\n",
    "\n",
    "    df_qa_dict = {\n",
    "        'selected35': df_w_selected35,\n",
    "        'individual': df_w_individual,\n",
    "        'individual_wordrate': df_w_individual_wordrate,\n",
    "        'wordrate_alone': df_w_wordrate_alone,\n",
    "        # 'avg': df_avg,\n",
    "        'avg_individual': df_avg_individual\n",
    "    }\n",
    "    joblib.dump(df_qa_dict, f'df_qa_dict_{subject}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load \"groundtruth\" comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load gemv average flatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemv_avgs_pilot = joblib.load(join(\n",
    "    RESULTS_DIR, \"processed\", \"flatmaps\", 'resps_avg_dict_pilot.pkl'))\n",
    "gemv_avgs_pilot5 = joblib.load(join(\n",
    "    RESULTS_DIR, \"processed\", \"flatmaps\", 'resps_avg_dict_pilot5.pkl'))\n",
    "gemv_avg_flatmaps = gemv_avgs_pilot | gemv_avgs_pilot5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load neurosynth avg. flatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neurosynth_flatmaps(subject, neurosynth_dir='/home/chansingh/mntv1/deep-fMRI/qa/neurosynth_data'):\n",
    "    subject_s = subject.replace('UT', '')\n",
    "    # neurosynth_dir = '/home/chansingh/mntv1/deep-fMRI/qa/neurosynth_data/all_association-test_z'\n",
    "\n",
    "    term_names = [k.replace('.nii.gz', '').replace(\n",
    "        '_association-test_z', '') for k in os.listdir(join(neurosynth_dir, f'all_in_{subject_s}-BOLD'))]\n",
    "\n",
    "    # filter dict for files that were in neurosynth\n",
    "    term_dict_ = {k: v for k, v in term_dict.items() if k in term_names}\n",
    "    # for k in term_dict.keys():\n",
    "    #     if k not in term_names:\n",
    "    #         print(k)\n",
    "\n",
    "    # filter dict for files that had questions run\n",
    "    questions_run = [k.replace('.pkl', '') for k in os.listdir(\n",
    "        '/home/chansingh/mntv1/deep-fMRI/qa/cache_gpt')]\n",
    "    term_dict_ = {k: v for k, v in term_dict_.items() if v in questions_run}\n",
    "\n",
    "    def _load_flatmap(term, neurosynth_dir, subject):\n",
    "        # output_file = join(neurosynth_dir, f'{term}_association-test_z.nii.gz')\n",
    "        output_file = join(\n",
    "            neurosynth_dir, f'all_in_{subject_s}-BOLD/{term}.nii.gz')\n",
    "        vol = cortex.Volume(output_file, subject, subject + '_auto').data\n",
    "        mask = cortex.db.get_mask(subject, subject + '_auto')\n",
    "        return vol[mask]\n",
    "\n",
    "    return {q: _load_flatmap(\n",
    "        term, neurosynth_dir, subject) for (term, q) in term_dict_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = 'avg_individual'\n",
    "corrs_df_list = defaultdict(list)\n",
    "for subject in ['UTS01', 'UTS02', 'UTS03']:\n",
    "    neurosynth_flatmaps = get_neurosynth_flatmaps(subject)\n",
    "    df_qa_dict = joblib.load(f'df_qa_dict_{subject.replace(\"UT\", \"\")}.pkl')\n",
    "    qa_flatmaps = df_qa_dict[setting].to_dict()['weights']\n",
    "\n",
    "    # get common flatmaps and put into d\n",
    "    common_keys = set(neurosynth_flatmaps.keys()) & set(\n",
    "        qa_flatmaps.keys())\n",
    "    d = defaultdict(list)\n",
    "    for k in common_keys:\n",
    "        d['questions'].append(k)\n",
    "        d['corr'].append(np.corrcoef(qa_flatmaps[k],\n",
    "                                     neurosynth_flatmaps[k])[0, 1])\n",
    "        d['flatmap_qa'].append(qa_flatmaps[k])\n",
    "        d['flatmap_neurosynth'].append(neurosynth_flatmaps[k])\n",
    "    d = pd.DataFrame(d).sort_values('corr', ascending=False)\n",
    "\n",
    "    corrs = viz._calc_corrs(\n",
    "        d['flatmap_qa'].values,\n",
    "        d['flatmap_neurosynth'].values,\n",
    "        titles_qa=d['questions'].values,\n",
    "        titles_gt=d['questions'].values,\n",
    "    )\n",
    "\n",
    "    corrs_df_list['corrs'].extend(np.diag(corrs).tolist())\n",
    "    corrs_df_list['questions'].extend(d['questions'].values.tolist())\n",
    "    corrs_df_list['subject'].extend([subject] * len(d['questions'].values))\n",
    "\n",
    "    # viz.corr_bars(\n",
    "    #     corrs,\n",
    "    #     out_dir_save=join(repo_dir, 'qa_results', 'neurosynth', setting),\n",
    "    #     xlab='Neurosynth',\n",
    "    # )\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_df = pd.DataFrame(corrs_df_list)\n",
    "d_mean = pd.DataFrame(corrs_df.groupby('questions')[\n",
    "                      'corrs'].mean()).reset_index()\n",
    "d_mean['subject'] = 'mean'\n",
    "corrs_df = pd.concat([corrs_df, d_mean])\n",
    "corrs_df = corrs_df.set_index('questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "colors = {\n",
    "    'UTS01': 'C0',\n",
    "    'UTS02': 'C1',\n",
    "    'UTS03': 'C2',\n",
    "    'mean': 'black'\n",
    "}\n",
    "for subject in ['mean', 'UTS01', 'UTS02', 'UTS03']:\n",
    "    r_df = corrs_df[corrs_df['subject'] == subject]\n",
    "    if subject == 'mean':\n",
    "        idx_sort = r_df['corrs'].sort_values(ascending=False).index\n",
    "\n",
    "    r_df = r_df.loc[idx_sort]\n",
    "\n",
    "    # plot corrs\n",
    "    if subject == 'mean':\n",
    "        plt.errorbar(\n",
    "            r_df['corrs'],\n",
    "            range(len(r_df)),\n",
    "            color='black',\n",
    "            fmt='o',\n",
    "            zorder=1000,\n",
    "            label=subject.capitalize(),\n",
    "        )\n",
    "    else:\n",
    "        plt.errorbar(\n",
    "            r_df['corrs'],\n",
    "            range(len(r_df)),\n",
    "            # xerr=np.sqrt(\n",
    "            # r_df['corrs'] * (1-r_df['corrs'])\n",
    "            # / r_df['num_test']),\n",
    "            alpha=0.5,\n",
    "            label=subject.upper(),\n",
    "            fmt='o')\n",
    "    plt.axvline(r_df['corrs'].mean(),\n",
    "                linestyle='--', color=colors[subject], zorder=-1000)\n",
    "\n",
    "    print('mean corr', r_df['corrs'].mean())\n",
    "\n",
    "# add horizontal bars\n",
    "plt.yticks(range(len(r_df)), [term_dict_rev[k] for k in idx_sort])\n",
    "plt.xlabel(\n",
    "    'Neurosynth flatmap correlation', fontsize='large')\n",
    "plt.grid(axis='y', alpha=0.2)\n",
    "plt.axvline(0, color='gray')\n",
    "\n",
    "abs_lim = max(np.abs(plt.xlim()))\n",
    "plt.xlim(-abs_lim, abs_lim)\n",
    "\n",
    "# annotate with baseline and text label\n",
    "plt.legend(fontsize='large')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('linear_decoding.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matches\n",
    "qa_list = [\n",
    "    # approx matches\n",
    "    'Is time mentioned in the input?',\n",
    "    'Does the input contain a measurement?',\n",
    "    # 'Does the input contain a number?',\n",
    "    'Does the sentence mention a specific location?',\n",
    "    # 'Does the sentence describe a relationship between people?',\n",
    "    # 'Does the sentence describe a relationship between people?',\n",
    "    'Does the text describe a mode of communication?',\n",
    "    # 'Does the sentence contain a negation?',\n",
    "]\n",
    "gemv_list = [\n",
    "    # approx matches\n",
    "    ('time', 212),\n",
    "    ('measurements', 171),\n",
    "    # ('measurements', 171),\n",
    "    # ('moments',\t337),\n",
    "    # ('locations', 122),\n",
    "    ('locations', 368),\n",
    "    # ('emotion', 179),\n",
    "    # ('emotional expression', 398),\n",
    "    ('communication', 299),\n",
    "    # ('negativity', 248)\n",
    "]\n",
    "\n",
    "\n",
    "qa_list += [\n",
    "    'Is the sentence abstract rather than concrete?',\n",
    "    'Does the sentence contain a cultural reference?',\n",
    "    'Does the sentence include dialogue?',\n",
    "    'Is the input related to a specific industry or profession?',\n",
    "    'Does the sentence contain a negation?',\n",
    "    'Does the input contain a number?',\n",
    "    \"Does the sentence express the narrator's opinion or judgment about an event or character?\",\n",
    "    'Does the sentence describe a personal or social interaction that leads to a change or revelation?',\n",
    "    'Does the sentence describe a personal reflection or thought?',\n",
    "    'Does the sentence involve an expression of personal values or beliefs?',\n",
    "    'Does the sentence describe a physical action?',\n",
    "    'Does the input involve planning or organizing?',\n",
    "    'Does the sentence contain a proper noun?',\n",
    "    'Does the sentence describe a relationship between people?',\n",
    "    'Does the sentence describe a sensory experience?',\n",
    "    'Does the sentence involve the mention of a specific object or item?',\n",
    "    'Does the sentence include technical or specialized terminology?',\n",
    "]\n",
    "\n",
    "gemv_list += [\n",
    "    ('abstract descriptions', 'qa'),\n",
    "    ('cultural references', 'qa'),\n",
    "    ('dialogue', 'qa'),\n",
    "    ('industry or profession', 'qa'),\n",
    "    ('negations', 'qa'),\n",
    "    ('numbers', 'qa'),\n",
    "    ('opinions or judgments', 'qa'),\n",
    "    ('personal or interactions interactions', 'qa'),\n",
    "    ('personal reflections or thoughts', 'qa'),\n",
    "    ('personal values or beliefs', 'qa'),\n",
    "    ('physical actions', 'qa'),\n",
    "    ('planning or organizing', 'qa'),\n",
    "    ('proper nouns', 'qa'),\n",
    "    ('relationships between people', 'qa'),\n",
    "    ('sensory experiences', 'qa'),\n",
    "    ('specific objects or items', 'qa'),\n",
    "    ('technical or specialized terminology', 'qa')\n",
    "]\n",
    "\n",
    "df_pairs = pd.DataFrame({\n",
    "    'qa_weight': qa_list,\n",
    "    'gemv_avg_resp': gemv_list,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'S02'\n",
    "df_qa_dict = joblib.load(f'df_qa_dict_{subject.replace(\"UT\", \"\")}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for setting in df_qa_dict.keys():\n",
    "for setting in ['avg']:\n",
    "    df_qa_weights = df_qa_dict[setting]\n",
    "    df_pairs = df_pairs[df_pairs['qa_weight'].isin(df_qa_weights.index)]\n",
    "    flatmaps_qa = df_qa_weights.loc[df_pairs['qa_weight'].values]['weights'].values\n",
    "    flatmaps_gemv = [gemv_avg_flatmaps[bd]\n",
    "                     for bd in df_pairs['gemv_avg_resp'].values]\n",
    "\n",
    "    corrs = viz._calc_corrs(\n",
    "        flatmaps_qa,\n",
    "        flatmaps_gemv,\n",
    "        titles_qa=[analyze_helper.abbrev_question(\n",
    "            q) for q in df_pairs['qa_weight'].astype(str)],\n",
    "        titles_gt=df_pairs['gemv_avg_resp'].apply(lambda x: x[0]).astype(str)\n",
    "    )\n",
    "\n",
    "    viz.corr_bars(\n",
    "        corrs,\n",
    "        out_dir_save=join(repo_dir, 'qa_results', 'hypothesis_tests', setting),\n",
    "        xlab='GEM-V',\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quickshow(wt_qas[0])\n",
    "# quickshow(wt_gemv[0])\n",
    "sasc.viz.quickshow(\n",
    "    flatmaps_qa[0],\n",
    "    subject=\"UTS02\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatmaps_qa[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.embgam')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "559535f78d940c882783b39501b2581b5193373045707e5f8a51d046029cfd38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
